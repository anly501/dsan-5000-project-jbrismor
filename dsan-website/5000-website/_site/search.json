[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jorge Bris Moreno",
    "section": "",
    "text": "Jorge Bris Moreno is a Master’s student of the Data Science and Analytics program at Georgetown University. In his undergraduate studies, he majored in Mathematics and Business from Franklin & Marshall College. His most relevant experiences have been in project management and business planning."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Finley Malloc",
    "section": "",
    "text": "Finley Malloc is the Chief Data Scientist at Wengo Analytics. When not innovating on data platforms, Finley enjoys spending time unicycling and playing with her pet iguana."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Finley Malloc",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Finley Malloc",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "About",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jorge Bris Moreno",
    "section": "Education",
    "text": "Education\nGeorgetown University | Washington, DC\n\nM.S in Data Science & Analytics | Jun 2023 - May 2025\nFranklin & Marshall College | Lancaster, PA\n\nB.A in Mathematics and Business | Aug 2019 - May 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Jorge Bris Moreno",
    "section": "Experience",
    "text": "Experience\nStatistician | PhD student’s colaborator Program in Industries of Communication and Culture | Nov 2022 - Present\nClient Engagement & Resource Assistant | ASSETS | Sept 2022 - May 2023\nProject Manager Intern | Miller Mats | May 2022 - Aug 2022"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jorge Bris Moreno",
    "section": "",
    "text": "Jorge Bris Moreno is a Master’s student of the Data Science and Analytics program at Georgetown University. In his undergraduate studies, he majored in Mathematics and Business from Franklin & Marshall College. His most relevant experiences have been in project management and business planning."
  },
  {
    "objectID": "index.html#relevant-experience",
    "href": "index.html#relevant-experience",
    "title": "Jorge Bris Moreno",
    "section": "Relevant Experience",
    "text": "Relevant Experience\nStatistician | PhD student’s research colaborator\n\nNov 2022 - Present\nClient Engagement & Resource Assistant | ASSETS\n\nSept 2022 - May 2023\nProject Manager Intern | Miller Mats\n\nMay 2022 - Aug 2022"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "What is the future of Public Transportation?",
    "section": "",
    "text": "Public transportation is any method of transportation available to the public that is sponsored by governmental entities. In some countries, it is the most used method of transportation while, in others, it is almost inexistent. What are the benefits of having public transportation? Should governmental entities invest on it? What are the preferred methods of transportation? These are just a few questions that every government asks themselves. The public transportation insdustry is one of the biggest lines of business worldwide, with trillions of everyday users. It is so important that cities plan their growth around it in order to provide equal access for everyone. Not only the methods of transport are taken into account, but all the infrastructure surrounding it such as freeways, roads, tunnels, etc. This infrastructure ensures social, economical, and environmental sustainability on the city (Murray et al. 1998)."
  },
  {
    "objectID": "Introduction.html#education",
    "href": "Introduction.html#education",
    "title": "Introduction",
    "section": "",
    "text": "University of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "Introduction.html#experience",
    "href": "Introduction.html#experience",
    "title": "What is the future of Public Transportation?",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "Introduction.html#summary",
    "href": "Introduction.html#summary",
    "title": "What is the future of Public Transportation?",
    "section": "",
    "text": "Public transportation is any method of transportation available to the public that is sponsored by governmental entities. In some countries, it is the most used method of transportation while, in others, it is almost inexistent. What are the benefits of having public transportation? Should governmental entities invest on it? What are the preferred methods of transportation? These are just a few questions that every government asks themselves. The public transportation insdustry is one of the biggest lines of business worldwide, with trillions of everyday users. It is so important that cities plan their growth around it in order to provide equal access for everyone. Not only the methods of transport are taken into account, but all the infrastructure surrounding it such as freeways, roads, tunnels, etc. This infrastructure ensures social, economical, and environmental sustainability on the city (Murray et al. 1998)."
  },
  {
    "objectID": "Introduction.html#motivation",
    "href": "Introduction.html#motivation",
    "title": "What is the future of Public Transportation?",
    "section": "Motivation",
    "text": "Motivation\nI believe that the use of public transportation has an uncountable number of benefits. Growing up with many family members working for the public transportation sector in Madrid (Spain), I have developed a strong sense of value for this industry. While many variables and many different (sometimes opposite) interests need to be taken into account, public transport has become essential for our society. Millions of people depend on it to carry out essential activities such as shopping, going to work, or going to do other daily chores. For this reason, it is an industry that deserves severe research and development. Furthermore, due to COVID-19, the number of riders of public transportation have decreased, and there is an urgent need to bring them back. This will allow cities to be more socially sustainable and reduce the polution levels (UITP 2023).\nThe goal of this research is to make a case about the worth of having public transportation around the world and make recommendations for future steps that should be taken in this field."
  },
  {
    "objectID": "Introduction.html#questions",
    "href": "Introduction.html#questions",
    "title": "What is the future of Public Transportation?",
    "section": "Questions",
    "text": "Questions\n\nHow does public transport affect pollution?\nWhat are the most pressing issues related to public transport?\nHow will intelligent (autonomous vehicles) transportation affect the job market?\nHow does public transportation affect the economy and development of a city?\nTo what extent does public transportation reduce the usage of private transportation?\nWhich are the financial support structures for public transportation and how did new global laws about pollution affect them?\nWhat is the relationship between public transportation and tourism?\nWhat is the future of public transportation security measures?\nWhat other type of eneergy (aside from gas) can public transportation use and how these alternatives affect pollution levels?\nHow does the travel time in public transportation affect the decision process about using a private tranport method?"
  },
  {
    "objectID": "Data_Gathering.html",
    "href": "Data_Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "All data files, whether generated using code or directly downloaded from the source can be found and downloaded in the data tab of the website."
  },
  {
    "objectID": "Data_Gathering.html#education",
    "href": "Data_Gathering.html#education",
    "title": "Data Gathering",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "Data_Gathering.html#experience",
    "href": "Data_Gathering.html#experience",
    "title": "Data Gathering",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "Data_Gathering.html#apis",
    "href": "Data_Gathering.html#apis",
    "title": "Data Gathering",
    "section": "APIs",
    "text": "APIs\n\nReddit API\nThe following code extracted reddit urls from reddit posts about Public Transportation and saved them to a .json.\nlibrary(RedditExtractoR)\nlibrary(jsonlite)\n\ntop_Pub_Transp_urls &lt;- find_thread_urls(keywords=\"public transportation\")\njsonlite::write_json(top_Pub_Transp_urls, \"top_pub_transp_urls.json\")\nThen, the following code extracted the content of those reddit posts and performed sentiment analysis on them, generated a data frame, and saved it to a .csv file\n\n\nCode\nimport pandas as pd\nimport json\n\n# Load the sentiment scores from the JSON file\nwith open('sentiment_scores.json', 'r') as json_file:\n    sentiment_scores = json.load(json_file)\n\n# Initialize lists to store data\nids = []\nneg_scores = []\nneu_scores = []\npos_scores = []\ncompound_scores = []\n\n# Extract the scores and create separate lists for each\nfor idx, item in enumerate(sentiment_scores, start=1):\n    ids.append(idx)\n    sentiment_score = item.get('sentiment_score', {})\n    neg_scores.append(sentiment_score.get('neg', 0))\n    neu_scores.append(sentiment_score.get('neu', 0))\n    pos_scores.append(sentiment_score.get('pos', 0))\n    compound_scores.append(sentiment_score.get('compound', 0))\n\n# Create a DataFrame\ndata = {\n    'ID': ids,\n    'Negative Score': neg_scores,\n    'Neutral Score': neu_scores,\n    'Positive Score': pos_scores,\n    'Compound Score': compound_scores\n}\n\ndf = pd.DataFrame(data)\n\n# Save to CSV\ndf.to_csv('sentiment_scores.csv', index=False)\n\n\nThe final data’s first few rows look like this:\n\n\n\n\n\nHowever, the text data has also been kept in a data file with another code so that later analysis can be performed on it.\n\n\nAPI for cityofchicago.org\nThe following code extracted the following data frame about buses information and saved it into a csv file.\n\n\nCode\nimport pandas as pd\nfrom sodapy import Socrata\n\nclient = Socrata(\"data.cityofchicago.org\", None)\n\nresults = client.get(\"bynn-gwxy\", limit=2000)\n\n# Convert to pandas DataFrame\nresults_df = pd.DataFrame.from_records(results)\n\n# Save to CSV\nresults_df.to_csv('Chicago_avg_Buses.csv')\n\n\nThis is a snapshot of the data:"
  },
  {
    "objectID": "Data_Gathering.html#data-from-data.world",
    "href": "Data_Gathering.html#data-from-data.world",
    "title": "Data Gathering",
    "section": "Data from data.world",
    "text": "Data from data.world\nThe file DC_Metro_Scorecard.xlsx and the zip folders: Walkable_distance_to_PubTrans, and capmetro_smart_trips_questionaire zip folders contain data that was downloaded from data.world. These are the three links where the data was downloaded from(in order of mention):\n\nhttps://data.world/makeovermonday/2016w51\nhttps://data.world/chhs/5e391154-f07d-4e0c-ab1b-687a0c4c5d06\nhttps://data.world/browning/capmetro-smart-trips-questionaire\n\nThe data looks like this (in order of mention):"
  },
  {
    "objectID": "Data_Gathering.html#data-from-bts.gov",
    "href": "Data_Gathering.html#data-from-bts.gov",
    "title": "Data Gathering",
    "section": "Data from bts.gov",
    "text": "Data from bts.gov\nThe files: bus_consumption_fuel_by_year.xlsx, energy_consumed_byMill_passenger_MILES.xlsx, fatalities_bus_over_time.xlsx, National_transport_usage_linked_Economic_trend.xlsx, and vehicle_production_countries.xlsx are downloaded from: https://www.bts.gov.\nThese are some snapshots of how the data looks like:"
  },
  {
    "objectID": "Data_Gathering.html#data-from-international-transport-forum",
    "href": "Data_Gathering.html#data-from-international-transport-forum",
    "title": "Data Gathering",
    "section": "Data from International Transport Forum",
    "text": "Data from International Transport Forum\nThe file Value_transport_by_countries.csv was downloaded from: https://stats.oecd.org/Index.aspx?DataSetCode=ITF_PASSENGER_TRANSPORT\nThe data on that file looks like this:"
  },
  {
    "objectID": "Naive_Bayes.html",
    "href": "Naive_Bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a supervised machine learning algorithm based on Bayes’ Theorem. It is used for classification tasks where every pair of features being classified is independent of each other.\nWhile Naive Bayes might seem a simple process, its results are very insightful and have been proven to work very efficiently. One of the benefits is that it needs very little data to be trained and, as more data comes in, it can always be trained incrementally. The three most common Naive Bayes algorithms are: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\n\n\nThe formula for Bayes’ Theorem is: \\(P(\\text{A}|\\text{B}) = \\frac{P(\\text{B}|\\text{A}) \\times P(\\text{A})} {P(\\text{B})}\\)\n\n\n\nIt is used when the variables are continuous and have a normal distribution\nFormula: \\(P(\\text{class}|\\text{data}) = \\frac{P(\\text{data}|\\text{class}) \\times P(\\text{class})} {P(\\text{data})}\\)\n\n\n\nIt is used when the variables are discrete and have a multinomial distribution.\n\n\n\nIt is used when the variables are binary and have a Bernoulli distribution.\n\n\n\n\nMissing values are ignored while training the model and ignored when a probability is calculated.\nPerforms well even when the independence assumption is not satisfied.\nIt is easily to interpretate and has fast prediction time."
  },
  {
    "objectID": "Naive_Bayes.html#introduction",
    "href": "Naive_Bayes.html#introduction",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a supervised machine learning algorithm based on Bayes’ Theorem. It is used for classification tasks where every pair of features being classified is independent of each other.\nWhile Naive Bayes might seem a simple process, its results are very insightful and have been proven to work very efficiently. One of the benefits is that it needs very little data to be trained and, as more data comes in, it can always be trained incrementally. The three most common Naive Bayes algorithms are: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\n\n\nThe formula for Bayes’ Theorem is: \\(P(\\text{A}|\\text{B}) = \\frac{P(\\text{B}|\\text{A}) \\times P(\\text{A})} {P(\\text{B})}\\)\n\n\n\nIt is used when the variables are continuous and have a normal distribution\nFormula: \\(P(\\text{class}|\\text{data}) = \\frac{P(\\text{data}|\\text{class}) \\times P(\\text{class})} {P(\\text{data})}\\)\n\n\n\nIt is used when the variables are discrete and have a multinomial distribution.\n\n\n\nIt is used when the variables are binary and have a Bernoulli distribution.\n\n\n\n\nMissing values are ignored while training the model and ignored when a probability is calculated.\nPerforms well even when the independence assumption is not satisfied.\nIt is easily to interpretate and has fast prediction time."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploring Data",
    "section": "",
    "text": "The goals of this tab is to gain a deeper understanding of the data, how variables behave, and identify anomalies, paterns, or potential insights for future exploration."
  },
  {
    "objectID": "EDA.html#eda-goals",
    "href": "EDA.html#eda-goals",
    "title": "Exploring Data",
    "section": "",
    "text": "The goals of this tab is to gain a deeper understanding of the data, how variables behave, and identify anomalies, paterns, or potential insights for future exploration."
  },
  {
    "objectID": "Naive_Bayes.html#naive-bayes-for-labeled-record-data",
    "href": "Naive_Bayes.html#naive-bayes-for-labeled-record-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes for labeled record data",
    "text": "Naive Bayes for labeled record data\nNaive Bayes has been used in our record data from our data set: Border_Crossing_Entry_Data_20231103.csv. The model tries to predict the method of transportation (“Measure” in the data set) based on the value and the State of entrance.\n\n\nCode\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(e1071)\n#library(\"DescTools\")\n\nchicago &lt;- read.csv('../../data/Raw_Data_project_Pub.Transport_5000/Border_Crossing_Entry_Data_20231103.csv')\nhead(chicago)\n\nchicago &lt;- chicago[, c(\"Value\", \"Measure\", \"State\")]\nvars &lt;- c(\"Value\", \"State\")\n\nset.seed(4444) # initialize the random seed\n\n# Generate the list of observations for the\n# train dataframe\nsub &lt;- createDataPartition(y = chicago$Measure, \n                           p = 0.80, \n                           list = FALSE)\n\nchicago[] &lt;- lapply(chicago, as.factor)\n\ncen_bcs &lt;- preProcess(x = chicago[ ,vars],\n                      method = c(\"BoxCox\", \"center\", \"scale\"))\n\nDM_bcs &lt;- predict(object = cen_bcs,\n                      newdata = chicago)\n\ntrain &lt;- DM_bcs[sub, ] \ntest &lt;- DM_bcs[-sub, ]\n\n# Train a Naive Bayes model\nnb_model &lt;- naiveBayes(Measure ~ ., data = train)\n\n# Make predictions on the test data\npredictions &lt;- predict(nb_model, newdata = test)\n\n# Ensure predictions contain all possible factor levels\npredictions &lt;- factor(predictions, levels = levels(test$Measure))\n\n# Evaluate the model (e.g., confusion matrix)\nconfusion_matrix &lt;- confusionMatrix(predictions, test$Measure)\nprint(confusion_matrix)\n\n# Print additional classification performance metrics\nprint(confusion_matrix$byClass)\n\n\nConfusion Matrix and Statistics\n\n                             Reference\nPrediction                    Bus Passengers Buses Pedestrians\n  Bus Passengers                         181   129          80\n  Buses                                  514  2172        1004\n  Pedestrians                            668   273        1127\n  Personal Vehicle Passengers            463    96         419\n  Personal Vehicles                      279    43         106\n  Rail Containers Empty                  100    82          22\n  Rail Containers Loaded                1852  1839        2324\n  Train Passengers                       368   289         228\n  Trains                                 575   513         341\n  Truck Containers Empty                 646   480         473\n  Truck Containers Loaded                356   161         127\n  Trucks                                 170    99          89\n                             Reference\nPrediction                    Personal Vehicle Passengers Personal Vehicles\n  Bus Passengers                                      145               142\n  Buses                                               263               303\n  Pedestrians                                        1305              1322\n  Personal Vehicle Passengers                        1936              1827\n  Personal Vehicles                                  1635              1282\n  Rail Containers Empty                                15                28\n  Rail Containers Loaded                              146               194\n  Train Passengers                                    144               291\n  Trains                                              293               423\n  Truck Containers Empty                              648               719\n  Truck Containers Loaded                             438               453\n  Trucks                                              220               209\n                             Reference\nPrediction                    Rail Containers Empty Rail Containers Loaded\n  Bus Passengers                                 67                     43\n  Buses                                         184                    142\n  Pedestrians                                   316                    270\n  Personal Vehicle Passengers                   189                    287\n  Personal Vehicles                             193                    302\n  Rail Containers Empty                         320                    401\n  Rail Containers Loaded                       3752                   3700\n  Train Passengers                              127                    102\n  Trains                                        316                    237\n  Truck Containers Empty                        225                    184\n  Truck Containers Loaded                       111                    101\n  Trucks                                         61                     76\n                             Reference\nPrediction                    Train Passengers Trains Truck Containers Empty\n  Bus Passengers                            70     96                    205\n  Buses                                    225    497                   1097\n  Pedestrians                               53     13                    827\n  Personal Vehicle Passengers               85     12                    366\n  Personal Vehicles                         60      2                    283\n  Rail Containers Empty                    310    322                    174\n  Rail Containers Loaded                  3662   3435                   1205\n  Train Passengers                         497    282                    491\n  Trains                                   352    527                    895\n  Truck Containers Empty                   260    457                    849\n  Truck Containers Loaded                  143    132                    370\n  Trucks                                    74     90                    214\n                             Reference\nPrediction                    Truck Containers Loaded Trucks\n  Bus Passengers                                  172    212\n  Buses                                           824    796\n  Pedestrians                                     867    976\n  Personal Vehicle Passengers                     660    812\n  Personal Vehicles                               542    657\n  Rail Containers Empty                           156    119\n  Rail Containers Loaded                         1191    533\n  Train Passengers                                375    436\n  Trains                                          658    806\n  Truck Containers Empty                          862    961\n  Truck Containers Loaded                         389    446\n  Trucks                                          198    252\n\nOverall Statistics\n                                          \n               Accuracy : 0.1712          \n                 95% CI : (0.1685, 0.1738)\n    No Information Rate : 0.093           \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.0973          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n\nStatistics by Class:\n\n                     Class: Bus Passengers Class: Buses Class: Pedestrians\nSensitivity                       0.029326      0.35168            0.17776\nSpecificity                       0.980867      0.91777            0.90291\nPos Pred Value                    0.117380      0.27079            0.14058\nNeg Pred Value                    0.920927      0.94221            0.92477\nPrevalence                        0.079838      0.07989            0.08201\nDetection Rate                    0.002341      0.02810            0.01458\nDetection Prevalence              0.019946      0.10376            0.10370\nBalanced Accuracy                 0.505097      0.63473            0.54034\n                     Class: Personal Vehicle Passengers\nSensitivity                                     0.26934\nSpecificity                                     0.92561\nPos Pred Value                                  0.27069\nNeg Pred Value                                  0.92514\nPrevalence                                      0.09298\nDetection Rate                                  0.02504\nDetection Prevalence                            0.09251\nBalanced Accuracy                               0.59747\n                     Class: Personal Vehicles Class: Rail Containers Empty\nSensitivity                           0.17823                     0.054598\nSpecificity                           0.94150                     0.975800\nPos Pred Value                        0.23811                     0.156174\nNeg Pred Value                        0.91781                     0.926373\nPrevalence                            0.09304                     0.075815\nDetection Rate                        0.01658                     0.004139\nDetection Prevalence                  0.06964                     0.026505\nBalanced Accuracy                     0.55986                     0.515199\n                     Class: Rail Containers Loaded Class: Train Passengers\nSensitivity                                0.63302                0.085823\nSpecificity                                0.71827                0.956192\nPos Pred Value                             0.15525                0.136915\nNeg Pred Value                             0.95989                0.928146\nPrevalence                                 0.07561                0.074909\nDetection Rate                             0.04786                0.006429\nDetection Prevalence                       0.30829                0.046956\nBalanced Accuracy                          0.67564                0.521007\n                     Class: Trains Class: Truck Containers Empty\nSensitivity               0.089855                       0.12170\nSpecificity               0.924288                       0.91590\nPos Pred Value            0.088780                       0.12552\nNeg Pred Value            0.925208                       0.91315\nPrevalence                0.075866                       0.09024\nDetection Rate            0.006817                       0.01098\nDetection Prevalence      0.076785                       0.08750\nBalanced Accuracy         0.507072                       0.51880\n                     Class: Truck Containers Loaded Class: Trucks\nSensitivity                                0.056426       0.03597\nSpecificity                                0.959695       0.97866\nPos Pred Value                             0.120545       0.14384\nNeg Pred Value                             0.912190       0.91061\nPrevalence                                 0.089177       0.09063\nDetection Rate                             0.005032       0.00326\nDetection Prevalence                       0.041743       0.02266\nBalanced Accuracy                          0.508060       0.50732\n                                   Sensitivity Specificity Pos Pred Value\nClass: Bus Passengers               0.02932599   0.9808674     0.11738003\nClass: Buses                        0.35168394   0.9177714     0.27078918\nClass: Pedestrians                  0.17776025   0.9029126     0.14057628\nClass: Personal Vehicle Passengers  0.26933779   0.9256122     0.27069351\nClass: Personal Vehicles            0.17822883   0.9414953     0.23811293\nClass: Rail Containers Empty        0.05459819   0.9757999     0.15617374\nClass: Rail Containers Loaded       0.63301967   0.7182698     0.15524693\nClass: Train Passengers             0.08582283   0.9561916     0.13691460\nClass: Trains                       0.08985507   0.9242882     0.08878032\nClass: Truck Containers Empty       0.12170298   0.9158977     0.12551745\nClass: Truck Containers Loaded      0.05642588   0.9596949     0.12054540\nClass: Trucks                       0.03596917   0.9786632     0.14383562\n                                   Neg Pred Value  Precision     Recall\nClass: Bus Passengers                   0.9209265 0.11738003 0.02932599\nClass: Buses                            0.9422105 0.27078918 0.35168394\nClass: Pedestrians                      0.9247655 0.14057628 0.17776025\nClass: Personal Vehicle Passengers      0.9251372 0.27069351 0.26933779\nClass: Personal Vehicles                0.9178149 0.23811293 0.17822883\nClass: Rail Containers Empty            0.9263733 0.15617374 0.05459819\nClass: Rail Containers Loaded           0.9598870 0.15524693 0.63301967\nClass: Train Passengers                 0.9281458 0.13691460 0.08582283\nClass: Trains                           0.9252077 0.08878032 0.08985507\nClass: Truck Containers Empty           0.9131452 0.12551745 0.12170298\nClass: Truck Containers Loaded          0.9121895 0.12054540 0.05642588\nClass: Trucks                           0.9106082 0.14383562 0.03596917\n                                           F1 Prevalence Detection Rate\nClass: Bus Passengers              0.04692766 0.07983753    0.002341314\nClass: Buses                       0.30598014 0.07988927    0.028095774\nClass: Pedestrians                 0.15699659 0.08201068    0.014578240\nClass: Personal Vehicle Passengers 0.27001395 0.09297994    0.025043010\nClass: Personal Vehicles           0.20386420 0.09304461    0.016583233\nClass: Rail Containers Empty       0.08091024 0.07581461    0.004139341\nClass: Rail Containers Loaded      0.24934295 0.07560764    0.047861125\nClass: Train Passengers            0.10550897 0.07490913    0.006428913\nClass: Trains                      0.08931446 0.07586635    0.006816976\nClass: Truck Containers Empty      0.12358079 0.09023762    0.010982188\nClass: Truck Containers Loaded     0.07686987 0.08917692    0.005031886\nClass: Trucks                      0.05754739 0.09062569    0.003259731\n                                   Detection Prevalence Balanced Accuracy\nClass: Bus Passengers                        0.01994645         0.5050967\nClass: Buses                                 0.10375516         0.6347277\nClass: Pedestrians                           0.10370342         0.5403364\nClass: Personal Vehicle Passengers           0.09251426         0.5974750\nClass: Personal Vehicles                     0.06964440         0.5598621\nClass: Rail Containers Empty                 0.02650471         0.5151990\nClass: Rail Containers Loaded                0.30829032         0.6756448\nClass: Train Passengers                      0.04695564         0.5210072\nClass: Trains                                0.07678477         0.5070717\nClass: Truck Containers Empty                0.08749531         0.5188003\nClass: Truck Containers Loaded               0.04174266         0.5080604\nClass: Trucks                                0.02266289         0.5073162\n\n\n\n\nCode\n# Convert the confusion matrix to a data frame\nconfusion_matrix_df &lt;- as.data.frame(as.table(confusion_matrix$table))\n\n# Create the plot using ggplot2\nlibrary(ggplot2)\ngg &lt;- ggplot(data = confusion_matrix_df, aes(x = Reference, y = Prediction)) +\n  geom_tile(aes(fill = Freq)) +\n  geom_text(aes(label = Freq), vjust = 1) +\n  scale_fill_gradient(low = \"#7cf09b\", high = \"#3ee882\") +\n  labs(\n    x = \"Reference\",\n    y = \"Prediction\",\n    fill = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12)) +\n  ggtitle(\"Confusion Matrix\")\n\nprint(gg)\n\n\n\n\n\nWe can infer that this model would not be the best to predict this outcome, as its accruacy is only 17%."
  },
  {
    "objectID": "Naive_Bayes.html#naive-bayes-for-labeled-text-data",
    "href": "Naive_Bayes.html#naive-bayes-for-labeled-text-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes for labeled text data",
    "text": "Naive Bayes for labeled text data\nNaive Bayes has been used in our text data from redit in order to predict the sentiment of the comments. The data has been labeled by the sentiment of the comments as shown in the data gathering tab. The data has been split into a training set and a test set. The training set has been used to train the model and the test set has been used to test the model.\n\n\nCode\n\n# read in the data\ndf=pd.read_json(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/sentiment_results.json\")\nprint(df.shape)\nprint(df.columns)\n\n# throw away all empty strings of text from df along with their url and sentiment_score\ndf = df[df.text != '']\nprint(df.shape)\nprint(df.columns)\n\n# convert sentiment_score to positive or negative\ndf['sentiment'] = df['sentiment_score'].apply(lambda score: 'positive' if score &gt;= 0 else 'negative')\n\n\n\n\nCode\ntexts = []\ny = []\n#https://spacy.io/usage/models\n\nimport re\nimport spacy.lang.en.stop_words as stopwords\nimport spacy\nparser = spacy.load('en_core_web_sm')\nstop_words = stopwords.STOP_WORDS\n\n\n# Iterate over rows\nfor i in range(df.shape[0]):\n    # QUICKLY CLEAN TEXT\n    keep = \"abcdefghijklmnopqrstuvwxyz \"\n    replace = \".,!;\"\n    tmp = \"\"\n    text_value = df[\"text\"].iloc[i]  # Accessing the \"text\" column using .iloc\n    sentiment_value = df[\"sentiment\"].iloc[i]  # Accessing the \"sentiment\" column using .iloc\n\n\n    text_value = re.sub('[^a-zA-Z ]+', '', text_value.replace(\"&lt;br /&gt;\", \"\").lower())\n    text_value = parser(text_value)\n    tokens = [token.lower_ for token in text_value]\n    #tokens = [token for token in text_value if token not in stop_words]\n    tokens = [token.lemma_ for token in text_value if token not in stop_words]\n\n\n    tmp = \" \".join(tokens)\n    texts.append(tmp)\n\n    # CONVERT STRINGS TO INT TAGS\n    if sentiment_value == \"positive\":\n        y.append(1)\n    elif sentiment_value == \"negative\":\n        y.append(0)\n\n    # PRINT FIRST COUPLE TEXTS\n    if i &lt; 3:\n        print(i)\n        print(tmp.replace(\"&lt;br /&gt;\", \"\"), '\\n')\n        print(tmp)\n        print(sentiment_value, y[i])\n\n# CONVERT Y TO NUMPY ARRAY\ny=np.array(y)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# PARAMETERS TO CONTROL SIZE OF FEATURE SPACE WITH COUNT-VECTORIZER\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\n# max_features=int, default=None\n#   If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n    # RUN COUNT VECTORIZER ON OUR COURPUS \n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n    #CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(texts,MAX_FEATURES=10000)\n\n\n\n\nCode\n#swap keys and values (value --&gt; ley)\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\n\n#RE-ORDER COLUMN SO IT IS SORTED FROM HIGH FREQ TERMS TO LOW \n# https://stackoverflow.com/questions/60758625/sort-pandas-dataframe-by-sum-of-columns\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\n\n# REMAP DICTIONARY TO CORRESPOND TO NEW COLUMN NUMBERS\nprint()\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n    # print(i2)\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\n\n# RENAME COLUMNS 0,1,2,3 .. \ndf2.columns = range(df2.columns.size)\nprint(df2.head())\nprint(df2.sum(axis=0))\nx=df2.to_numpy()\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\nfrom sklearn.model_selection import train_test_split\n\ndef train_MNB_model(X, Y, i_print=False):\n    if i_print:\n        print(X.shape, Y.shape)\n\n    # SPLIT USING train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # INITIALIZE MODEL\n    model = MultinomialNB()\n\n    # TRAIN MODEL\n    start = time.process_time()\n    model.fit(X_train, y_train)\n    time_train = time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET\n    start = time.process_time()\n    yp_train = model.predict(X_train)\n    yp_test = model.predict(X_test)\n    time_eval = time.process_time() - start\n\n    acc_train = accuracy_score(y_train, yp_train) * 100\n    acc_test = accuracy_score(y_test, yp_test) * 100\n\n    if i_print:\n        print(acc_train, acc_test, time_train, time_eval)\n\n    return acc_train, acc_test, time_train, time_eval\n\n# TEST\nprint(type(x), type(y))\nprint(x.shape, y.shape)\n(acc_train, acc_test, time_train, time_eval) = train_MNB_model(x, y, i_print=True)\n\n##UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n# SAVE RESULTS\nsave_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/results_naive_bayes\")\n\n\n\n\nCode\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-ob')\n    plt.plot(num_features,test_accuracies,'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n# PLOT RESULTS\nplot_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/results_naive_bayes\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsave_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/partial_grid_search\")\nplot_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes seems to do a good job in accruacy if the number of features is kept under 2000 or between 4000 and 6000."
  },
  {
    "objectID": "Data_cleaning.html",
    "href": "Data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport json\nimport re\n\n# Load the existing JSON file with Reddit post data\nwith open('top_pub_transp_urls.json', 'r') as json_file:\n    data = json.load(json_file)\n\n# Function to clean the text\ndef clean_text(text):\n    # Remove unwanted characters and escape sequences\n    cleaned_text = re.sub(r'\\\\u....', '', text)  # Remove escape sequences\n    cleaned_text = re.sub(r'[^A-Za-z\\s]', ' ', cleaned_text)  # Remove non-alphabetic characters\n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra whitespace\n    cleaned_text = cleaned_text.strip()  # Strip leading and trailing spaces\n    return cleaned_text\n\n# Extract and clean the text data\ncleaned_text_data = []\n\nfor item in data:\n    cleaned_text = clean_text(item.get(\"text\", \"\"))\n    cleaned_text_data.append({\"url\": item[\"url\"], \"cleaned_text\": cleaned_text})\n\n# Save the cleaned text data to a new JSON file\nwith open('cleaned_text_data.json', 'w') as json_file:\n    json.dump(cleaned_text_data, json_file, indent=4)\n\nprint(\"Text data cleaned and saved to cleaned_text_data.json.\")\n\n\nThe previous code cleans the text data such that we only get the urls and the text of each reddit post. It can be found in the data tab. That way it can be later used for key word extraction.\nNote: the sentiment analysis was already extracted and saved in the data tab.\nclean data as vocabulary each col and each row number of appareances"
  },
  {
    "objectID": "Data_cleaning.html#text-data",
    "href": "Data_cleaning.html#text-data",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport json\nimport re\n\n# Load the existing JSON file with Reddit post data\nwith open('top_pub_transp_urls.json', 'r') as json_file:\n    data = json.load(json_file)\n\n# Function to clean the text\ndef clean_text(text):\n    # Remove unwanted characters and escape sequences\n    cleaned_text = re.sub(r'\\\\u....', '', text)  # Remove escape sequences\n    cleaned_text = re.sub(r'[^A-Za-z\\s]', ' ', cleaned_text)  # Remove non-alphabetic characters\n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra whitespace\n    cleaned_text = cleaned_text.strip()  # Strip leading and trailing spaces\n    return cleaned_text\n\n# Extract and clean the text data\ncleaned_text_data = []\n\nfor item in data:\n    cleaned_text = clean_text(item.get(\"text\", \"\"))\n    cleaned_text_data.append({\"url\": item[\"url\"], \"cleaned_text\": cleaned_text})\n\n# Save the cleaned text data to a new JSON file\nwith open('cleaned_text_data.json', 'w') as json_file:\n    json.dump(cleaned_text_data, json_file, indent=4)\n\nprint(\"Text data cleaned and saved to cleaned_text_data.json.\")\n\n\nThe previous code cleans the text data such that we only get the urls and the text of each reddit post. It can be found in the data tab. That way it can be later used for key word extraction.\nNote: the sentiment analysis was already extracted and saved in the data tab.\nclean data as vocabulary each col and each row number of appareances"
  },
  {
    "objectID": "EDA.html#vehicle-production-by-countries",
    "href": "EDA.html#vehicle-production-by-countries",
    "title": "Exploring Data",
    "section": "Vehicle Production by countries",
    "text": "Vehicle Production by countries\nHere, we will be exploring the vehicle_production_countries data set.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(boot)\nlibrary(AICcmodavg)\nlibrary(readxl)\nlibrary(broom)\n\nvehicle_production_countries_comercial &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/vehicle_production_countries.xlsx\", sheet=\"Comercial-vehicles\", skip = 1)\n\n# Replace \"N\", \"U\", and \"Z\" with NULL in all columns except the first one\nvehicle_production_countries_comercial[-1] &lt;- lapply(vehicle_production_countries_comercial[-1], function(x) ifelse(x %in% c(\"N\", \"U\", \"Z\"), NA, x))\n\nvehicle_production_countries_comercial &lt;- vehicle_production_countries_comercial %&gt;%\n  mutate(across(-1, as.numeric))\n\ncolnames(vehicle_production_countries_comercial)[colnames(vehicle_production_countries_comercial) == \"...1\"] &lt;- \"Country\"\ncolnames(vehicle_production_countries_comercial)\n\n# Transpose the data frame\nvehicle_production_countries_comercial_transposed &lt;- as.data.frame(t(vehicle_production_countries_comercial))\n\n# Set the first row as the column names\ncolnames(vehicle_production_countries_comercial_transposed) &lt;- vehicle_production_countries_comercial_transposed[1, ]\n\n# Remove the first row\nvehicle_production_countries_comercial_transposed &lt;- vehicle_production_countries_comercial_transposed[-1, ]\n\n# Convert the year columns to numeric\nvehicle_production_countries_comercial_transposed[] &lt;- lapply(vehicle_production_countries_comercial_transposed, as.numeric)\n\nvehicle_production_countries_comercial_transposed &lt;- vehicle_production_countries_comercial_transposed[, !colnames(vehicle_production_countries_comercial_transposed) %in% \"Total world\"]\n\ndf &lt;- vehicle_production_countries_comercial_transposed %&gt;% \n  rownames_to_column(var = \"Year\")\n\n# Replace \"(R) 2019\" with 2019 and \"(R) 2020\" with 2020\ndf &lt;- df %&gt;%\n  mutate(Year = ifelse(Year == \"(R) 2019\", 2019, ifelse(Year == \"(R) 2020\", 2020, Year)))\n\n\n\n\nCode\n# Pivot the data to long format\ndf_long &lt;- df %&gt;%\n  pivot_longer(-Year, names_to = \"Country\", values_to = \"Value\")\n\n# Plotlot\nggplot(df_long, aes(x = Year, y = Value, color = Country)) +\n  geom_point() +\n  labs(\n    title = \"Value per year and per country in comercial vehicle production\",\n    x = \"Year\",\n    y = \"Value of comercial vehicles produced\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", axis.text.x = element_text(angle = 90, hjust = 1))\n\n\nWarning message:\n\"Removed 188 rows containing missing values (geom_point).\"\n\n\n\n\n\nFrom this plot we can see how Japan (back in the 1960s) used to be one of the biggest producers but not anymore. The US has been in the lead since the 1990s until around 2008, which was expected due to the recession. However, it is impressive how China was able to excel during the recession in this market at an almost exponential rate. And it still has kept increasing since then.\nIn order to run an ANOVA test and look if countries, years, and interaction among the both have an statistical significance on comercial vehicle production, we are going to assume that all the missing values are not relevant to the contribution of the overall market and, therefore, close to 0. Thuis, for the sake of this process, we will make them 0.\n\n\nCode\n# Replace NA values with 0\nvehicle_production_countries_comercial &lt;- replace(vehicle_production_countries_comercial, is.na(vehicle_production_countries_comercial), 0)\n\n\n\n\nCode\ndata_long &lt;- vehicle_production_countries_comercial %&gt;%\n  pivot_longer(cols = -Country, names_to = \"Year\", values_to = \"Value\")\n\ndata_long &lt;- data_long[data_long$Country != \"Total world\", ]\n# Perform ANOVA\nmodel &lt;- aov(Value ~ Year * Country, data = data_long)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\n\nWarning message in anova.lm(object):\n\"ANOVA F-tests on an essentially perfect fit are unreliable\"\n\n\nAnalysis of Variance Table\n\nResponse: Value\n              Df     Sum Sq  Mean Sq F value Pr(&gt;F)\nYear          31   85531235  2759072     NaN    NaN\nCountry       32 2172668273 67895884     NaN    NaN\nYear:Country 992 1139993857  1149187     NaN    NaN\nResiduals      0          0      NaN               \n\n\nSeing this, we will simplify our model, taking away the interaction (in order to no overfit the data with our ANOVA model).\n\n\nCode\nmodel &lt;- aov(Value ~ Year + Country, data = data_long)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Value\n           Df     Sum Sq  Mean Sq F value    Pr(&gt;F)    \nYear       31   85531235  2759072  2.4009 3.244e-05 ***\nCountry    32 2172668273 67895884 59.0816 &lt; 2.2e-16 ***\nResiduals 992 1139993857  1149187                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth have a significant effect (which was expected). However, this helped us also understand that their interaction would overfit the model in this case."
  },
  {
    "objectID": "EDA.html#energy-and-usage-of-buses",
    "href": "EDA.html#energy-and-usage-of-buses",
    "title": "Exploring Data",
    "section": "Energy and usage of Buses",
    "text": "Energy and usage of Buses\nIn this part, we will be exploring the energy_consumed_byMill_passenger_MILES file.\n\n\nCode\nenergy_consum &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/energy_consumed_byMill_passenger_MILES.xlsx\", sheet=\"Energy\")\n\ncolnames(energy_consum) &lt;- energy_consum[1, ]\n\n# Remove the first row and the first column\nenergy_consum &lt;- energy_consum[-1, ]\n\n# Set the first column to \"Data_to_Explore\"\ncolnames(energy_consum)[1] &lt;- \"Data_to_Explore\"\n\n# Filter rows based on specific criteria\nenergy_consum &lt;- energy_consum[energy_consum$Data_to_Explore %in% c(\n  \"Vehicle-miles (millions)\",\n  \"Passenger-miles (millions)\",\n  \"Energy consumed, total (billion Btu)\",\n  \"Energy intensity (Btu/passenger-mile)\"\n), ]\n\n# Replace all \"N\" values (except in the first column) with NA\nenergy_consum[, -1] &lt;- apply(energy_consum[, -1], 2, function(x) ifelse(x == \"N\", NA, x))\n\n# Drop specified columns because of numerous NA\nenergy_consum &lt;- energy_consum %&gt;% select(-\"1960\", -\"1965\", -\"1970\", -\"1975\", -\"1980\", -\"1985\", -\"1990\", -\"1991\", -\"1992\", -\"1993\", -\"1994\", -\"1995\")\n\n# Change columns from character to numeric (except the first column)\nenergy_consum &lt;- energy_consum %&gt;%\n  mutate_at(vars(-1), as.numeric)\n\nenergy_consum_long &lt;-as.data.frame(t(energy_consum), index=False)\n\ncolnames(energy_consum_long) &lt;- energy_consum_long[1, ]\n\n# Remove the first row and the first column\nenergy_consum_long &lt;- energy_consum_long[-1, ]\n\ncolnames(energy_consum_long) &lt;- c(\"Vehicle_milesMill\", \"Passenger_miles_mill\",\"Energy_consumed_total_bill_Btu\", \"Energy_intensity_Btu_passenger_mile\")\n\nYears = c(1996:2021)\nenergy_consum_long['Years'] &lt;- Years\n\n# data frame used for the models\nenergy_consum_long_no_Covid &lt;- energy_consum_long[!(energy_consum_long$Years %in% c(2020, 2021)), ]\n\n\nFor the fitted model in the following plots, year 2020 and 2021 are going to be ignored and considered outliers due to COVID-19. They are heavy outliers.\n\n\nCode\n#plot(energy_consum_long$Energy_intensity_Btu_passenger_mile)\n\n# Fit a linear model\nlm_model &lt;- lm(energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile ~ energy_consum_long_no_Covid$Year)\n\n# Create the plot\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile,\n     xlab = \"Year\",\n     ylab = \"Energy consumed, total (billion Btu)\",\n     col = \"blue3\",\n     pch = 19)\n\n# Calculate confidence interval\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\n# Add confidence interval lines\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\n# Shade the area between confidence interval bounds\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\n# Add the trend line\nabline(lm_model, col = \"red\")\n\n# Add the points\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile, col = \"blue3\", pch = 19)\n\n\n\n\n\nFrom this plot we can see how the total energy consumed (in billion Btu) has been constantly decreasing. However, is it because our vehicles are more efficient or due to cuts in public transportation?\n\n\nCode\n#plot(energy_consum_long$Passenger_miles_mill)\n\n# Fit a linear model\nlm_model &lt;- lm(energy_consum_long_no_Covid$Passenger_miles_mill ~ energy_consum_long_no_Covid$Year)\n\n# Create the plot\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Passenger_miles_mill,\n     xlab = \"Year\",\n     ylab = \"Passenger-miles (millions)\",\n     col = \"blue3\",\n     pch = 19)\n\n# Calculate confidence interval\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\n# Add confidence interval lines\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\n# Shade the area between confidence interval bounds\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\n# Add the trend line\nabline(lm_model, col = \"red\")\n\n# Add the points\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Passenger_miles_mill, col = \"blue3\", pch = 19)\n\n\n\n\n\nWhile the fitted model shows an increase in the usage of public transportation, we can see that it has been constantly dropping since around 2014. This can bring up many questions such as: Is it due to an investment problem? Is it because there are not enough incentives to use public transportation? Do people own more cars?\n\n\nCode\n#plot(energy_consum_long$Energy_intensity_Btu_passenger_mile)\n\n# Fit a linear model\nlm_model &lt;- lm(energy_consum_long_no_Covid$Vehicle_milesMill ~ energy_consum_long_no_Covid$Year)\n\n# Create the plot\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Vehicle_milesMill,\n     xlab = \"Year\",\n     ylab = \"Vehicle miles (millions)\",\n     col = \"blue3\",\n     pch = 19)\n\n# Calculate confidence interval\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\n# Add confidence interval lines\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\n# Shade the area between confidence interval bounds\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\n# Add the trend line\nabline(lm_model, col = \"red\")\n\n# Add the points\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Vehicle_milesMill, col = \"blue3\", pch = 19)\n\n\n\n\n\nThe vehicle miles have been constantly increasing, which means that there have been more and more routes added over time. However, why has this not been enough to increase the demand of public transportation?"
  },
  {
    "objectID": "EDA.html#dc-metro-scorecard",
    "href": "EDA.html#dc-metro-scorecard",
    "title": "Exploring Data",
    "section": "DC Metro Scorecard",
    "text": "DC Metro Scorecard\nThis part will focus on the DC_Metro_Scorecard data, which counts the reliability and efficiency of DC Metro from 2014 to 2016.\n\n\nCode\nDC_metro &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/DC_Metro_Scorecard.xlsx\", sheet=\"Sheet1\")\n\n# Select (or throwing away) unwanted columns for Data Exploration\nDC_metro &lt;- DC_metro%&gt;% select (-\"Crimes Target\", -\"Employee Injury Rate Target\", -\"Customer Injury Rate Target\", -\"Elevator Reliability\", -\"Elevator Reliability Target\", -\"Escalator Reliability Target\",\n-\"Rail Fleet Reliability Target\", -\"Bus On-Time Performance Target\", -\"Bus Fleet Reliability Target\", -\"Escalator Reliability\", -\"Rail On-Time Performance Target\")\n\n# Rename Columns\ncolnames(DC_metro) &lt;- c('Year','Month','Bus_on_time','Bus_fleet_reliability','Rail_fleet_reliability', 'Rail_on_time', 'Customer_injury_rate_per_1_Mill', 'Employee_injury_rate_per_200k_h', 'Crimes_per_1_Mill_passengers', 'Crimes_per_1_Mill_passengers')\n\n\nFirstly, we are going to see wherther the year and the month have a significant effect on the values seen in our data. This will help us understand if it has become better over the years or if there are months that have effects on the outcomes of public transportation due to weather or other circumstances.\n\n\nCode\nBus_on_time &lt;- DC_metro %&gt;% select('Year', 'Month', 'Bus_on_time')\n\nmodel &lt;- aov(Bus_on_time ~ Year + Month, data = Bus_on_time)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Bus_on_time\n          Df    Sum Sq    Mean Sq F value    Pr(&gt;F)    \nYear       2 0.0012562 0.00062809  7.6584 0.0031782 ** \nMonth     12 0.0056355 0.00046963  5.7263 0.0002625 ***\nResiduals 21 0.0017223 0.00008201                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the buses being on time.\n\n\nCode\nBus_fleet_reliability &lt;- DC_metro %&gt;% select('Year', 'Month', 'Bus_fleet_reliability')\n\nmodel &lt;- aov(Bus_fleet_reliability ~ Year + Month, data = Bus_fleet_reliability)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Bus_fleet_reliability\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nYear       2 5604536 2802268  9.3323 0.001259 **\nMonth     12 9091418  757618  2.5231 0.030574 * \nResiduals 21 6305777  300275                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the bus fleet reliability (miles travelled before a breakdown).\n\n\nCode\nRail_fleet_reliability &lt;- DC_metro %&gt;% select('Year', 'Month', 'Rail_fleet_reliability')\n\nmodel &lt;- aov(Rail_fleet_reliability ~ Year + Month, data = Rail_fleet_reliability)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Rail_fleet_reliability\n          Df     Sum Sq   Mean Sq F value  Pr(&gt;F)  \nYear       2  536503502 268251751  2.8728 0.07892 .\nMonth     12 2480920803 206743400  2.2141 0.05343 .\nResiduals 21 1960928969  93377570                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case, year and month don’t seem to have a significant effect on rail transportation fleet reliability (miles travelled before a breakdown).\n\n\nCode\nRail_on_time &lt;- DC_metro %&gt;% select('Year', 'Month', 'Rail_on_time')\n\nmodel &lt;- aov(Rail_on_time ~ Year + Month, data = Rail_on_time)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Rail_on_time\n          Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    \nYear       2 0.060978 0.0304888 52.2126 7.084e-09 ***\nMonth     12 0.020835 0.0017363  2.9734   0.01397 *  \nResiduals 21 0.012263 0.0005839                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the rail transportation being on time.\n\n\nCode\nCustomer_injury_rate_per_1_Mill &lt;- DC_metro %&gt;% select('Year', 'Month', 'Customer_injury_rate_per_1_Mill')\n\nmodel &lt;- aov(Customer_injury_rate_per_1_Mill ~ Year + Month, data = Customer_injury_rate_per_1_Mill)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Customer_injury_rate_per_1_Mill\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nYear       2  0.1121 0.05606  0.1770 0.83903  \nMonth     12 11.8039 0.98366  3.1056 0.01119 *\nResiduals 21  6.6515 0.31674                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOnly the month seems to have a significant effect on the customer injury rate (which is expected due to weather). However, if year does not, have we taken any necessary actions to improve the security on public transportation?\n\n\nCode\nEmployee_injury_rate_per_200k_h &lt;- DC_metro %&gt;% select('Year', 'Month', 'Employee_injury_rate_per_200k_h')\n\nmodel &lt;- aov(Employee_injury_rate_per_200k_h ~ Year + Month, data = Employee_injury_rate_per_200k_h)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Employee_injury_rate_per_200k_h\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nYear       2 11.822  5.9109  9.0165 0.00149 **\nMonth     12 12.759  1.0632  1.6218 0.16025   \nResiduals 21 13.767  0.6556                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case as year has a significant effect on employee injury rates and months do not, we can assume that some actions have been taken during the years.\n\n\nCode\nCrimes_per_1_Mill_passengers &lt;- DC_metro %&gt;% select('Year', 'Month', 'Crimes_per_1_Mill_passengers')\n\nmodel &lt;- aov(Crimes_per_1_Mill_passengers ~ Year + Month, data = Crimes_per_1_Mill_passengers)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Crimes_per_1_Mill_passengers\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nYear       1  0.2313 0.23133  0.6465 0.44209  \nMonth     12 14.6777 1.22314  3.4182 0.03656 *\nResiduals  9  3.2205 0.35783                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOnly months seem to have a significant effect on crime rates produced on public transportation. We could use this information to decide what months of the year we should increase security and question if this effect is due to holidays or other important events.\n\n\nCode\n# preparing data to set a column in Date format in order to properly graph it\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Year\", str_replace, \"CY \", \"\")\nhead(DC_metro)\n\nDC_metro$Year &lt;- as.numeric(DC_metro$Year)\n\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jan\", \"01\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Feb\", \"02\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Mar\", \"03\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Apr\", \"04\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"May\", \"05\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jun\", \"06\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jul\", \"07\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Aug\", \"08\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Sep\", \"09\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Oct\", \"10\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Nov\", \"11\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Dec\", \"12\")\n\nDC_metro &lt;- DC_metro[order(DC_metro$Year, DC_metro$Month), ]\n# Drop YTD rows for month\nDC_metro &lt;- subset(DC_metro, DC_metro$Month!='YTD')\nhead(DC_metro)\n\nlibrary(zoo)\n\nDC_metro$Date &lt;- as.yearmon(paste(DC_metro$Month, DC_metro$Year, sep = \" \"), format = \"%m %Y\")\n\n\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Bus_on_time)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Bus on Time\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nWhile the spread of the points is very wide, it is concerning to see a trend in which the Buses seem to be more and more delayed over time. That means that we are not taking the necessary steps to improve it.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Bus_fleet_reliability)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Fleet Reliability\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nFleet reliability has improved over time (even though the spread of the data is too wide). That is probably due to vehicle improvements.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Rail_fleet_reliability)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Rail Fleet reliability\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nThere seems to not be a clear improvement on the fleet reliability of rail transportation over this period of time.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Rail_on_time)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Rail on time performance\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nRail transportation seems to clearly have been getting worse over time. This problem is concerning and should be tackled as soon as possible. Is it due to safety reasons, investment problems, or poor planification?\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Customer_injury_rate_per_1_Mill)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Customer injuries per 1 Million passangers\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nCustomer injuries seem to have been stable overtime (with a couple of outliers that could be interesting to look for).\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Employee_injury_rate_per_200k_h)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Employee injuries per 200k h\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nThe employee injuries have been increasing over time. This means that if the trend keeps following this pattern, we should probably invest more in safety and take some more precautions for the employees.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Crimes_per_1_Mill_passengers)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Crimes per 1 Million passangers\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nThe crime rates have been increasing over time. This means that if the trend keeps following this pattern, we should probably invest in more security on public transportation."
  },
  {
    "objectID": "EDA.html#text-data",
    "href": "EDA.html#text-data",
    "title": "Exploring Data",
    "section": "Text Data",
    "text": "Text Data\nIn order to see what what are the most important concerns regarding public transportation to the users, we are going top explore what people mention the most in their reddits about public transportation through a word cloud.\n\n\nCode\nimport re\nimport spacy.lang.en.stop_words as stopwords\nimport spacy\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# read in the data\ndf=pd.read_json(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/sentiment_results.json\")\nprint(df.shape)\nprint(df.columns)\n\n# throw away all empty strings of text from df along with their url and sentiment_score\ndf = df[df.text != '']\nprint(df.shape)\nprint(df.columns)\n\ntexts = []\ny = []\n#https://spacy.io/usage/models\n\n\nparser = spacy.load('en_core_web_sm')\nstop_words = stopwords.STOP_WORDS\n\n\n# Iterate over rows\nfor i in range(df.shape[0]):\n    # QUICKLY CLEAN TEXT\n    keep = \"abcdefghijklmnopqrstuvwxyz \"\n    replace = \".,!;\"\n    tmp = \"\"\n    text_value = df[\"text\"].iloc[i]  # Accessing the \"text\" column using .iloc\n\n\n    text_value = re.sub('[^a-zA-Z ]+', '', text_value.replace(\"&lt;br /&gt;\", \"\").lower())\n    text_value = parser(text_value)\n    tokens = [token.lower_ for token in text_value]\n    #tokens = [token for token in text_value if token not in stop_words]\n    tokens = [token.lemma_ for token in text_value if token not in stop_words]\n\n\n    tmp = \" \".join(tokens)\n    texts.append(tmp)\nwordcloud_text = \" \".join(texts)\n\n\n\n\nCode\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\nworldcloud_text = \" \".join(texts)\n\n# Words to remove\nwords_to_remove = [\"one\", \"go\", \"even\", \"give\", \"will\", \"need\", \"say\", \"well\", \"still\", \"make\", \"think\", \"look\", \"etc\", \"actually\", \"yet\", \"put\"]\n\n# Remove specified words from the result\nfor word in words_to_remove:\n    wordcloud_text = wordcloud_text.replace(word, \"\")\n\n\n\ngenerate_word_cloud(wordcloud_text)\n\n\n\n\n\nFrom this world loud, we can see what topics are the most talked about by people regarding publioc transportation. This can help us see what matters to them the most and focus on these topics."
  }
]