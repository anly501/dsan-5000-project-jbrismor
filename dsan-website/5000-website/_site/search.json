[
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Code\nimport pandas as pd\ndf=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv')\nCode\nfrom sklearn.preprocessing import StandardScaler\n\n# normalize every column of the dataframe df\nscaler = StandardScaler()\ndf_scaled = (scaler.fit_transform(df))\nCode\ndf_scaled=pd.DataFrame(df_scaled)\ndf_scaled.shape\nCode\n# Libraries for clustering\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import MeanShift\nfrom sklearn.cluster import Birch\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import pairwise_distances, silhouette_score\nimport pandas as pd"
  },
  {
    "objectID": "Clustering.html#k-means",
    "href": "Clustering.html#k-means",
    "title": "Clustering",
    "section": "k-means++",
    "text": "k-means++\n\n\nCode\ninertia_values = []\ndistortion_values = []\n\nfor i in range(1, 40):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(df_scaled)\n    inertia_values.append(kmeans.inertia_)\n    cluster_assignments = kmeans.predict(df_scaled)\n    centers = kmeans.cluster_centers_\n    distortion = sum(np.min(pairwise_distances(df_scaled, centers, metric='euclidean'), axis=1)) / df_scaled.shape[0]\n    distortion_values.append(distortion)\n\nkmeans_df = pd.DataFrame({'Clusters': range(1, 40), 'Distortion': distortion_values, 'Inertia': inertia_values})\n\nprint(kmeans_df)\n\n\n    Clusters  Distortion       Inertia\n0          1    5.947283  1.567254e+07\n1          2    5.442291  1.438002e+07\n2          3    5.387453  1.340551e+07\n3          4    5.209756  1.281327e+07\n4          5    5.205517  1.225636e+07\n5          6    5.000645  1.179125e+07\n6          7    5.053200  1.123879e+07\n7          8    4.990364  1.076965e+07\n8          9    4.980376  1.031742e+07\n9         10    4.931528  1.005151e+07\n10        11    4.854808  9.776845e+06\n11        12    4.747632  9.569428e+06\n12        13    4.751885  9.344415e+06\n13        14    4.720352  9.081983e+06\n14        15    4.683612  8.926758e+06\n15        16    4.628865  8.750473e+06\n16        17    4.572890  8.585398e+06\n17        18    4.568782  8.337635e+06\n18        19    4.579223  8.198614e+06\n19        20    4.585187  8.046608e+06\n20        21    4.466414  7.944140e+06\n21        22    4.494016  7.737475e+06\n22        23    4.451415  7.612116e+06\n23        24    4.490546  7.429404e+06\n24        25    4.380493  7.350393e+06\n25        26    4.356751  7.252953e+06\n26        27    4.337594  7.153619e+06\n27        28    4.355580  6.995440e+06\n28        29    4.290759  6.942684e+06\n29        30    4.331594  6.807912e+06\n30        31    4.262690  6.690472e+06\n31        32    4.226878  6.649918e+06\n32        33    4.226878  6.591574e+06\n33        34    4.236690  6.487489e+06\n34        35    4.256159  6.387659e+06\n35        36    4.213076  6.301272e+06\n36        37    4.191339  6.263803e+06\n37        38    4.149187  6.213005e+06\n38        39    4.135633  6.159273e+06\n\n\n\n\nCode\n# plot distortion and inertia for kmeans, you can either plot them seperately or use fig, ax = plt.subplots(1, 2) to plot them in the same figure. Suggest the optimal number of clusters based on the plot.\nfig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\n# Plot Distortion\naxs[0].plot(kmeans_df['Clusters'], kmeans_df['Distortion'], marker='', linestyle='-', color='lightgreen')\naxs[0].set_title('Elbow Method using Distortion')\naxs[0].set_ylabel('Distortion')\naxs[0].set_xlabel('Number of clusters')\naxs[0].set_xticks([])\n\n# Plot Inertia\naxs[1].plot(kmeans_df['Clusters'], kmeans_df['Inertia'], marker='', linestyle='-', color='orange')\naxs[1].set_title('Elbow Method using Inertia')\naxs[1].set_ylabel('Inertia')\naxs[1].set_xlabel('Number of clusters')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe biggest inertia score is set at 11 cluster. Thus, we will pick 11 clusters for this data using k_means++ and compare it to a few other methods.\n\n\nCode\n#give me the shape of df_norm\nprint(df_norm.shape)\n\n\n(220740, 71)"
  },
  {
    "objectID": "Clustering.html#mean-shift-on-dimensionality-reduced-data",
    "href": "Clustering.html#mean-shift-on-dimensionality-reduced-data",
    "title": "Clustering",
    "section": "Mean shift on dimensionality reduced data",
    "text": "Mean shift on dimensionality reduced data\n\n\nCode\nscores_pca_df=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv')\n\n\n\n\nCode\nsamples = scores_pca_df.sample(n=5000, random_state=1)\n\n\n\n\nCode\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\n\nhierarchical_cluster = AgglomerativeClustering(n_clusters=11, affinity='euclidean', linkage='ward') #chose 3 as that is the number of species. We could have changed it.\nlabels = hierarchical_cluster.fit_predict(samples)\nprint(\"Cluster Labels total:\")\nprint(list(set(labels)))\n\n\n/Users/jorgebrismoreno/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_agglomerative.py:1005: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n\n\nCluster Labels total:\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n\nCode\nscores_pca_df=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv')\n\nsamples = scores_pca_df.sample(n=10000, random_state=1)\n\n\n\n\nCode\nfrom sklearn.metrics import silhouette_score\nimport random\n\nrandom.seed(42)\nbest_scores = []\nepsilons = []\nclusters = []\nz = [i / 10 for i in range(5, 20)]\nz2 = range(2, 10)\n\nfor i in z:\n    max_score = -1\n    best_cluster = -1\n    best_eps = -1\n    for j in z2:\n        model = DBSCAN(eps=i, min_samples=j)\n        predics = model.fit_predict(samples)\n        num_clusters = len(pd.Series(predics).unique())\n        if num_clusters &gt; 1:\n            score = silhouette_score(samples, predics)\n            if score &gt; max_score:\n                max_score = score\n                best_cluster = num_clusters\n                best_eps = i\n\n    best_scores.append(max_score)\n    clusters.append(best_cluster)\n    epsilons.append(best_eps)\n\ndb = pd.DataFrame({'Epsilons': epsilons, 'Best_Clusters': clusters, 'Best_Silhouette': best_scores})\nprint(db.sort_values(by=\"Best_Silhouette\", ascending=False))\n\nsns.lineplot(data=db, x='Clusters',y='Silhouette')\nplt.show()\n\n\n\n\nCode\n# Perform MeanShift Clustering and predict number \nfrom sklearn.cluster import estimate_bandwidth\nbandwidth = estimate_bandwidth(samples)\n\nclustering_meanshift = MeanShift(bandwidth=bandwidth, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)\nclustering_meanshift.fit(samples)\npredics = clustering_meanshift.predict(samples)\nprint('Number of estimated clusters : ', len(np.unique(predics)))"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "What is the future of Public Transportation?",
    "section": "",
    "text": "Public transportation is any method of transportation available to the public that is sponsored by governmental entities. In some countries, it is the most used method of transportation while, in others, it is almost inexistent. What are the benefits of having public transportation? Should governmental entities invest on it? What are the preferred methods of transportation? These are just a few questions that every government asks themselves. The public transportation insdustry is one of the biggest lines of business worldwide, with trillions of everyday users. It is so important that cities plan their growth around it in order to provide equal access for everyone. Not only the methods of transport are taken into account, but all the infrastructure surrounding it such as freeways, roads, tunnels, etc. This infrastructure ensures social, economical, and environmental sustainability on the city (Murray et al. 1998)."
  },
  {
    "objectID": "Introduction.html#summary",
    "href": "Introduction.html#summary",
    "title": "What is the future of Public Transportation?",
    "section": "",
    "text": "Public transportation is any method of transportation available to the public that is sponsored by governmental entities. In some countries, it is the most used method of transportation while, in others, it is almost inexistent. What are the benefits of having public transportation? Should governmental entities invest on it? What are the preferred methods of transportation? These are just a few questions that every government asks themselves. The public transportation insdustry is one of the biggest lines of business worldwide, with trillions of everyday users. It is so important that cities plan their growth around it in order to provide equal access for everyone. Not only the methods of transport are taken into account, but all the infrastructure surrounding it such as freeways, roads, tunnels, etc. This infrastructure ensures social, economical, and environmental sustainability on the city (Murray et al. 1998)."
  },
  {
    "objectID": "Introduction.html#motivation",
    "href": "Introduction.html#motivation",
    "title": "What is the future of Public Transportation?",
    "section": "Motivation",
    "text": "Motivation\nI believe that the use of public transportation has an uncountable number of benefits. Growing up with many family members working for the public transportation sector in Madrid (Spain), I have developed a strong sense of value for this industry. While many variables and many different (sometimes opposite) interests need to be taken into account, public transport has become essential for our society. Millions of people depend on it to carry out essential activities such as shopping, going to work, or going to do other daily chores. For this reason, it is an industry that deserves severe research and development. Furthermore, due to COVID-19, the number of riders of public transportation have decreased, and there is an urgent need to bring them back. This will allow cities to be more socially sustainable and reduce the polution levels (UITP 2023).\nThe goal of this research is to make a case about the worth of having public transportation around the world and make recommendations for future steps that should be taken in this field."
  },
  {
    "objectID": "Introduction.html#questions",
    "href": "Introduction.html#questions",
    "title": "What is the future of Public Transportation?",
    "section": "Questions",
    "text": "Questions\n\nHow does public transport affect pollution?\nWhat are the most pressing issues related to public transport?\nHow will intelligent (autonomous vehicles) transportation affect the job market?\nHow does public transportation affect the economy and development of a city?\nTo what extent does public transportation reduce the usage of private transportation?\nWhich are the financial support structures for public transportation and how did new global laws about pollution affect them?\nWhat is the relationship between public transportation and tourism?\nWhat is the future of public transportation security measures?\nWhat other type of eneergy (aside from gas) can public transportation use and how these alternatives affect pollution levels?\nHow does the travel time in public transportation affect the decision process about using a private tranport method?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jorge Bris Moreno",
    "section": "",
    "text": "Jorge Bris Moreno is a Master’s student of the Data Science and Analytics program at Georgetown University. In his undergraduate studies, he majored in Mathematics and Business from Franklin & Marshall College. His most relevant experiences have been in project management and business planning."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jorge Bris Moreno",
    "section": "",
    "text": "Jorge Bris Moreno is a Master’s student of the Data Science and Analytics program at Georgetown University. In his undergraduate studies, he majored in Mathematics and Business from Franklin & Marshall College. His most relevant experiences have been in project management and business planning."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jorge Bris Moreno",
    "section": "Education",
    "text": "Education\nGeorgetown University | Washington, DC\n\nM.S in Data Science & Analytics | Jun 2023 - May 2025\nFranklin & Marshall College | Lancaster, PA\n\nB.A in Mathematics and Business | Aug 2019 - May 2023"
  },
  {
    "objectID": "index.html#relevant-experience",
    "href": "index.html#relevant-experience",
    "title": "Jorge Bris Moreno",
    "section": "Relevant Experience",
    "text": "Relevant Experience\nStatistician | PhD student’s research colaborator\n\nNov 2022 - Present\nClient Engagement & Resource Assistant | ASSETS\n\nSept 2022 - May 2023\nProject Manager Intern | Miller Mats\n\nMay 2022 - Aug 2022"
  },
  {
    "objectID": "Data_Gathering.html",
    "href": "Data_Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "All data files, whether generated using code or directly downloaded from the source can be found and downloaded in the data tab of the website."
  },
  {
    "objectID": "Data_Gathering.html#apis",
    "href": "Data_Gathering.html#apis",
    "title": "Data Gathering",
    "section": "APIs",
    "text": "APIs\n\nReddit API\nThe following code extracted reddit urls from reddit posts about Public Transportation and saved them to a .json.\nlibrary(RedditExtractoR)\nlibrary(jsonlite)\n\ntop_Pub_Transp_urls &lt;- find_thread_urls(keywords=\"public transportation\")\njsonlite::write_json(top_Pub_Transp_urls, \"top_pub_transp_urls.json\")\nThen, the following code extracted the content of those reddit posts and performed sentiment analysis on them, generated a data frame, and saved it to a .csv file\n\n\nCode\nimport pandas as pd\nimport json\n\n# Load the sentiment scores from the JSON file\nwith open('sentiment_scores.json', 'r') as json_file:\n    sentiment_scores = json.load(json_file)\n\n# Initialize lists to store data\nids = []\nneg_scores = []\nneu_scores = []\npos_scores = []\ncompound_scores = []\n\n# Extract the scores and create separate lists for each\nfor idx, item in enumerate(sentiment_scores, start=1):\n    ids.append(idx)\n    sentiment_score = item.get('sentiment_score', {})\n    neg_scores.append(sentiment_score.get('neg', 0))\n    neu_scores.append(sentiment_score.get('neu', 0))\n    pos_scores.append(sentiment_score.get('pos', 0))\n    compound_scores.append(sentiment_score.get('compound', 0))\n\n# Create a DataFrame\ndata = {\n    'ID': ids,\n    'Negative Score': neg_scores,\n    'Neutral Score': neu_scores,\n    'Positive Score': pos_scores,\n    'Compound Score': compound_scores\n}\n\ndf = pd.DataFrame(data)\n\n# Save to CSV\ndf.to_csv('sentiment_scores.csv', index=False)\n\n\nThe final data’s first few rows look like this:\n\n\n\n\n\nHowever, the text data has also been kept in a data file with another code so that later analysis can be performed on it.\n\n\nAPI for cityofchicago.org\nThe following code extracted the following data frame about buses information and saved it into a csv file.\n\n\nCode\nimport pandas as pd\nfrom sodapy import Socrata\n\nclient = Socrata(\"data.cityofchicago.org\", None)\n\nresults = client.get(\"bynn-gwxy\", limit=2000)\n\n# Convert to pandas DataFrame\nresults_df = pd.DataFrame.from_records(results)\n\n# Save to CSV\nresults_df.to_csv('Chicago_avg_Buses.csv')\n\n\nThis is a snapshot of the data:"
  },
  {
    "objectID": "Data_Gathering.html#data-from-bts.gov",
    "href": "Data_Gathering.html#data-from-bts.gov",
    "title": "Data Gathering",
    "section": "Data from bts.gov",
    "text": "Data from bts.gov\nThe files: bus_consumption_fuel_by_year.xlsx, energy_consumed_byMill_passenger_MILES.xlsx, fatalities_bus_over_time.xlsx, National_transport_usage_linked_Economic_trend.xlsx, and vehicle_production_countries.xlsx are downloaded from: https://www.bts.gov.\nThese are some snapshots of how the data looks like:"
  },
  {
    "objectID": "Data_Gathering.html#data-from-international-transport-forum",
    "href": "Data_Gathering.html#data-from-international-transport-forum",
    "title": "Data Gathering",
    "section": "Data from International Transport Forum",
    "text": "Data from International Transport Forum\nThe file Value_transport_by_countries.csv was downloaded from: https://stats.oecd.org/Index.aspx?DataSetCode=ITF_PASSENGER_TRANSPORT\nThe data on that file looks like this:"
  },
  {
    "objectID": "Data_Gathering.html#data-from-data.world",
    "href": "Data_Gathering.html#data-from-data.world",
    "title": "Data Gathering",
    "section": "Data from data.world",
    "text": "Data from data.world\nThe file DC_Metro_Scorecard.xlsx and the zip folders: Walkable_distance_to_PubTrans, and capmetro_smart_trips_questionaire zip folders contain data that was downloaded from data.world. These are the three links where the data was downloaded from(in order of mention):\n\nhttps://data.world/makeovermonday/2016w51\nhttps://data.world/chhs/5e391154-f07d-4e0c-ab1b-687a0c4c5d06\nhttps://data.world/browning/capmetro-smart-trips-questionaire\n\nThe data looks like this (in order of mention):"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploring Data",
    "section": "",
    "text": "The goals of this tab is to gain a deeper understanding of the data, how variables behave, and identify anomalies, paterns, or potential insights for future exploration."
  },
  {
    "objectID": "EDA.html#eda-goals",
    "href": "EDA.html#eda-goals",
    "title": "Exploring Data",
    "section": "",
    "text": "The goals of this tab is to gain a deeper understanding of the data, how variables behave, and identify anomalies, paterns, or potential insights for future exploration."
  },
  {
    "objectID": "EDA.html#vehicle-production-by-countries",
    "href": "EDA.html#vehicle-production-by-countries",
    "title": "Exploring Data",
    "section": "Vehicle Production by countries",
    "text": "Vehicle Production by countries\nHere, we will be exploring the vehicle_production_countries data set.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(boot)\nlibrary(AICcmodavg)\nlibrary(readxl)\nlibrary(broom)\n\nvehicle_production_countries_comercial &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/vehicle_production_countries.xlsx\", sheet=\"Comercial-vehicles\", skip = 1)\n\n# Replace \"N\", \"U\", and \"Z\" with NULL in all columns except the first one\nvehicle_production_countries_comercial[-1] &lt;- lapply(vehicle_production_countries_comercial[-1], function(x) ifelse(x %in% c(\"N\", \"U\", \"Z\"), NA, x))\n\nvehicle_production_countries_comercial &lt;- vehicle_production_countries_comercial %&gt;%\n  mutate(across(-1, as.numeric))\n\ncolnames(vehicle_production_countries_comercial)[colnames(vehicle_production_countries_comercial) == \"...1\"] &lt;- \"Country\"\ncolnames(vehicle_production_countries_comercial)\n\n# Transpose the data frame\nvehicle_production_countries_comercial_transposed &lt;- as.data.frame(t(vehicle_production_countries_comercial))\n\n# Set the first row as the column names\ncolnames(vehicle_production_countries_comercial_transposed) &lt;- vehicle_production_countries_comercial_transposed[1, ]\n\n# Remove the first row\nvehicle_production_countries_comercial_transposed &lt;- vehicle_production_countries_comercial_transposed[-1, ]\n\n# Convert the year columns to numeric\nvehicle_production_countries_comercial_transposed[] &lt;- lapply(vehicle_production_countries_comercial_transposed, as.numeric)\n\nvehicle_production_countries_comercial_transposed &lt;- vehicle_production_countries_comercial_transposed[, !colnames(vehicle_production_countries_comercial_transposed) %in% \"Total world\"]\n\ndf &lt;- vehicle_production_countries_comercial_transposed %&gt;% \n  rownames_to_column(var = \"Year\")\n\n# Replace \"(R) 2019\" with 2019 and \"(R) 2020\" with 2020\ndf &lt;- df %&gt;%\n  mutate(Year = ifelse(Year == \"(R) 2019\", 2019, ifelse(Year == \"(R) 2020\", 2020, Year)))\n\n\n\n\nCode\n# Pivot the data to long format\ndf_long &lt;- df %&gt;%\n  pivot_longer(-Year, names_to = \"Country\", values_to = \"Value\")\n\n# Plotlot\nggplot(df_long, aes(x = Year, y = Value, color = Country)) +\n  geom_point() +\n  labs(\n    title = \"Value per year and per country in comercial vehicle production\",\n    x = \"Year\",\n    y = \"Value of comercial vehicles produced\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", axis.text.x = element_text(angle = 90, hjust = 1))\n\n\nWarning message:\n\"Removed 188 rows containing missing values (geom_point).\"\n\n\n\n\n\nFrom this plot we can see how Japan (back in the 1960s) used to be one of the biggest producers but not anymore. The US has been in the lead since the 1990s until around 2008, which was expected due to the recession. However, it is impressive how China was able to excel during the recession in this market at an almost exponential rate. And it still has kept increasing since then.\nIn order to run an ANOVA test and look if countries, years, and interaction among the both have an statistical significance on comercial vehicle production, we are going to assume that all the missing values are not relevant to the contribution of the overall market and, therefore, close to 0. Thuis, for the sake of this process, we will make them 0.\n\n\nCode\n# Replace NA values with 0\nvehicle_production_countries_comercial &lt;- replace(vehicle_production_countries_comercial, is.na(vehicle_production_countries_comercial), 0)\n\n\n\n\nCode\ndata_long &lt;- vehicle_production_countries_comercial %&gt;%\n  pivot_longer(cols = -Country, names_to = \"Year\", values_to = \"Value\")\n\ndata_long &lt;- data_long[data_long$Country != \"Total world\", ]\n# Perform ANOVA\nmodel &lt;- aov(Value ~ Year * Country, data = data_long)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\n\nWarning message in anova.lm(object):\n\"ANOVA F-tests on an essentially perfect fit are unreliable\"\n\n\nAnalysis of Variance Table\n\nResponse: Value\n              Df     Sum Sq  Mean Sq F value Pr(&gt;F)\nYear          31   85531235  2759072     NaN    NaN\nCountry       32 2172668273 67895884     NaN    NaN\nYear:Country 992 1139993857  1149187     NaN    NaN\nResiduals      0          0      NaN               \n\n\nSeing this, we will simplify our model, taking away the interaction (in order to no overfit the data with our ANOVA model).\n\n\nCode\nmodel &lt;- aov(Value ~ Year + Country, data = data_long)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Value\n           Df     Sum Sq  Mean Sq F value    Pr(&gt;F)    \nYear       31   85531235  2759072  2.4009 3.244e-05 ***\nCountry    32 2172668273 67895884 59.0816 &lt; 2.2e-16 ***\nResiduals 992 1139993857  1149187                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth have a significant effect (which was expected). However, this helped us also understand that their interaction would overfit the model in this case."
  },
  {
    "objectID": "EDA.html#energy-and-usage-of-buses",
    "href": "EDA.html#energy-and-usage-of-buses",
    "title": "Exploring Data",
    "section": "Energy and usage of Buses",
    "text": "Energy and usage of Buses\nIn this part, we will be exploring the energy_consumed_byMill_passenger_MILES file.\n\n\nCode\nenergy_consum &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/energy_consumed_byMill_passenger_MILES.xlsx\", sheet=\"Energy\")\n\ncolnames(energy_consum) &lt;- energy_consum[1, ]\n\n# Remove the first row and the first column\nenergy_consum &lt;- energy_consum[-1, ]\n\n# Set the first column to \"Data_to_Explore\"\ncolnames(energy_consum)[1] &lt;- \"Data_to_Explore\"\n\n# Filter rows based on specific criteria\nenergy_consum &lt;- energy_consum[energy_consum$Data_to_Explore %in% c(\n  \"Vehicle-miles (millions)\",\n  \"Passenger-miles (millions)\",\n  \"Energy consumed, total (billion Btu)\",\n  \"Energy intensity (Btu/passenger-mile)\"\n), ]\n\n# Replace all \"N\" values (except in the first column) with NA\nenergy_consum[, -1] &lt;- apply(energy_consum[, -1], 2, function(x) ifelse(x == \"N\", NA, x))\n\n# Drop specified columns because of numerous NA\nenergy_consum &lt;- energy_consum %&gt;% select(-\"1960\", -\"1965\", -\"1970\", -\"1975\", -\"1980\", -\"1985\", -\"1990\", -\"1991\", -\"1992\", -\"1993\", -\"1994\", -\"1995\")\n\n# Change columns from character to numeric (except the first column)\nenergy_consum &lt;- energy_consum %&gt;%\n  mutate_at(vars(-1), as.numeric)\n\nenergy_consum_long &lt;-as.data.frame(t(energy_consum), index=False)\n\ncolnames(energy_consum_long) &lt;- energy_consum_long[1, ]\n\n# Remove the first row and the first column\nenergy_consum_long &lt;- energy_consum_long[-1, ]\n\ncolnames(energy_consum_long) &lt;- c(\"Vehicle_milesMill\", \"Passenger_miles_mill\",\"Energy_consumed_total_bill_Btu\", \"Energy_intensity_Btu_passenger_mile\")\n\nYears = c(1996:2021)\nenergy_consum_long['Years'] &lt;- Years\n\n# data frame used for the models\nenergy_consum_long_no_Covid &lt;- energy_consum_long[!(energy_consum_long$Years %in% c(2020, 2021)), ]\n\n\nFor the fitted model in the following plots, year 2020 and 2021 are going to be ignored and considered outliers due to COVID-19. They are heavy outliers.\n\n\nCode\n#plot(energy_consum_long$Energy_intensity_Btu_passenger_mile)\n\n# Fit a linear model\nlm_model &lt;- lm(energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile ~ energy_consum_long_no_Covid$Year)\n\n# Create the plot\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile,\n     xlab = \"Year\",\n     ylab = \"Energy consumed, total (billion Btu)\",\n     col = \"blue3\",\n     pch = 19)\n\n# Calculate confidence interval\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\n# Add confidence interval lines\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\n# Shade the area between confidence interval bounds\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\n# Add the trend line\nabline(lm_model, col = \"red\")\n\n# Add the points\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile, col = \"blue3\", pch = 19)\n\n\n\n\n\nFrom this plot we can see how the total energy consumed (in billion Btu) has been constantly decreasing. However, is it because our vehicles are more efficient or due to cuts in public transportation?\n\n\nCode\n#plot(energy_consum_long$Passenger_miles_mill)\n\n# Fit a linear model\nlm_model &lt;- lm(energy_consum_long_no_Covid$Passenger_miles_mill ~ energy_consum_long_no_Covid$Year)\n\n# Create the plot\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Passenger_miles_mill,\n     xlab = \"Year\",\n     ylab = \"Passenger-miles (millions)\",\n     col = \"blue3\",\n     pch = 19)\n\n# Calculate confidence interval\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\n# Add confidence interval lines\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\n# Shade the area between confidence interval bounds\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\n# Add the trend line\nabline(lm_model, col = \"red\")\n\n# Add the points\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Passenger_miles_mill, col = \"blue3\", pch = 19)\n\n\n\n\n\nWhile the fitted model shows an increase in the usage of public transportation, we can see that it has been constantly dropping since around 2014. This can bring up many questions such as: Is it due to an investment problem? Is it because there are not enough incentives to use public transportation? Do people own more cars?\n\n\nCode\n#plot(energy_consum_long$Energy_intensity_Btu_passenger_mile)\n\n# Fit a linear model\nlm_model &lt;- lm(energy_consum_long_no_Covid$Vehicle_milesMill ~ energy_consum_long_no_Covid$Year)\n\n# Create the plot\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Vehicle_milesMill,\n     xlab = \"Year\",\n     ylab = \"Vehicle miles (millions)\",\n     col = \"blue3\",\n     pch = 19)\n\n# Calculate confidence interval\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\n# Add confidence interval lines\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\n# Shade the area between confidence interval bounds\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\n# Add the trend line\nabline(lm_model, col = \"red\")\n\n# Add the points\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Vehicle_milesMill, col = \"blue3\", pch = 19)\n\n\n\n\n\nThe vehicle miles have been constantly increasing, which means that there have been more and more routes added over time. However, why has this not been enough to increase the demand of public transportation?"
  },
  {
    "objectID": "EDA.html#dc-metro-scorecard",
    "href": "EDA.html#dc-metro-scorecard",
    "title": "Exploring Data",
    "section": "DC Metro Scorecard",
    "text": "DC Metro Scorecard\nThis part will focus on the DC_Metro_Scorecard data, which counts the reliability and efficiency of DC Metro from 2014 to 2016.\n\n\nCode\nDC_metro &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/DC_Metro_Scorecard.xlsx\", sheet=\"Sheet1\")\n\n# Select (or throwing away) unwanted columns for Data Exploration\nDC_metro &lt;- DC_metro%&gt;% select (-\"Crimes Target\", -\"Employee Injury Rate Target\", -\"Customer Injury Rate Target\", -\"Elevator Reliability\", -\"Elevator Reliability Target\", -\"Escalator Reliability Target\",\n-\"Rail Fleet Reliability Target\", -\"Bus On-Time Performance Target\", -\"Bus Fleet Reliability Target\", -\"Escalator Reliability\", -\"Rail On-Time Performance Target\")\n\n# Rename Columns\ncolnames(DC_metro) &lt;- c('Year','Month','Bus_on_time','Bus_fleet_reliability','Rail_fleet_reliability', 'Rail_on_time', 'Customer_injury_rate_per_1_Mill', 'Employee_injury_rate_per_200k_h', 'Crimes_per_1_Mill_passengers', 'Crimes_per_1_Mill_passengers')\n\n\nFirstly, we are going to see wherther the year and the month have a significant effect on the values seen in our data. This will help us understand if it has become better over the years or if there are months that have effects on the outcomes of public transportation due to weather or other circumstances.\n\n\nCode\nBus_on_time &lt;- DC_metro %&gt;% select('Year', 'Month', 'Bus_on_time')\n\nmodel &lt;- aov(Bus_on_time ~ Year + Month, data = Bus_on_time)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Bus_on_time\n          Df    Sum Sq    Mean Sq F value    Pr(&gt;F)    \nYear       2 0.0012562 0.00062809  7.6584 0.0031782 ** \nMonth     12 0.0056355 0.00046963  5.7263 0.0002625 ***\nResiduals 21 0.0017223 0.00008201                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the buses being on time.\n\n\nCode\nBus_fleet_reliability &lt;- DC_metro %&gt;% select('Year', 'Month', 'Bus_fleet_reliability')\n\nmodel &lt;- aov(Bus_fleet_reliability ~ Year + Month, data = Bus_fleet_reliability)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Bus_fleet_reliability\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nYear       2 5604536 2802268  9.3323 0.001259 **\nMonth     12 9091418  757618  2.5231 0.030574 * \nResiduals 21 6305777  300275                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the bus fleet reliability (miles travelled before a breakdown).\n\n\nCode\nRail_fleet_reliability &lt;- DC_metro %&gt;% select('Year', 'Month', 'Rail_fleet_reliability')\n\nmodel &lt;- aov(Rail_fleet_reliability ~ Year + Month, data = Rail_fleet_reliability)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Rail_fleet_reliability\n          Df     Sum Sq   Mean Sq F value  Pr(&gt;F)  \nYear       2  536503502 268251751  2.8728 0.07892 .\nMonth     12 2480920803 206743400  2.2141 0.05343 .\nResiduals 21 1960928969  93377570                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case, year and month don’t seem to have a significant effect on rail transportation fleet reliability (miles travelled before a breakdown).\n\n\nCode\nRail_on_time &lt;- DC_metro %&gt;% select('Year', 'Month', 'Rail_on_time')\n\nmodel &lt;- aov(Rail_on_time ~ Year + Month, data = Rail_on_time)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Rail_on_time\n          Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    \nYear       2 0.060978 0.0304888 52.2126 7.084e-09 ***\nMonth     12 0.020835 0.0017363  2.9734   0.01397 *  \nResiduals 21 0.012263 0.0005839                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the rail transportation being on time.\n\n\nCode\nCustomer_injury_rate_per_1_Mill &lt;- DC_metro %&gt;% select('Year', 'Month', 'Customer_injury_rate_per_1_Mill')\n\nmodel &lt;- aov(Customer_injury_rate_per_1_Mill ~ Year + Month, data = Customer_injury_rate_per_1_Mill)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Customer_injury_rate_per_1_Mill\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nYear       2  0.1121 0.05606  0.1770 0.83903  \nMonth     12 11.8039 0.98366  3.1056 0.01119 *\nResiduals 21  6.6515 0.31674                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOnly the month seems to have a significant effect on the customer injury rate (which is expected due to weather). However, if year does not, have we taken any necessary actions to improve the security on public transportation?\n\n\nCode\nEmployee_injury_rate_per_200k_h &lt;- DC_metro %&gt;% select('Year', 'Month', 'Employee_injury_rate_per_200k_h')\n\nmodel &lt;- aov(Employee_injury_rate_per_200k_h ~ Year + Month, data = Employee_injury_rate_per_200k_h)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Employee_injury_rate_per_200k_h\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nYear       2 11.822  5.9109  9.0165 0.00149 **\nMonth     12 12.759  1.0632  1.6218 0.16025   \nResiduals 21 13.767  0.6556                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case as year has a significant effect on employee injury rates and months do not, we can assume that some actions have been taken during the years.\n\n\nCode\nCrimes_per_1_Mill_passengers &lt;- DC_metro %&gt;% select('Year', 'Month', 'Crimes_per_1_Mill_passengers')\n\nmodel &lt;- aov(Crimes_per_1_Mill_passengers ~ Year + Month, data = Crimes_per_1_Mill_passengers)\n\n# Summarize the ANOVA results with significance codes\nsummary_model &lt;- summary(model)\n\n# Print the ANOVA table with significance codes\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Crimes_per_1_Mill_passengers\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nYear       1  0.2313 0.23133  0.6465 0.44209  \nMonth     12 14.6777 1.22314  3.4182 0.03656 *\nResiduals  9  3.2205 0.35783                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOnly months seem to have a significant effect on crime rates produced on public transportation. We could use this information to decide what months of the year we should increase security and question if this effect is due to holidays or other important events.\n\n\nCode\n# preparing data to set a column in Date format in order to properly graph it\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Year\", str_replace, \"CY \", \"\")\nhead(DC_metro)\n\nDC_metro$Year &lt;- as.numeric(DC_metro$Year)\n\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jan\", \"01\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Feb\", \"02\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Mar\", \"03\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Apr\", \"04\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"May\", \"05\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jun\", \"06\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jul\", \"07\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Aug\", \"08\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Sep\", \"09\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Oct\", \"10\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Nov\", \"11\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Dec\", \"12\")\n\nDC_metro &lt;- DC_metro[order(DC_metro$Year, DC_metro$Month), ]\n# Drop YTD rows for month\nDC_metro &lt;- subset(DC_metro, DC_metro$Month!='YTD')\nhead(DC_metro)\n\nlibrary(zoo)\n\nDC_metro$Date &lt;- as.yearmon(paste(DC_metro$Month, DC_metro$Year, sep = \" \"), format = \"%m %Y\")\n\n\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Bus_on_time)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Bus on Time\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nWhile the spread of the points is very wide, it is concerning to see a trend in which the Buses seem to be more and more delayed over time. That means that we are not taking the necessary steps to improve it.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Bus_fleet_reliability)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Fleet Reliability\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nFleet reliability has improved over time (even though the spread of the data is too wide). That is probably due to vehicle improvements.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Rail_fleet_reliability)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Rail Fleet reliability\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nThere seems to not be a clear improvement on the fleet reliability of rail transportation over this period of time.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Rail_on_time)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Rail on time performance\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nRail transportation seems to clearly have been getting worse over time. This problem is concerning and should be tackled as soon as possible. Is it due to safety reasons, investment problems, or poor planification?\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Customer_injury_rate_per_1_Mill)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Customer injuries per 1 Million passangers\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nCustomer injuries seem to have been stable overtime (with a couple of outliers that could be interesting to look for).\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Employee_injury_rate_per_200k_h)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Employee injuries per 200k h\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nThe employee injuries have been increasing over time. This means that if the trend keeps following this pattern, we should probably invest more in safety and take some more precautions for the employees.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Crimes_per_1_Mill_passengers)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  # Add x-axis label\n  ylab(\"Crimes per 1 Million passangers\")  # Add y-axis label\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nThe crime rates have been increasing over time. This means that if the trend keeps following this pattern, we should probably invest in more security on public transportation."
  },
  {
    "objectID": "EDA.html#text-data",
    "href": "EDA.html#text-data",
    "title": "Exploring Data",
    "section": "Text Data",
    "text": "Text Data\nIn order to see what what are the most important concerns regarding public transportation to the users, we are going top explore what people mention the most in their reddits about public transportation through a word cloud.\n\n\nCode\nimport re\nimport spacy.lang.en.stop_words as stopwords\nimport spacy\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# read in the data\ndf=pd.read_json(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/sentiment_results.json\")\nprint(df.shape)\nprint(df.columns)\n\n# throw away all empty strings of text from df along with their url and sentiment_score\ndf = df[df.text != '']\nprint(df.shape)\nprint(df.columns)\n\ntexts = []\ny = []\n#https://spacy.io/usage/models\n\n\nparser = spacy.load('en_core_web_sm')\nstop_words = stopwords.STOP_WORDS\n\n\n# Iterate over rows\nfor i in range(df.shape[0]):\n    # QUICKLY CLEAN TEXT\n    keep = \"abcdefghijklmnopqrstuvwxyz \"\n    replace = \".,!;\"\n    tmp = \"\"\n    text_value = df[\"text\"].iloc[i]  # Accessing the \"text\" column using .iloc\n\n\n    text_value = re.sub('[^a-zA-Z ]+', '', text_value.replace(\"&lt;br /&gt;\", \"\").lower())\n    text_value = parser(text_value)\n    tokens = [token.lower_ for token in text_value]\n    #tokens = [token for token in text_value if token not in stop_words]\n    tokens = [token.lemma_ for token in text_value if token not in stop_words]\n\n\n    tmp = \" \".join(tokens)\n    texts.append(tmp)\nwordcloud_text = \" \".join(texts)\n\n\n\n\nCode\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n\n    def plot_cloud(wordcloud):\n        # Set figure size\n        plt.figure(figsize=(40, 30))\n        # Display image\n        plt.imshow(wordcloud) \n        # No axis details\n        plt.axis(\"off\");\n\n    # Generate word cloud\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\nworldcloud_text = \" \".join(texts)\n\n# Words to remove\nwords_to_remove = [\"one\", \"go\", \"even\", \"give\", \"will\", \"need\", \"say\", \"well\", \"still\", \"make\", \"think\", \"look\", \"etc\", \"actually\", \"yet\", \"put\"]\n\n# Remove specified words from the result\nfor word in words_to_remove:\n    wordcloud_text = wordcloud_text.replace(word, \"\")\n\n\n\ngenerate_word_cloud(wordcloud_text)\n\n\n\n\n\nFrom this world loud, we can see what topics are the most talked about by people regarding publioc transportation. This can help us see what matters to them the most and focus on these topics."
  },
  {
    "objectID": "Dimension_red.html",
    "href": "Dimension_red.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "The goal of this tab is to look closer at every variable in the EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv and see which variables explain the most variance in order to understand that data set better due to its large shape (71 columns). The columns that explain the most variance will be utilized for different methods of clustering due to computational limitations and to also visualize the clusterings calculated."
  },
  {
    "objectID": "Data_cleaning.html",
    "href": "Data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Final.csv'\n\ndf = pd.read_csv(file_path)\n\n# Drop unwanted columns\ndf.drop(columns=[\"OBJECTID\", \"GEOID10\", \"GEOID20\", \"STATEFP\", \"COUNTYFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\", \"CSA\", \"CSA_Name\", \"CBSA\", \"CBSA_Name\", \"CountHU\", \"HH\", \"D1A\", \"D1C8_OFF\", \"D4D\", \"D4E\", \"D2A_JPHH\", \"D2B_E5MIX\", \"D2B_E5MIXA\", \"D2B_E8MIX\",\"D2B_E8MIXA\",\"D2A_EPHHM\",\"D2C_TRPMX1\",\"D2C_TRPMX2\",\"D2C_TRIPEQ\",\"D2R_JOBPOP\",\"D2R_WRKEMP\",\"D2A_WRKEMP\",\"D2C_WREMLX\",\"D4A\",\"D4B025\",\"D4B050\",\"D4C\",\"D5AR\",\"D5AE\",\"D5BR\",\"D5BE\",\"D5CR\",\"D5CRI\",\"D5CE\",\"D5CEI\",\"D5DR\",\"D5DRI\",\"D5DE\",\"D5DEI\"], inplace=True)\n\n# count NA values\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# print all the columns with NA values\nprint(df.isna().sum())\n\n# Assuming your DataFrame is called 'new_df'\ndf.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv', index=False)\n\n\nCBSA_POP        0\nCBSA_EMP        0\nCBSA_WRK        0\nAc_Total        0\nAc_Water        0\nAc_Land         0\nAc_Unpr         0\nTotPop          0\nP_WrkAge        0\nAutoOwn0        0\nPct_AO0         0\nAutoOwn1        0\nPct_AO1         0\nAutoOwn2p       0\nPct_AO2p        0\nWorkers         0\nR_LowWageWk     0\nR_MedWageWk     0\nR_HiWageWk      0\nR_PCTLOWWAGE    0\nTotEmp          0\nE5_Ret          0\nE5_Off          0\nE5_Ind          0\nE5_Svc          0\nE5_Ent          0\nE8_Ret          0\nE8_off          0\nE8_Ind          0\nE8_Svc          0\nE8_Ent          0\nE8_Ed           0\nE8_Hlth         0\nE8_Pub          0\nE_LowWageWk     0\nE_MedWageWk     0\nE_HiWageWk      0\nE_PctLowWage    0\nD1B             0\nD1C             0\nD1C5_RET        0\nD1C5_OFF        0\nD1C5_IND        0\nD1C5_SVC        0\nD1C5_ENT        0\nD1C8_RET        0\nD1C8_IND        0\nD1C8_SVC        0\nD1C8_ENT        0\nD1C8_ED         0\nD1C8_HLTH       0\nD1C8_PUB        0\nD1D             0\nD1_FLAG         0\nD3A             0\nD3AAO           0\nD3AMM           0\nD3APO           0\nD3B             0\nD3BAO           0\nD3BMM3          0\nD3BMM4          0\nD3BPO3          0\nD3BPO4          0\nD2A_Ranked      0\nD2B_Ranked      0\nD3B_Ranked      0\nD4A_Ranked      0\nNatWalkInd      0\nShape_Length    0\nShape_Area      0\ndtype: int64"
  },
  {
    "objectID": "Data_cleaning.html#information-about-us-cities-dataset",
    "href": "Data_cleaning.html#information-about-us-cities-dataset",
    "title": "Data Cleaning",
    "section": "",
    "text": "Code\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Final.csv'\n\ndf = pd.read_csv(file_path)\n\n# Drop unwanted columns\ndf.drop(columns=[\"OBJECTID\", \"GEOID10\", \"GEOID20\", \"STATEFP\", \"COUNTYFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\", \"CSA\", \"CSA_Name\", \"CBSA\", \"CBSA_Name\", \"CountHU\", \"HH\", \"D1A\", \"D1C8_OFF\", \"D4D\", \"D4E\", \"D2A_JPHH\", \"D2B_E5MIX\", \"D2B_E5MIXA\", \"D2B_E8MIX\",\"D2B_E8MIXA\",\"D2A_EPHHM\",\"D2C_TRPMX1\",\"D2C_TRPMX2\",\"D2C_TRIPEQ\",\"D2R_JOBPOP\",\"D2R_WRKEMP\",\"D2A_WRKEMP\",\"D2C_WREMLX\",\"D4A\",\"D4B025\",\"D4B050\",\"D4C\",\"D5AR\",\"D5AE\",\"D5BR\",\"D5BE\",\"D5CR\",\"D5CRI\",\"D5CE\",\"D5CEI\",\"D5DR\",\"D5DRI\",\"D5DE\",\"D5DEI\"], inplace=True)\n\n# count NA values\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# print all the columns with NA values\nprint(df.isna().sum())\n\n# Assuming your DataFrame is called 'new_df'\ndf.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv', index=False)\n\n\nCBSA_POP        0\nCBSA_EMP        0\nCBSA_WRK        0\nAc_Total        0\nAc_Water        0\nAc_Land         0\nAc_Unpr         0\nTotPop          0\nP_WrkAge        0\nAutoOwn0        0\nPct_AO0         0\nAutoOwn1        0\nPct_AO1         0\nAutoOwn2p       0\nPct_AO2p        0\nWorkers         0\nR_LowWageWk     0\nR_MedWageWk     0\nR_HiWageWk      0\nR_PCTLOWWAGE    0\nTotEmp          0\nE5_Ret          0\nE5_Off          0\nE5_Ind          0\nE5_Svc          0\nE5_Ent          0\nE8_Ret          0\nE8_off          0\nE8_Ind          0\nE8_Svc          0\nE8_Ent          0\nE8_Ed           0\nE8_Hlth         0\nE8_Pub          0\nE_LowWageWk     0\nE_MedWageWk     0\nE_HiWageWk      0\nE_PctLowWage    0\nD1B             0\nD1C             0\nD1C5_RET        0\nD1C5_OFF        0\nD1C5_IND        0\nD1C5_SVC        0\nD1C5_ENT        0\nD1C8_RET        0\nD1C8_IND        0\nD1C8_SVC        0\nD1C8_ENT        0\nD1C8_ED         0\nD1C8_HLTH       0\nD1C8_PUB        0\nD1D             0\nD1_FLAG         0\nD3A             0\nD3AAO           0\nD3AMM           0\nD3APO           0\nD3B             0\nD3BAO           0\nD3BMM3          0\nD3BMM4          0\nD3BPO3          0\nD3BPO4          0\nD2A_Ranked      0\nD2B_Ranked      0\nD3B_Ranked      0\nD4A_Ranked      0\nNatWalkInd      0\nShape_Length    0\nShape_Area      0\ndtype: int64"
  },
  {
    "objectID": "Data_cleaning.html#text-data",
    "href": "Data_cleaning.html#text-data",
    "title": "Data Cleaning",
    "section": "Text data",
    "text": "Text data\n\n\nCode\nimport json\nimport re\n\n# Load the existing JSON file with Reddit post data\nwith open('top_pub_transp_urls.json', 'r') as json_file:\n    data = json.load(json_file)\n\n# Function to clean the text\ndef clean_text(text):\n    # Remove unwanted characters and escape sequences\n    cleaned_text = re.sub(r'\\\\u....', '', text)  # Remove escape sequences\n    cleaned_text = re.sub(r'[^A-Za-z\\s]', ' ', cleaned_text)  # Remove non-alphabetic characters\n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra whitespace\n    cleaned_text = cleaned_text.strip()  # Strip leading and trailing spaces\n    return cleaned_text\n\n# Extract and clean the text data\ncleaned_text_data = []\n\nfor item in data:\n    cleaned_text = clean_text(item.get(\"text\", \"\"))\n    cleaned_text_data.append({\"url\": item[\"url\"], \"cleaned_text\": cleaned_text})\n\n# Save the cleaned text data to a new JSON file\nwith open('cleaned_text_data.json', 'w') as json_file:\n    json.dump(cleaned_text_data, json_file, indent=4)\n\nprint(\"Text data cleaned and saved to cleaned_text_data.json.\")\n\n\nThe previous code cleans the text data such that we only get the urls and the text of each reddit post. It can be found in the data tab. That way it can be later used for key word extraction.\nNote: the sentiment analysis was already extracted and saved in the data tab.\nclean data as vocabulary each col and each row number of appareances"
  },
  {
    "objectID": "Naive_Bayes.html",
    "href": "Naive_Bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a supervised machine learning algorithm based on Bayes’ Theorem. It is used for classification tasks where every pair of features being classified is independent of each other.\nWhile Naive Bayes might seem a simple process, its results are very insightful and have been proven to work very efficiently. One of the benefits is that it needs very little data to be trained and, as more data comes in, it can always be trained incrementally. The three most common Naive Bayes algorithms are: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\n\n\nThe formula for Bayes’ Theorem is: \\(P(\\text{A}|\\text{B}) = \\frac{P(\\text{B}|\\text{A}) \\times P(\\text{A})} {P(\\text{B})}\\)\n\n\n\nIt is used when the variables are continuous and have a normal distribution\nFormula: \\(P(\\text{class}|\\text{data}) = \\frac{P(\\text{data}|\\text{class}) \\times P(\\text{class})} {P(\\text{data})}\\)\n\n\n\nIt is used when the variables are discrete and have a multinomial distribution.\n\n\n\nIt is used when the variables are binary and have a Bernoulli distribution.\n\n\n\n\nMissing values are ignored while training the model and ignored when a probability is calculated.\nPerforms well even when the independence assumption is not satisfied.\nIt is easily to interpretate and has fast prediction time."
  },
  {
    "objectID": "Naive_Bayes.html#introduction",
    "href": "Naive_Bayes.html#introduction",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a supervised machine learning algorithm based on Bayes’ Theorem. It is used for classification tasks where every pair of features being classified is independent of each other.\nWhile Naive Bayes might seem a simple process, its results are very insightful and have been proven to work very efficiently. One of the benefits is that it needs very little data to be trained and, as more data comes in, it can always be trained incrementally. The three most common Naive Bayes algorithms are: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\n\n\nThe formula for Bayes’ Theorem is: \\(P(\\text{A}|\\text{B}) = \\frac{P(\\text{B}|\\text{A}) \\times P(\\text{A})} {P(\\text{B})}\\)\n\n\n\nIt is used when the variables are continuous and have a normal distribution\nFormula: \\(P(\\text{class}|\\text{data}) = \\frac{P(\\text{data}|\\text{class}) \\times P(\\text{class})} {P(\\text{data})}\\)\n\n\n\nIt is used when the variables are discrete and have a multinomial distribution.\n\n\n\nIt is used when the variables are binary and have a Bernoulli distribution.\n\n\n\n\nMissing values are ignored while training the model and ignored when a probability is calculated.\nPerforms well even when the independence assumption is not satisfied.\nIt is easily to interpretate and has fast prediction time."
  },
  {
    "objectID": "Naive_Bayes.html#naive-bayes-for-labeled-record-data",
    "href": "Naive_Bayes.html#naive-bayes-for-labeled-record-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes for labeled record data",
    "text": "Naive Bayes for labeled record data\nNaive Bayes has been used in our record data from our data set: Border_Crossing_Entry_Data_20231103.csv. The model tries to predict the method of transportation (“Measure” in the data set) based on the value and the State of entrance.\n\n\nCode\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(e1071)\n#library(\"DescTools\")\n\nchicago &lt;- read.csv('../../data/Raw_Data_project_Pub.Transport_5000/Border_Crossing_Entry_Data_20231103.csv')\nhead(chicago)\n\nchicago &lt;- chicago[, c(\"Value\", \"Measure\", \"State\")]\nvars &lt;- c(\"Value\", \"State\")\n\nset.seed(4444) # initialize the random seed\n\n# Generate the list of observations for the\n# train dataframe\nsub &lt;- createDataPartition(y = chicago$Measure, \n                           p = 0.80, \n                           list = FALSE)\n\nchicago[] &lt;- lapply(chicago, as.factor)\n\ncen_bcs &lt;- preProcess(x = chicago[ ,vars],\n                      method = c(\"BoxCox\", \"center\", \"scale\"))\n\nDM_bcs &lt;- predict(object = cen_bcs,\n                      newdata = chicago)\n\ntrain &lt;- DM_bcs[sub, ] \ntest &lt;- DM_bcs[-sub, ]\n\n# Train a Naive Bayes model\nnb_model &lt;- naiveBayes(Measure ~ ., data = train)\n\n# Make predictions on the test data\npredictions &lt;- predict(nb_model, newdata = test)\n\n# Ensure predictions contain all possible factor levels\npredictions &lt;- factor(predictions, levels = levels(test$Measure))\n\n# Evaluate the model (e.g., confusion matrix)\nconfusion_matrix &lt;- confusionMatrix(predictions, test$Measure)\nprint(confusion_matrix)\n\n# Print additional classification performance metrics\nprint(confusion_matrix$byClass)\n\n\nConfusion Matrix and Statistics\n\n                             Reference\nPrediction                    Bus Passengers Buses Pedestrians\n  Bus Passengers                         181   129          80\n  Buses                                  514  2172        1004\n  Pedestrians                            668   273        1127\n  Personal Vehicle Passengers            463    96         419\n  Personal Vehicles                      279    43         106\n  Rail Containers Empty                  100    82          22\n  Rail Containers Loaded                1852  1839        2324\n  Train Passengers                       368   289         228\n  Trains                                 575   513         341\n  Truck Containers Empty                 646   480         473\n  Truck Containers Loaded                356   161         127\n  Trucks                                 170    99          89\n                             Reference\nPrediction                    Personal Vehicle Passengers Personal Vehicles\n  Bus Passengers                                      145               142\n  Buses                                               263               303\n  Pedestrians                                        1305              1322\n  Personal Vehicle Passengers                        1936              1827\n  Personal Vehicles                                  1635              1282\n  Rail Containers Empty                                15                28\n  Rail Containers Loaded                              146               194\n  Train Passengers                                    144               291\n  Trains                                              293               423\n  Truck Containers Empty                              648               719\n  Truck Containers Loaded                             438               453\n  Trucks                                              220               209\n                             Reference\nPrediction                    Rail Containers Empty Rail Containers Loaded\n  Bus Passengers                                 67                     43\n  Buses                                         184                    142\n  Pedestrians                                   316                    270\n  Personal Vehicle Passengers                   189                    287\n  Personal Vehicles                             193                    302\n  Rail Containers Empty                         320                    401\n  Rail Containers Loaded                       3752                   3700\n  Train Passengers                              127                    102\n  Trains                                        316                    237\n  Truck Containers Empty                        225                    184\n  Truck Containers Loaded                       111                    101\n  Trucks                                         61                     76\n                             Reference\nPrediction                    Train Passengers Trains Truck Containers Empty\n  Bus Passengers                            70     96                    205\n  Buses                                    225    497                   1097\n  Pedestrians                               53     13                    827\n  Personal Vehicle Passengers               85     12                    366\n  Personal Vehicles                         60      2                    283\n  Rail Containers Empty                    310    322                    174\n  Rail Containers Loaded                  3662   3435                   1205\n  Train Passengers                         497    282                    491\n  Trains                                   352    527                    895\n  Truck Containers Empty                   260    457                    849\n  Truck Containers Loaded                  143    132                    370\n  Trucks                                    74     90                    214\n                             Reference\nPrediction                    Truck Containers Loaded Trucks\n  Bus Passengers                                  172    212\n  Buses                                           824    796\n  Pedestrians                                     867    976\n  Personal Vehicle Passengers                     660    812\n  Personal Vehicles                               542    657\n  Rail Containers Empty                           156    119\n  Rail Containers Loaded                         1191    533\n  Train Passengers                                375    436\n  Trains                                          658    806\n  Truck Containers Empty                          862    961\n  Truck Containers Loaded                         389    446\n  Trucks                                          198    252\n\nOverall Statistics\n                                          \n               Accuracy : 0.1712          \n                 95% CI : (0.1685, 0.1738)\n    No Information Rate : 0.093           \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.0973          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n\nStatistics by Class:\n\n                     Class: Bus Passengers Class: Buses Class: Pedestrians\nSensitivity                       0.029326      0.35168            0.17776\nSpecificity                       0.980867      0.91777            0.90291\nPos Pred Value                    0.117380      0.27079            0.14058\nNeg Pred Value                    0.920927      0.94221            0.92477\nPrevalence                        0.079838      0.07989            0.08201\nDetection Rate                    0.002341      0.02810            0.01458\nDetection Prevalence              0.019946      0.10376            0.10370\nBalanced Accuracy                 0.505097      0.63473            0.54034\n                     Class: Personal Vehicle Passengers\nSensitivity                                     0.26934\nSpecificity                                     0.92561\nPos Pred Value                                  0.27069\nNeg Pred Value                                  0.92514\nPrevalence                                      0.09298\nDetection Rate                                  0.02504\nDetection Prevalence                            0.09251\nBalanced Accuracy                               0.59747\n                     Class: Personal Vehicles Class: Rail Containers Empty\nSensitivity                           0.17823                     0.054598\nSpecificity                           0.94150                     0.975800\nPos Pred Value                        0.23811                     0.156174\nNeg Pred Value                        0.91781                     0.926373\nPrevalence                            0.09304                     0.075815\nDetection Rate                        0.01658                     0.004139\nDetection Prevalence                  0.06964                     0.026505\nBalanced Accuracy                     0.55986                     0.515199\n                     Class: Rail Containers Loaded Class: Train Passengers\nSensitivity                                0.63302                0.085823\nSpecificity                                0.71827                0.956192\nPos Pred Value                             0.15525                0.136915\nNeg Pred Value                             0.95989                0.928146\nPrevalence                                 0.07561                0.074909\nDetection Rate                             0.04786                0.006429\nDetection Prevalence                       0.30829                0.046956\nBalanced Accuracy                          0.67564                0.521007\n                     Class: Trains Class: Truck Containers Empty\nSensitivity               0.089855                       0.12170\nSpecificity               0.924288                       0.91590\nPos Pred Value            0.088780                       0.12552\nNeg Pred Value            0.925208                       0.91315\nPrevalence                0.075866                       0.09024\nDetection Rate            0.006817                       0.01098\nDetection Prevalence      0.076785                       0.08750\nBalanced Accuracy         0.507072                       0.51880\n                     Class: Truck Containers Loaded Class: Trucks\nSensitivity                                0.056426       0.03597\nSpecificity                                0.959695       0.97866\nPos Pred Value                             0.120545       0.14384\nNeg Pred Value                             0.912190       0.91061\nPrevalence                                 0.089177       0.09063\nDetection Rate                             0.005032       0.00326\nDetection Prevalence                       0.041743       0.02266\nBalanced Accuracy                          0.508060       0.50732\n                                   Sensitivity Specificity Pos Pred Value\nClass: Bus Passengers               0.02932599   0.9808674     0.11738003\nClass: Buses                        0.35168394   0.9177714     0.27078918\nClass: Pedestrians                  0.17776025   0.9029126     0.14057628\nClass: Personal Vehicle Passengers  0.26933779   0.9256122     0.27069351\nClass: Personal Vehicles            0.17822883   0.9414953     0.23811293\nClass: Rail Containers Empty        0.05459819   0.9757999     0.15617374\nClass: Rail Containers Loaded       0.63301967   0.7182698     0.15524693\nClass: Train Passengers             0.08582283   0.9561916     0.13691460\nClass: Trains                       0.08985507   0.9242882     0.08878032\nClass: Truck Containers Empty       0.12170298   0.9158977     0.12551745\nClass: Truck Containers Loaded      0.05642588   0.9596949     0.12054540\nClass: Trucks                       0.03596917   0.9786632     0.14383562\n                                   Neg Pred Value  Precision     Recall\nClass: Bus Passengers                   0.9209265 0.11738003 0.02932599\nClass: Buses                            0.9422105 0.27078918 0.35168394\nClass: Pedestrians                      0.9247655 0.14057628 0.17776025\nClass: Personal Vehicle Passengers      0.9251372 0.27069351 0.26933779\nClass: Personal Vehicles                0.9178149 0.23811293 0.17822883\nClass: Rail Containers Empty            0.9263733 0.15617374 0.05459819\nClass: Rail Containers Loaded           0.9598870 0.15524693 0.63301967\nClass: Train Passengers                 0.9281458 0.13691460 0.08582283\nClass: Trains                           0.9252077 0.08878032 0.08985507\nClass: Truck Containers Empty           0.9131452 0.12551745 0.12170298\nClass: Truck Containers Loaded          0.9121895 0.12054540 0.05642588\nClass: Trucks                           0.9106082 0.14383562 0.03596917\n                                           F1 Prevalence Detection Rate\nClass: Bus Passengers              0.04692766 0.07983753    0.002341314\nClass: Buses                       0.30598014 0.07988927    0.028095774\nClass: Pedestrians                 0.15699659 0.08201068    0.014578240\nClass: Personal Vehicle Passengers 0.27001395 0.09297994    0.025043010\nClass: Personal Vehicles           0.20386420 0.09304461    0.016583233\nClass: Rail Containers Empty       0.08091024 0.07581461    0.004139341\nClass: Rail Containers Loaded      0.24934295 0.07560764    0.047861125\nClass: Train Passengers            0.10550897 0.07490913    0.006428913\nClass: Trains                      0.08931446 0.07586635    0.006816976\nClass: Truck Containers Empty      0.12358079 0.09023762    0.010982188\nClass: Truck Containers Loaded     0.07686987 0.08917692    0.005031886\nClass: Trucks                      0.05754739 0.09062569    0.003259731\n                                   Detection Prevalence Balanced Accuracy\nClass: Bus Passengers                        0.01994645         0.5050967\nClass: Buses                                 0.10375516         0.6347277\nClass: Pedestrians                           0.10370342         0.5403364\nClass: Personal Vehicle Passengers           0.09251426         0.5974750\nClass: Personal Vehicles                     0.06964440         0.5598621\nClass: Rail Containers Empty                 0.02650471         0.5151990\nClass: Rail Containers Loaded                0.30829032         0.6756448\nClass: Train Passengers                      0.04695564         0.5210072\nClass: Trains                                0.07678477         0.5070717\nClass: Truck Containers Empty                0.08749531         0.5188003\nClass: Truck Containers Loaded               0.04174266         0.5080604\nClass: Trucks                                0.02266289         0.5073162\n\n\n\n\nCode\n# Convert the confusion matrix to a data frame\nconfusion_matrix_df &lt;- as.data.frame(as.table(confusion_matrix$table))\n\n# Create the plot using ggplot2\nlibrary(ggplot2)\ngg &lt;- ggplot(data = confusion_matrix_df, aes(x = Reference, y = Prediction)) +\n  geom_tile(aes(fill = Freq)) +\n  geom_text(aes(label = Freq), vjust = 1) +\n  scale_fill_gradient(low = \"#7cf09b\", high = \"#3ee882\") +\n  labs(\n    x = \"Reference\",\n    y = \"Prediction\",\n    fill = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12)) +\n  ggtitle(\"Confusion Matrix\")\n\nprint(gg)\n\n\n\n\n\nWe can infer that this model would not be the best to predict this outcome, as its accruacy is only 17%."
  },
  {
    "objectID": "Naive_Bayes.html#naive-bayes-for-labeled-text-data",
    "href": "Naive_Bayes.html#naive-bayes-for-labeled-text-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes for labeled text data",
    "text": "Naive Bayes for labeled text data\nNaive Bayes has been used in our text data from redit in order to predict the sentiment of the comments. The data has been labeled by the sentiment of the comments as shown in the data gathering tab. The data has been split into a training set and a test set. The training set has been used to train the model and the test set has been used to test the model.\n\n\nCode\n\n# read in the data\ndf=pd.read_json(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/sentiment_results.json\")\nprint(df.shape)\nprint(df.columns)\n\n# throw away all empty strings of text from df along with their url and sentiment_score\ndf = df[df.text != '']\nprint(df.shape)\nprint(df.columns)\n\n# convert sentiment_score to positive or negative\ndf['sentiment'] = df['sentiment_score'].apply(lambda score: 'positive' if score &gt;= 0 else 'negative')\n\n\n\n\nCode\ntexts = []\ny = []\n#https://spacy.io/usage/models\n\nimport re\nimport spacy.lang.en.stop_words as stopwords\nimport spacy\nparser = spacy.load('en_core_web_sm')\nstop_words = stopwords.STOP_WORDS\n\n\n# Iterate over rows\nfor i in range(df.shape[0]):\n    # QUICKLY CLEAN TEXT\n    keep = \"abcdefghijklmnopqrstuvwxyz \"\n    replace = \".,!;\"\n    tmp = \"\"\n    text_value = df[\"text\"].iloc[i]  # Accessing the \"text\" column using .iloc\n    sentiment_value = df[\"sentiment\"].iloc[i]  # Accessing the \"sentiment\" column using .iloc\n\n\n    text_value = re.sub('[^a-zA-Z ]+', '', text_value.replace(\"&lt;br /&gt;\", \"\").lower())\n    text_value = parser(text_value)\n    tokens = [token.lower_ for token in text_value]\n    #tokens = [token for token in text_value if token not in stop_words]\n    tokens = [token.lemma_ for token in text_value if token not in stop_words]\n\n\n    tmp = \" \".join(tokens)\n    texts.append(tmp)\n\n    # CONVERT STRINGS TO INT TAGS\n    if sentiment_value == \"positive\":\n        y.append(1)\n    elif sentiment_value == \"negative\":\n        y.append(0)\n\n    # PRINT FIRST COUPLE TEXTS\n    if i &lt; 3:\n        print(i)\n        print(tmp.replace(\"&lt;br /&gt;\", \"\"), '\\n')\n        print(tmp)\n        print(sentiment_value, y[i])\n\n# CONVERT Y TO NUMPY ARRAY\ny=np.array(y)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# PARAMETERS TO CONTROL SIZE OF FEATURE SPACE WITH COUNT-VECTORIZER\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\n# max_features=int, default=None\n#   If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n    # RUN COUNT VECTORIZER ON OUR COURPUS \n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n    #CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(texts,MAX_FEATURES=10000)\n\n\n\n\nCode\n#swap keys and values (value --&gt; ley)\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\n\n#RE-ORDER COLUMN SO IT IS SORTED FROM HIGH FREQ TERMS TO LOW \n# https://stackoverflow.com/questions/60758625/sort-pandas-dataframe-by-sum-of-columns\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\n\n# REMAP DICTIONARY TO CORRESPOND TO NEW COLUMN NUMBERS\nprint()\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n    # print(i2)\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\n\n# RENAME COLUMNS 0,1,2,3 .. \ndf2.columns = range(df2.columns.size)\nprint(df2.head())\nprint(df2.sum(axis=0))\nx=df2.to_numpy()\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\nfrom sklearn.model_selection import train_test_split\n\ndef train_MNB_model(X, Y, i_print=False):\n    if i_print:\n        print(X.shape, Y.shape)\n\n    # SPLIT USING train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # INITIALIZE MODEL\n    model = MultinomialNB()\n\n    # TRAIN MODEL\n    start = time.process_time()\n    model.fit(X_train, y_train)\n    time_train = time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET\n    start = time.process_time()\n    yp_train = model.predict(X_train)\n    yp_test = model.predict(X_test)\n    time_eval = time.process_time() - start\n\n    acc_train = accuracy_score(y_train, yp_train) * 100\n    acc_test = accuracy_score(y_test, yp_test) * 100\n\n    if i_print:\n        print(acc_train, acc_test, time_train, time_eval)\n\n    return acc_train, acc_test, time_train, time_eval\n\n# TEST\nprint(type(x), type(y))\nprint(x.shape, y.shape)\n(acc_train, acc_test, time_train, time_eval) = train_MNB_model(x, y, i_print=True)\n\n##UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n# SAVE RESULTS\nsave_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/results_naive_bayes\")\n\n\n\n\nCode\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-ob')\n    plt.plot(num_features,test_accuracies,'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n# PLOT RESULTS\nplot_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/results_naive_bayes\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsave_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/partial_grid_search\")\nplot_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes seems to do a good job in accruacy if the number of features is kept under 2000 or between 4000 and 6000."
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "About",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "Clustering.html#explanation-of-clustering-methods",
    "href": "Clustering.html#explanation-of-clustering-methods",
    "title": "Clustering",
    "section": "Explanation of clustering methods",
    "text": "Explanation of clustering methods\nClsutering focuses on finding unknown similarities or groupings in the data (unsupervised). The goal is to see if there are any groupings in a given data set in order to make generalizations about the data. The data set used for this project is the EPA Smart Location Database (EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv in the clean data folder), which contains census block data for the US. The data set contains 71 variables and 220740 census blocks, which will be used for clustering. There are three methods that will be used for this study: K-means, Hierarchical, and DBSCAN. The goal is to see if there are any groupings in the data in order to make generalizations about these census blocks. If there is a small enough group of groupings, we can make generalizations about the census blocks use them for public transportation planning.\n\nK-means\nK-means is a clustering method that focuses on finding the centers of the clusters and then assigning the data points to each cluster. The number of clusters is chosen by the performer, so the work around is to provide a range of possible number of clusters and get their scores in order to find the most optimal one. The algorithm randomly selects k points in the data, calculates the center of the cluster, and then assigns each point to the closest cluster. The algorithm is heavily impacted by the initial random points selected, so the algorithm is run multiple times and the best result is selected for the number of clusters specified. Furthermore, in order to get more accruacy, the data is scaled before running this algorithm as it is extreamly sensitive to outliers.\n\n\nDBSCAN\nDBSCAN is another clustering method that tries to find clusters based on the density (distances between points and a minimum number of points). It also detects and marks outliers as points that are in very low density regions. The algorithm runs until all points are placed in a cluster or marked as an outlier. Then, the best result is selected. The algorithm is also sensitive to outliers the same way as k-means, so the data set is scaled to reduce the effect of outliers.\nDBSCAN requires two parameters: epsilon and minimum samples. Epsilon is the maximum distance between two points in order for them to be considered in the same cluster. The Minimum sample is the minimum number of points in each cluster. TIn order to find the best parameters, the algorithm is run multiple times with different values for epsilon and minimum samples. The best result is then selected.\n\n\nHierarchical Clustering\nHierarchical clustering is a clustering method that focuses on finding clusters based on distance. The algorithm works by finding the two closest points in the data set and forming a cluster. The distance between the two points is the distance between the two clusters. The algorithm is repeated until all the points are assigned to a cluster. The algorithm is sensitive to the distance, so the algorithm is run multiple times with different values for the distance. The best result is selected. The algorithm is also sensitive to outliers, so the data set is scaled to reduce the effect of outliers.\nHierarchical clustering is a method that finds the clusters based on distance. The algorithm works by finding the two closest points in the data set and forming a cluster. The algorithm is repeated until all points are assigned to a cluster. Thus, the algorithm runs multiple times with different values for the distance and then the best result is selected. As the previous two, the algorithm is also sensitive to outliers, so the data set is scaled to reduce the effect of outliers.\n\n\nScores used:\n\nDistortion: Distortion is the sum of squared errors. It can be used empirically to find the optimal number of clusters by plotting the distortion for each number of clusters and finding the elbow point.\nDavies-Boulding Index: This index is the average of the ratios of of all clusters. Each cluster ratio is the average distance between points in a cluster and the centroid of the cluster to the distance between the centroids of the cluster and the nearest cluster. The lower the index, the better the clustering.\nOther options that have been rejected: Due to the size of the data set, the Silhouette Score and Inertia where discard. These scores were either too computationaly expensive or not suitable for the data set."
  },
  {
    "objectID": "Clustering.html#k-means-1",
    "href": "Clustering.html#k-means-1",
    "title": "Clustering",
    "section": "k-means++",
    "text": "k-means++\nThese are the following scores obtained per number of clusters in k-means++. We will only use distortion as our parameter due to the large size of our data set and my laptop’s computational limitations. Thus, we will use the elbow method to empirically find the optimal number of clusters.\n\n\nCode\ndistortion_values = []\n\nfor i in range(1, 65):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(df_scaled)\n    cluster_assignments = kmeans.predict(df_scaled)\n    centers = kmeans.cluster_centers_\n\n    distortion = sum(np.min(pairwise_distances(df_scaled, centers, metric='euclidean'), axis=1)) / df_scaled.shape[0]\n    distortion_values.append(distortion)\n\nkmeans_df = pd.DataFrame({'Clusters': range(1, 65), 'Distortion': distortion_values})\n\npd.set_option(\"display.max_rows\", None)\nprint(kmeans_df)\n\n\n\n    Clusters  Distortion\n0          1    5.947283\n1          2    5.442291\n2          3    5.387453\n3          4    5.209756\n4          5    5.205517\n5          6    5.000645\n6          7    5.053200\n7          8    4.990364\n8          9    4.980376\n9         10    4.931528\n10        11    4.854808\n11        12    4.747632\n12        13    4.751885\n13        14    4.720352\n14        15    4.683612\n15        16    4.628865\n16        17    4.572890\n17        18    4.568782\n18        19    4.579223\n19        20    4.585187\n20        21    4.466414\n21        22    4.494016\n22        23    4.451415\n23        24    4.490546\n24        25    4.380493\n25        26    4.356751\n26        27    4.337594\n27        28    4.355580\n28        29    4.290759\n29        30    4.331594\n30        31    4.262690\n31        32    4.226878\n32        33    4.226878\n33        34    4.236690\n34        35    4.256159\n35        36    4.213076\n36        37    4.191339\n37        38    4.149187\n38        39    4.135633\n39        40    4.164670\n40        41    4.124976\n41        42    4.106101\n42        43    4.100170\n43        44    4.101785\n44        45    4.062143\n45        46    4.060281\n46        47    4.056054\n47        48    4.063727\n48        49    4.005314\n49        50    4.003852\n50        51    4.008747\n51        52    3.986774\n52        53    3.970085\n53        54    4.023853\n54        55    3.983847\n55        56    3.958778\n56        57    3.938189\n57        58    3.952439\n58        59    3.927238\n59        60    3.897854\n60        61    3.891907\n61        62    3.921774\n62        63    3.889286\n63        64    3.856719\n\n\n\n\nCode\nplt.figure(figsize=(10, 5))\nplt.plot(kmeans_df['Clusters'], kmeans_df['Distortion'], marker='', linestyle='-', color='darkblue')\nplt.title('Elbow Method using Distortion')\nplt.ylabel('Distortion')\nplt.xlabel('Number of clusters')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nBased on this graph, using the elbow method we can infer that the elbow could be located at 12 clusters. Even though 19, 21, or 23 clusters also could be good solutions, DBSCAN gives a similar answer (5 or 13 clusters being the two best options). To visualize it in 2D, we plotted the data points with the two most explanatory variables obtained from PCA. With a greater computational power, we would be able to use other methods that would be more reliable and exact.\n\n\nCode\ndf_scaled = df_scaled.iloc[:, [3, 6]]\n\n\n\n\nCode\n# Apply KMeans clustering\nkmeans = KMeans(n_clusters=12, random_state=0)\nkmeans_labels = kmeans.fit_predict(df_scaled)\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# KMeans\nscatter = ax.scatter(df_scaled.iloc[:, 0], df_scaled.iloc[:, 1], c=kmeans_labels, cmap='viridis', marker='o')\nax.set_title('KMeans Clustering')\nax.set_xlabel('Column 0')\nax.set_ylabel('Column 4')\nlegend2 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\nax.add_artist(legend2)\n\nplt.show()"
  },
  {
    "objectID": "Clustering.html#strategic-sampling-for-other-clustering-methods",
    "href": "Clustering.html#strategic-sampling-for-other-clustering-methods",
    "title": "Clustering",
    "section": "Strategic sampling for other clustering methods",
    "text": "Strategic sampling for other clustering methods\nFor the rest of clustering methods, due to their computational complexity compared to k-means++ and my laptop’s limitations, we are going to run them in strategically sampled data from the dimensionality reduced data set. This data set is organized with the first column being the variable that explains the most variance and the last being the one that explains the least. We will sample based on the first variable, making 10 even splits from the minimum of that column and the maximum in order to have samples from all “types” of cities.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nscores_pca_df=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv')\n\n\n\n\nCode\nimport pandas as pd\n\n# Convert columns to numeric type, coercing errors\nscores_pca_df.iloc[:, 0] = pd.to_numeric(scores_pca_df.iloc[:, 0], errors='coerce')\n\n# Print minimum and maximum values for the first column\nmin_col1 = scores_pca_df.iloc[:, 0].min(skipna=True)  # Skip NaN values\nmax_col1 = scores_pca_df.iloc[:, 0].max(skipna=True)  # Skip NaN values\nprint(f\"Min value of the first column: {min_col1}\")\nprint(f\"Max value of the first column: {max_col1}\")\n\n\nMin value of the first column: -41.107059975789234\nMax value of the first column: 272.90891754302567\n\n\n\n\nCode\nfor_samples = scores_pca_df\n\nbreakpoints = [-float('inf'), -14.93975, 11.2275, 37.39475, 63.562, 89.72925, 115.8965, 142.06375, 168.231, 194.39825, 220.5655, 246.73275, float('inf')]\n\n# Create labels based on the intervals of the first column\nfor_samples['labels'] = pd.cut(for_samples.iloc[:, 0], bins=breakpoints, labels=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'])\n\n# Display the updated dataframe\nprint(np.sum(for_samples['labels']==\"C\"))\n\n\n1507\n\n\n\n\nCode\nstratified_sample = for_samples.groupby('labels', group_keys=False).apply(lambda x: x.sample(frac=0.09))\nlen(stratified_sample)\n\n\n/var/folders/qs/g1kzn_w50ws9rb1j_kzg78c40000gn/T/ipykernel_37509/1052100277.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  stratified_sample = for_samples.groupby('labels', group_keys=False).apply(lambda x: x.sample(frac=0.09))\n\n\n19866\n\n\n\n\nCode\nprint(np.sum(stratified_sample['labels']==\"J\"))\n\n\n0\n\n\n\n\nCode\nstratified_sample_no_lab = stratified_sample.drop(columns=['labels'])"
  },
  {
    "objectID": "Clustering.html#hierarchical-clustering-1",
    "href": "Clustering.html#hierarchical-clustering-1",
    "title": "Clustering",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nDue to the elevated computational complexity of this method, as k-means and DBSCAN already gave us good results, we will run Hierarchical clustering using 12 clusters as the number of clusters. We will use the same dimensionality reduced data obtained from PCA, but downsampled. The sampling was done using the stratified sampling method, which is a method that ensures that the sample is representative of the population and would better represent the data set (in order to properly assess clustering). Finally, the sample was only 20% of the original data set, which is still a large enough sample to be representative of the population.\n\n\nCode\nimport pandas as pd\n\nscores_pca_df.iloc[:, 0] = pd.to_numeric(scores_pca_df.iloc[:, 0], errors='coerce')\n\nmin_col1 = scores_pca_df.iloc[:, 0].min(skipna=True)\nmax_col1 = scores_pca_df.iloc[:, 0].max(skipna=True)\nprint(f\"Min value of the first column: {min_col1}\")\nprint(f\"Max value of the first column: {max_col1}\")\n\n\n\n\nCode\nfor_samples = scores_pca_df\n\nbreakpoints = [-float('inf'), -14.93975, 11.2275, 37.39475, 63.562, 89.72925, 115.8965, 142.06375, 168.231, 194.39825, 220.5655, 246.73275, float('inf')]\n\nfor_samples['labels'] = pd.cut(for_samples.iloc[:, 0], bins=breakpoints, labels=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'])\n\nstratified_sample = for_samples.groupby('labels', group_keys=False).apply(lambda x: x.sample(frac=0.2))\n\nstratified_sample_no_lab = stratified_sample.drop(columns=['labels'])\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import davies_bouldin_score\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nlinkage_matrix = linkage(stratified_sample_no_lab, method='ward')\n\ndendrogram(linkage_matrix)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n\nnum_clusters = 12\nlabels = AgglomerativeClustering(n_clusters=num_clusters, linkage='ward').fit_predict(stratified_sample_no_lab)\n\ndb_index = davies_bouldin_score(stratified_sample_no_lab, labels)\nprint(f'Davies-Bouldin Index: {db_index}')\n\nresult_df = pd.DataFrame({\n    'Feature_1': stratified_sample_no_lab['PC1'],\n    'Feature_2': stratified_sample_no_lab['PC2'],\n    'Cluster': labels\n})\n\nprint(result_df)\n\n\n\n\n\n\nDavies-Bouldin Index: 1.4871933145528213\n         Feature_1  Feature_2  Cluster\n24683   -17.298789  22.884256       10\n25006   -18.274415  25.452802       10\n30365    -2.575073   3.024186        0\n38241     2.206078  -2.983790       11\n128967   -1.500392   1.324897        0\n...            ...        ...      ...\n150094   97.759211  26.286094        8\n209124   92.407836  32.282488        9\n152816  107.218005  44.218345        8\n42838   117.086261  64.029969        8\n16705   160.011595  -3.324794        7\n\n[44149 rows x 3 columns]\n\n\nFor the results, Feature_1 and Feature_2 are the coordinates of hte data points based on the first two features. Furthermore, an Davies Boulding Index of less than 1.5 is a good result for this data set, which is a good indication for this number of clusters. The results also indicate the cluster to which each data point has been assigned to. Finally, the dendogram also allow us to understand better the distribution of the data points and the clusters in a more visual way."
  },
  {
    "objectID": "Clustering.html#dbscan-1",
    "href": "Clustering.html#dbscan-1",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN\nFor DBSCAN due to my laptops computational limitations, we used the dimensionality reduced data obtained from PCA (which explained more than 90% of the variance and held 25 out of the original 71 variables). We used Davies Boulding Index as our score due to its computational simplicity and its easy readability. We ran the algorithm with different values for epsilon and minimum samples. The algorithm took 1.5 hours to run, which is a long time, but it is a good result considering the size of the data set.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nscores_pca_df=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv')\n\n\nThese are our results:\n\n\nCode\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import davies_bouldin_score\nimport numpy as np\n\nl_eps = np.arange(0.5, 2, 0.1)\nn_sample = range(5, 11)\n\nresults = {}\n\nfor eps in l_eps:\n    best_db_index = float('inf')\n    opt_sample = None\n    opt_num_clusters = None\n\n    for n in n_sample:\n        model = DBSCAN(eps=eps, min_samples=n).fit(scores_pca_df)\n        try:\n            unique_labels = np.unique(model.labels_)\n            if len(unique_labels) &gt; 1 or (len(unique_labels) == 1 and unique_labels[0] != -1):\n                db_score = davies_bouldin_score(scores_pca_df, model.labels_)\n                num_clusters = len(unique_labels) - 1 \n                \n                if db_score &lt; best_db_index:\n                    best_db_index = db_score\n                    opt_sample = n\n                    opt_num_clusters = num_clusters\n        except:\n            continue\n    \n    results[eps] = {\n        'min_samples': opt_sample,\n        'db_index': best_db_index,\n        'num_clusters': opt_num_clusters\n    }\n\nprint(\"Results for each epsilon value:\")\nfor eps, values in results.items():\n    print(f\"For epsilon={eps}: Min_samples={values['min_samples']}, Davies-Bouldin Index={values['db_index']}, Number of Clusters={values['num_clusters']}\")\n\n\nResults for each epsilon value:\nFor epsilon=0.5: Min_samples=10, Davies-Bouldin Index=0.5465900750864565, Number of Clusters=5\nFor epsilon=0.6: Min_samples=10, Davies-Bouldin Index=1.0735197191834835, Number of Clusters=13\nFor epsilon=0.7: Min_samples=10, Davies-Bouldin Index=1.2753967110181181, Number of Clusters=24\nFor epsilon=0.7999999999999999: Min_samples=10, Davies-Bouldin Index=1.2865395597058111, Number of Clusters=24\nFor epsilon=0.8999999999999999: Min_samples=9, Davies-Bouldin Index=1.4075326833557051, Number of Clusters=32\nFor epsilon=0.9999999999999999: Min_samples=10, Davies-Bouldin Index=1.701066737957798, Number of Clusters=50\nFor epsilon=1.0999999999999999: Min_samples=5, Davies-Bouldin Index=1.7688572327797196, Number of Clusters=311\nFor epsilon=1.1999999999999997: Min_samples=5, Davies-Bouldin Index=1.683123753663869, Number of Clusters=296\nFor epsilon=1.2999999999999998: Min_samples=8, Davies-Bouldin Index=1.6238516834146322, Number of Clusters=68\nFor epsilon=1.4: Min_samples=9, Davies-Bouldin Index=1.471318653013628, Number of Clusters=28\nFor epsilon=1.4999999999999998: Min_samples=10, Davies-Bouldin Index=1.3802203666819477, Number of Clusters=20\nFor epsilon=1.5999999999999996: Min_samples=9, Davies-Bouldin Index=1.5742656310946384, Number of Clusters=32\nFor epsilon=1.6999999999999997: Min_samples=9, Davies-Bouldin Index=1.576282420115537, Number of Clusters=34\nFor epsilon=1.7999999999999998: Min_samples=8, Davies-Bouldin Index=1.474295360271144, Number of Clusters=44\nFor epsilon=1.8999999999999997: Min_samples=9, Davies-Bouldin Index=1.3577512519715513, Number of Clusters=23\n\n\n\n\nCode\nnum_clusters_values = [result['num_clusters'] for result in results.values()]\ndb_index_values = [result['db_index'] for result in results.values()]\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.plot(num_clusters_values, db_index_values, marker='o', linestyle='-', color='b')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Davies-Bouldin Index')\nplt.title('Number of Clusters vs Davies-Bouldin Index for DBSCAN')\nplt.grid(True)\nplt.xlim(0, 150)\nplt.show()\n\n\n\n\n\nBased on this results, we see that the first two best results are 5 and 13 clusters. As we can see that 13 clusters is still a good result based on the Davies Boulding Index, we will ukeep this in mind as it is more similar to the result also obtained in K-means++."
  },
  {
    "objectID": "Dimension_red.html#pca",
    "href": "Dimension_red.html#pca",
    "title": "Dimensionality Reduction",
    "section": "PCA",
    "text": "PCA\n\n\nCode\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n\n# read the data\ndf=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv',)\n\n# normalize every column of the dataframe df\nscaler = StandardScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df))\n\n\n\n\nCode\nfrom sklearn.decomposition import PCA\npca=PCA()\npca.fit(df_scaled)\n\n\nPCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA()\n\n\n\n\nCode\npca.explained_variance_ratio_\n\n\narray([1.50108382e-01, 1.15609443e-01, 7.82879871e-02, 6.17889215e-02,\n       5.63492419e-02, 4.19796970e-02, 3.79998848e-02, 3.40466529e-02,\n       3.01126629e-02, 2.86904491e-02, 2.67076414e-02, 2.40012178e-02,\n       2.24608260e-02, 2.20378564e-02, 1.98976844e-02, 1.91382357e-02,\n       1.78008382e-02, 1.63100277e-02, 1.61154128e-02, 1.47804334e-02,\n       1.42514899e-02, 1.31911235e-02, 1.24278167e-02, 1.11199160e-02,\n       1.05395067e-02, 1.02387927e-02, 9.43188704e-03, 8.64975599e-03,\n       7.52941658e-03, 6.92614696e-03, 6.65040542e-03, 6.46543543e-03,\n       6.38477581e-03, 5.71041588e-03, 4.07517833e-03, 3.90315234e-03,\n       3.76328725e-03, 3.64003766e-03, 3.56666943e-03, 3.19575731e-03,\n       3.04708274e-03, 2.61840439e-03, 2.13646048e-03, 1.53852297e-03,\n       1.22802965e-03, 9.91551753e-04, 8.72316358e-04, 6.53308714e-04,\n       5.32654575e-04, 3.81616309e-04, 8.87763636e-05, 2.12411862e-05,\n       5.57174879e-06, 1.25833539e-13, 1.51285044e-21, 6.27480366e-22,\n       5.47474315e-22, 1.94928016e-22, 1.18390033e-22, 3.19497017e-23,\n       2.96036585e-32, 8.33811318e-33, 6.97748688e-33, 5.07695509e-33,\n       2.56030390e-33, 9.31767565e-34, 6.52600298e-34, 6.52600298e-34,\n       6.52600298e-34, 6.52600298e-34, 3.17154293e-35])\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nCode\n# find what point pca.explained_variance_ratio_.cumsum()) is greater than 0.8\nnp.where(np.cumsum(pca.explained_variance_ratio_) &gt; 0.9)[0][0]\n\n\n25\n\n\nBy rule of thumb, we want to keep at least 80% of the variance. However, to be more percise, we are going to choose 25 components such that the explained variance is greater than 90%.\n\n\nCode\npca=PCA(n_components=25)\npca.fit(df_scaled)\n\n\nPCA(n_components=25)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCAPCA(n_components=25)\n\n\n\n\nCode\npca.transform(df_scaled)\nscores_pca = pca.transform(df_scaled)\n\nscores_pca_df = pd.DataFrame(scores_pca, columns=[f'PC{i}' for i in range(1, scores_pca.shape[1] + 1)])\nscores_pca_df.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv', index=False)"
  },
  {
    "objectID": "Dimension_red.html#t-sne",
    "href": "Dimension_red.html#t-sne",
    "title": "Dimensionality Reduction",
    "section": "t-SNE",
    "text": "t-SNE\n\n\nCode\nfrom __future__ import print_function\nimport time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\n\n\n\nCode\n\nsample_1 = df.sample(n=5000)\n\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming your dataset has features only (no labels)\nX = df.iloc[:, :]\n\n# Initialize the t-SNE model\ntsne = TSNE(n_components=2, random_state=42)\n\n# Fit and transform the data\nX_tsne = tsne.fit_transform(X)\n\n# Use KMeans to get cluster assignments\nkmeans = KMeans(n_clusters=12, random_state=42)  # You can change the number of clusters as needed\nclusters = kmeans.fit_predict(X_tsne)\n\n# Create a DataFrame for the t-SNE results with cluster assignments\ntsne_df = pd.DataFrame(data=X_tsne, columns=['Dimension 1', 'Dimension 2'])\ntsne_df['Cluster'] = clusters\n\n# Plot the t-SNE results with hue based on cluster\nsns.scatterplot(x='Dimension 1', y='Dimension 2', hue='Cluster', data=tsne_df, palette='viridis', legend='full')\nplt.title('t-SNE Visualization with Hue (Cluster)')\nplt.show()\n\n\n/Users/jorgebrismoreno/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\ncheck perpelxity result. Or seaborn pairplot with more variables (with pca). and 3 d plots.\n\n\nCode\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming your dataset has features only (no labels)\nX = df.iloc[:, :]\n\n# Initialize the t-SNE model\ntsne = TSNE(n_components=2, random_state=42)\n\n# Fit and transform the data\nX_tsne = tsne.fit_transform(X)\n\n# Use KMeans to get cluster assignments\nkmeans = KMeans(n_clusters=19, random_state=42)  # You can change the number of clusters as needed\nclusters = kmeans.fit_predict(X_tsne)\n\n# Create a DataFrame for the t-SNE results with cluster assignments\ntsne_df = pd.DataFrame(data=X_tsne, columns=['Dimension 1', 'Dimension 2'])\ntsne_df['Cluster'] = clusters\n\n# Plot the t-SNE results with hue based on cluster\nsns.scatterplot(x='Dimension 1', y='Dimension 2', hue='Cluster', data=tsne_df, palette='viridis', legend='full')\nplt.title('t-SNE Visualization with Hue (Cluster)')\nplt.show()\n\n\n/Users/jorgebrismoreno/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming your dataset has features only (no labels)\nX = df.iloc[:, :]\n\n# Initialize the t-SNE model\ntsne = TSNE(n_components=2, random_state=42)\n\n# Fit and transform the data\nX_tsne = tsne.fit_transform(X)\n\n# Use KMeans to get cluster assignments\nkmeans = KMeans(n_clusters=21, random_state=42)  # You can change the number of clusters as needed\nclusters = kmeans.fit_predict(X_tsne)\n\n# Create a DataFrame for the t-SNE results with cluster assignments\ntsne_df = pd.DataFrame(data=X_tsne, columns=['Dimension 1', 'Dimension 2'])\ntsne_df['Cluster'] = clusters\n\n# Plot the t-SNE results with hue based on cluster\nsns.scatterplot(x='Dimension 1', y='Dimension 2', hue='Cluster', data=tsne_df, palette='viridis', legend='full')\nplt.title('t-SNE Visualization with Hue (Cluster)')\nplt.show()\n\n\n/Users/jorgebrismoreno/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming your dataset has features only (no labels)\nX = df.iloc[:, :]\n\n# Initialize the t-SNE model\ntsne = TSNE(n_components=2, random_state=42)\n\n# Fit and transform the data\nX_tsne = tsne.fit_transform(X)\n\n# Use KMeans to get cluster assignments\nkmeans = KMeans(n_clusters=23, random_state=42)  # You can change the number of clusters as needed\nclusters = kmeans.fit_predict(X_tsne)\n\n# Create a DataFrame for the t-SNE results with cluster assignments\ntsne_df = pd.DataFrame(data=X_tsne, columns=['Dimension 1', 'Dimension 2'])\ntsne_df['Cluster'] = clusters\n\n# Plot the t-SNE results with hue based on cluster\nsns.scatterplot(x='Dimension 1', y='Dimension 2', hue='Cluster', data=tsne_df, palette='viridis', legend='full')\nplt.title('t-SNE Visualization with Hue (Cluster)')\nplt.show()\n\n\n/Users/jorgebrismoreno/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)"
  },
  {
    "objectID": "Clustering.html#dbscan-2",
    "href": "Clustering.html#dbscan-2",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\nCode\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import davies_bouldin_score\nimport numpy as np\n\nl_eps = np.arange(0.5, 2, 0.1)\nn_sample = range(5, 11)\n\nresults = {}\n\nfor eps in l_eps:\n    best_db_index = float('inf')\n    opt_sample = None\n    opt_num_clusters = None\n\n    for n in n_sample:\n        model = DBSCAN(eps=eps, min_samples=n).fit(scores_pca_df)\n        try:\n            unique_labels = np.unique(model.labels_)\n            if len(unique_labels) &gt; 1 or (len(unique_labels) == 1 and unique_labels[0] != -1):\n                db_score = davies_bouldin_score(scores_pca_df, model.labels_)\n                num_clusters = len(unique_labels) - 1 \n                \n                if db_score &lt; best_db_index:\n                    best_db_index = db_score\n                    opt_sample = n\n                    opt_num_clusters = num_clusters\n        except:\n            continue\n    \n    results[eps] = {\n        'min_samples': opt_sample,\n        'db_index': best_db_index,\n        'num_clusters': opt_num_clusters\n    }\n\nprint(\"Results for each epsilon value:\")\nfor eps, values in results.items():\n    print(f\"For epsilon={eps}: Min_samples={values['min_samples']}, Davies-Bouldin Index={values['db_index']}, Number of Clusters={values['num_clusters']}\")\n\n\nResults for each epsilon value:\nFor epsilon=0.5: Min_samples=10, Davies-Bouldin Index=0.5465900750864565, Number of Clusters=5\nFor epsilon=0.6: Min_samples=10, Davies-Bouldin Index=1.0735197191834835, Number of Clusters=13\nFor epsilon=0.7: Min_samples=10, Davies-Bouldin Index=1.2753967110181181, Number of Clusters=24\nFor epsilon=0.7999999999999999: Min_samples=10, Davies-Bouldin Index=1.2865395597058111, Number of Clusters=24\nFor epsilon=0.8999999999999999: Min_samples=9, Davies-Bouldin Index=1.4075326833557051, Number of Clusters=32\nFor epsilon=0.9999999999999999: Min_samples=10, Davies-Bouldin Index=1.701066737957798, Number of Clusters=50\nFor epsilon=1.0999999999999999: Min_samples=5, Davies-Bouldin Index=1.7688572327797196, Number of Clusters=311\nFor epsilon=1.1999999999999997: Min_samples=5, Davies-Bouldin Index=1.683123753663869, Number of Clusters=296\nFor epsilon=1.2999999999999998: Min_samples=8, Davies-Bouldin Index=1.6238516834146322, Number of Clusters=68\nFor epsilon=1.4: Min_samples=9, Davies-Bouldin Index=1.471318653013628, Number of Clusters=28\nFor epsilon=1.4999999999999998: Min_samples=10, Davies-Bouldin Index=1.3802203666819477, Number of Clusters=20\nFor epsilon=1.5999999999999996: Min_samples=9, Davies-Bouldin Index=1.5742656310946384, Number of Clusters=32\nFor epsilon=1.6999999999999997: Min_samples=9, Davies-Bouldin Index=1.576282420115537, Number of Clusters=34\nFor epsilon=1.7999999999999998: Min_samples=8, Davies-Bouldin Index=1.474295360271144, Number of Clusters=44\nFor epsilon=1.8999999999999997: Min_samples=9, Davies-Bouldin Index=1.3577512519715513, Number of Clusters=23\n\n\n\n\nCode\nnum_clusters_values = [result['num_clusters'] for result in results.values()]\ndb_index_values = [result['db_index'] for result in results.values()]\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.plot(num_clusters_values, db_index_values, marker='o', linestyle='-', color='b')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Davies-Bouldin Index')\nplt.title('Number of Clusters vs Davies-Bouldin Index for DBSCAN')\nplt.grid(True)\nplt.xlim(0, 150)\nplt.show()"
  },
  {
    "objectID": "Clustering.html#conclusion",
    "href": "Clustering.html#conclusion",
    "title": "Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nWe can determine that there seems to be possible clustering in the data set. With more advance computational power, it could be determined with more precision. However, with the methodas utilized, we can determine that there are probably 12 clusters in the data set. These clusters could be used to make generalizations about the census blocks and use them for public transportation planning. A better visualization of the clusters can be seen in the Dimensionality Reduction tab, were the cluster labels are used along with the two more explanatory variables calclated with T-sne in order to visualize the clear divisions."
  }
]