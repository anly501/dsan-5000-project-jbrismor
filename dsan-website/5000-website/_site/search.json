[
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Code\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import MeanShift\nfrom sklearn.cluster import Birch\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import pairwise_distances, silhouette_score\nimport warnings\nimport pandas as pd\nCode\ndf=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv')\n\nscaler = StandardScaler()\ndf_scaled = (scaler.fit_transform(df))\ndf_scaled=pd.DataFrame(df_scaled)"
  },
  {
    "objectID": "Clustering.html#explanation-of-clustering-methods",
    "href": "Clustering.html#explanation-of-clustering-methods",
    "title": "Clustering",
    "section": "Explanation of clustering methods",
    "text": "Explanation of clustering methods\nClustering focuses on finding unknown similarities or groupings in the data (unsupervised). The goal is to see if there are any groupings in a given data set in order to make generalizations about the data. The data set used for this project is the EPA Smart Location Database (EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv in the clean data folder), which contains census block data for the US. The data set contains 71 variables and 220740 census blocks, which will be used for clustering. Every explanation of each variable can be found in the raw data folder under the EPA_SLD_3.0_TechnicalDocumentationUserGuide_May2021.pdf or in the following citation (Jim Chapman 2021). There are three methods that will be used for this study: K-means, Hierarchical, and DBSCAN. The goal is to see if there are any groupings in the data in order to make generalizations about these census blocks. If there is a small enough group of groupings, we can make generalizations about the census blocks use them for public transportation planning.\n\nK-means\nK-means is a clustering method that focuses on finding the centers of the clusters and then assigning the data points to each cluster. The number of clusters is chosen by the performer, so the work around is to provide a range of possible number of clusters and get their scores in order to find the most optimal one. The algorithm randomly selects k points in the data, calculates the center of the cluster, and then assigns each point to the closest cluster. The algorithm is heavily impacted by the initial random points selected, so the algorithm is run multiple times and the best result is selected for the number of clusters specified. Furthermore, in order to get more accruacy, the data is scaled before running this algorithm as it is extreamly sensitive to outliers.\n\n\nDBSCAN\nDBSCAN is another clustering method that tries to find clusters based on the density (distances between points and a minimum number of points). It also detects and marks outliers as points that are in very low density regions. The algorithm runs until all points are placed in a cluster or marked as an outlier. Then, the best result is selected. The algorithm is also sensitive to outliers the same way as k-means, so the data set is scaled to reduce the effect of outliers.\nDBSCAN requires two parameters: epsilon and minimum samples. Epsilon is the maximum distance between two points in order for them to be considered in the same cluster. The Minimum sample is the minimum number of points in each cluster. In order to find the best parameters, the algorithm is run multiple times with different values for epsilon and minimum samples. The best result is then selected.\n\n\nHierarchical Clustering\nHierarchical clustering is a clustering method that focuses on finding clusters based on distance. The algorithm works by finding the two closest points in the data set and forming a cluster. The distance between the two points is the distance between the two clusters. The algorithm is repeated until all the points are assigned to a cluster. The algorithm is sensitive to the distance, so the algorithm is run multiple times with different values for the distance. The best result is selected. The algorithm is also sensitive to outliers, so the data set is scaled to reduce the effect of outliers.\nHierarchical clustering is a method that finds the clusters based on distance. The algorithm works by finding the two closest points in the data set and forming a cluster. The algorithm is repeated until all points are assigned to a cluster. Thus, the algorithm runs multiple times with different values for the distance and then the best result is selected. As the previous two, the algorithm is also sensitive to outliers, so the data set is scaled to reduce the effect of outliers.\n\n\nScores used:\n\nDistortion: Distortion is the sum of squared errors. It can be used empirically to find the optimal number of clusters by plotting the distortion for each number of clusters and finding the elbow point.\nDavies-Boulding Index: This index is the average of the ratios of of all clusters. Each cluster ratio is the average distance between points in a cluster and the centroid of the cluster to the distance between the centroids of the cluster and the nearest cluster. The lower the index, the better the clustering (Petrovic 2006).\nOther options that have been rejected: Due to the size of the data set, the Silhouette Score and Inertia where discard. These scores were either too computationaly expensive or not suitable for the data set."
  },
  {
    "objectID": "Clustering.html#k-means-1",
    "href": "Clustering.html#k-means-1",
    "title": "Clustering",
    "section": "k-means++",
    "text": "k-means++\nThese are the following scores obtained per number of clusters in k-means++. We will only use distortion as our parameter due to the large size of our data set and my laptop’s computational limitations. Thus, we will use the elbow method to empirically find the optimal number of clusters.\n\n\nCode\ndistortion_values = []\n\nfor i in range(1, 65):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(df_scaled)\n    cluster_assignments = kmeans.predict(df_scaled)\n    centers = kmeans.cluster_centers_\n\n    distortion = sum(np.min(pairwise_distances(df_scaled, centers, metric='euclidean'), axis=1)) / df_scaled.shape[0]\n    distortion_values.append(distortion)\n\nkmeans_df = pd.DataFrame({'Clusters': range(1, 65), 'Distortion': distortion_values})\n\npd.set_option(\"display.max_rows\", None)\nprint(kmeans_df)\n\n\n\n\nCode\nplt.figure(figsize=(10, 5))\nplt.plot(kmeans_df['Clusters'], kmeans_df['Distortion'], marker='', linestyle='-', color='darkblue')\nplt.title('Elbow Method using Distortion')\nplt.ylabel('Distortion')\nplt.xlabel('Number of clusters')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nBased on this graph, using the elbow method we can infer that the elbow could be located at 12 clusters. Even though 19, 21, or 23 clusters also could be good solutions, DBSCAN gives a similar answer (5 or 13 clusters being the two best options). Thus, we will use 12 clusters for k-means++."
  },
  {
    "objectID": "Clustering.html#dbscan-1",
    "href": "Clustering.html#dbscan-1",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN\nFor DBSCAN due to my laptops computational limitations, we used the dimensionality reduced data obtained from PCA (which explained more than 90% of the variance and held 25 out of the original 71 variables). We used Davies Boulding Index as our score due to its computational simplicity and its easy readability. We ran the algorithm with different values for epsilon and minimum samples. The algorithm took 1.5 hours to run, which is a long time, but it is a good result considering the size of the data set.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nscores_pca_df=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv')\n\n\nThese are the results:\n\n\nCode\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import davies_bouldin_score\nimport numpy as np\n\nl_eps = np.arange(0.5, 2, 0.1)\nn_sample = range(5, 11)\n\nresults = {}\n\nfor eps in l_eps:\n    best_db_index = float('inf')\n    opt_sample = None\n    opt_num_clusters = None\n\n    for n in n_sample:\n        model = DBSCAN(eps=eps, min_samples=n).fit(scores_pca_df)\n        try:\n            unique_labels = np.unique(model.labels_)\n            if len(unique_labels) &gt; 1 or (len(unique_labels) == 1 and unique_labels[0] != -1):\n                db_score = davies_bouldin_score(scores_pca_df, model.labels_)\n                num_clusters = len(unique_labels) - 1 \n                \n                if db_score &lt; best_db_index:\n                    best_db_index = db_score\n                    opt_sample = n\n                    opt_num_clusters = num_clusters\n        except:\n            continue\n    \n    results[eps] = {\n        'min_samples': opt_sample,\n        'db_index': best_db_index,\n        'num_clusters': opt_num_clusters\n    }\n\nprint(\"Results for each epsilon value:\")\nfor eps, values in results.items():\n    print(f\"For epsilon={eps}: Min_samples={values['min_samples']}, Davies-Bouldin Index={values['db_index']}, Number of Clusters={values['num_clusters']}\")\n\n\nResults for each epsilon value:\nFor epsilon=0.5: Min_samples=10, Davies-Bouldin Index=0.5465900750864565, Number of Clusters=5\nFor epsilon=0.6: Min_samples=10, Davies-Bouldin Index=1.0735197191834835, Number of Clusters=13\nFor epsilon=0.7: Min_samples=10, Davies-Bouldin Index=1.2753967110181181, Number of Clusters=24\nFor epsilon=0.7999999999999999: Min_samples=10, Davies-Bouldin Index=1.2865395597058111, Number of Clusters=24\nFor epsilon=0.8999999999999999: Min_samples=9, Davies-Bouldin Index=1.4075326833557051, Number of Clusters=32\nFor epsilon=0.9999999999999999: Min_samples=10, Davies-Bouldin Index=1.701066737957798, Number of Clusters=50\nFor epsilon=1.0999999999999999: Min_samples=5, Davies-Bouldin Index=1.7688572327797196, Number of Clusters=311\nFor epsilon=1.1999999999999997: Min_samples=5, Davies-Bouldin Index=1.683123753663869, Number of Clusters=296\nFor epsilon=1.2999999999999998: Min_samples=8, Davies-Bouldin Index=1.6238516834146322, Number of Clusters=68\nFor epsilon=1.4: Min_samples=9, Davies-Bouldin Index=1.471318653013628, Number of Clusters=28\nFor epsilon=1.4999999999999998: Min_samples=10, Davies-Bouldin Index=1.3802203666819477, Number of Clusters=20\nFor epsilon=1.5999999999999996: Min_samples=9, Davies-Bouldin Index=1.5742656310946384, Number of Clusters=32\nFor epsilon=1.6999999999999997: Min_samples=9, Davies-Bouldin Index=1.576282420115537, Number of Clusters=34\nFor epsilon=1.7999999999999998: Min_samples=8, Davies-Bouldin Index=1.474295360271144, Number of Clusters=44\nFor epsilon=1.8999999999999997: Min_samples=9, Davies-Bouldin Index=1.3577512519715513, Number of Clusters=23\n\n\n\n\nCode\nnum_clusters_values = [result['num_clusters'] for result in results.values()]\ndb_index_values = [result['db_index'] for result in results.values()]\n\nplt.figure(figsize=(10, 6))\nplt.plot(num_clusters_values, db_index_values, marker='o', linestyle='-', color='b')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Davies-Bouldin Index')\nplt.title('Number of Clusters vs Davies-Bouldin Index for DBSCAN')\nplt.grid(True)\nplt.xlim(0, 150)\nplt.show()\n\n\n\n\n\nBased on this results, we see that the first two best results are 5 and 13 clusters. As we can see that 13 clusters is still a good result based on the Davies Boulding Index, we will keep this in mind as it is more similar to the result also obtained in K-means++."
  },
  {
    "objectID": "Clustering.html#hierarchical-clustering-1",
    "href": "Clustering.html#hierarchical-clustering-1",
    "title": "Clustering",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nDue to the elevated computational complexity of this method, as k-means and DBSCAN already gave us good results, we will run Hierarchical clustering using 12 clusters as the number of clusters. We will use the same dimensionality reduced data obtained from PCA, but downsampled. The sampling was done using the stratified sampling method, which is a method that ensures that the sample is representative of the population and would better represent the data set (in order to properly assess clustering). Finally, the sample was only 20% of the original data set, which is still a large enough sample to be representative of the population.\n\n\nCode\nimport pandas as pd\nscores_pca_df=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv')\n\n\n\n\nCode\nimport pandas as pd\n\nscores_pca_df.iloc[:, 0] = pd.to_numeric(scores_pca_df.iloc[:, 0], errors='coerce')\n\nmin_col1 = scores_pca_df.iloc[:, 0].min(skipna=True)\nmax_col1 = scores_pca_df.iloc[:, 0].max(skipna=True)\nprint(f\"Min value of the first column: {min_col1}\")\nprint(f\"Max value of the first column: {max_col1}\")\n\n\n\n\nCode\nfor_samples = scores_pca_df\n\nbreakpoints = [-float('inf'), -14.93975, 11.2275, 37.39475, 63.562, 89.72925, 115.8965, 142.06375, 168.231, 194.39825, 220.5655, 246.73275, float('inf')]\n\nfor_samples['labels'] = pd.cut(for_samples.iloc[:, 0], bins=breakpoints, labels=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'])\n\nstratified_sample = for_samples.groupby('labels', group_keys=False).apply(lambda x: x.sample(frac=0.2))\n\nstratified_sample_no_lab = stratified_sample.drop(columns=['labels'])\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import davies_bouldin_score\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nlinkage_matrix = linkage(stratified_sample_no_lab, method='ward')\n\ndendrogram(linkage_matrix)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n\nnum_clusters = 12\nlabels = AgglomerativeClustering(n_clusters=num_clusters, linkage='ward').fit_predict(stratified_sample_no_lab)\n\ndb_index = davies_bouldin_score(stratified_sample_no_lab, labels)\nprint(f'Davies-Bouldin Index: {db_index}')\n\nresult_df = pd.DataFrame({\n    'Feature_1': stratified_sample_no_lab['PC1'],\n    'Feature_2': stratified_sample_no_lab['PC2'],\n    'Cluster': labels\n})\n\nprint(result_df)\n\n\n\n\n\n\nDavies-Bouldin Index: 1.4871933145528213\n         Feature_1  Feature_2  Cluster\n24683   -17.298789  22.884256       10\n25006   -18.274415  25.452802       10\n30365    -2.575073   3.024186        0\n38241     2.206078  -2.983790       11\n128967   -1.500392   1.324897        0\n...            ...        ...      ...\n150094   97.759211  26.286094        8\n209124   92.407836  32.282488        9\n152816  107.218005  44.218345        8\n42838   117.086261  64.029969        8\n16705   160.011595  -3.324794        7\n\n[44149 rows x 3 columns]\n\n\nFor the results, Feature_1 and Feature_2 are the coordinates of the data points based on the first two features. Furthermore, a Davies Boulding Index of less than 1.5 is a good result for this data set, which is a good indication for this number of clusters. The results also indicate the cluster to which each data point has been assigned to. Finally, the dendogram also allow us to understand better the distribution of the data points and the clusters in a more visual way."
  },
  {
    "objectID": "Clustering.html#conclusion",
    "href": "Clustering.html#conclusion",
    "title": "Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nBased on the results obtained from the three methods, we have a good reason to believe that clusters exist. However, due to our computational constrains encountered and the great size of our data set (in terms of variables), it will be helpful to visualize the clusters in 2D based on different variables. Firstly, we will visualize them on handpicked variables that we believe are the most explanatory from the primary data set. Then, we will use the PCA’s reduced data set to visualize the clusters in 2D from the first ten variables. Finally, we will make our inferences based on what we can see from the graphs.\n\nVisualization of the clusters of the primary data set\nThe following code adds the clusters to the data set and plots every pair of the following variables: ‘CBSA_POP’, ‘Ac_Water’, ‘Ac_Land’, ‘TotPop’, ‘P_WrkAge’, ‘E_HiWageWk’, ‘AutoOwn0’, ‘NatWalkInd’, ‘E_MedWageWk’, ‘Ac_Unpr’. These have been selected due to their importance when describing a census block and due to easy human interpretability. The objective is to see if there are any patterns in the clusters that can be used to make generalizations about the census blocks.\nNote: Only 60% of the data set was used for this visualization due to the computational limitations of my laptop. However, it has been sampled using the stratified sampling method, which ensures that the sample is representative of the population.\n\n\nCode\nwarnings.filterwarnings('ignore')\n\ndf= pd.read_csv(\"../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv\")\nscaler = StandardScaler()\ndf_scaled = (scaler.fit_transform(df))\n\nkmeans = KMeans(n_clusters=12, random_state=42)\ndf['Clusters'] = kmeans.fit_predict(df_scaled)\n\ncolumns_to_plot = ['CBSA_POP', 'Ac_Water', 'Ac_Land', 'TotPop', 'P_WrkAge', 'E_HiWageWk', 'AutoOwn0', 'NatWalkInd', 'E_MedWageWk', 'Ac_Unpr']\n\nsns.pairplot(df, vars=columns_to_plot, hue='Clusters', palette='viridis')\nplt.show()\n\n\n\n\n\nThe previous pairplot is very interesting. While in some pairs of the variables there is a complete overlap of the clusters, in others, we can really see the separation of these. Ac_Land and AC_Unpr, TotPop and AC_Land, AC_Land and P_wrkAge, P_wrkAge and AC_Unpr, and AC_Water and E_MedWage are great visualizations for this. This is a good sign that there seems to be clusters in our data set that are human interpretable and can be used to make generalizations about the census blocks.\n\n\nVisualization of the clusters of the PCA data set\nThe following code adds the clusters to the PCA data set and plots every pair of the first ten variables of the PCA data set. A new csv file has been created (pca_clustered_data.csv) for reproducibility purposes. The objective is to see if there are any patterns in the clusters that can be used to make generalizations about the census blocks and compare the results with the previous pairplots.\nNote: Only 60% of the data set was used for this visualization due to the computational limitations of my laptop. However, it has been sampled using the stratified sampling method, which ensures that the sample is representative of the population.\n\n\nCode\ndf_pca=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv')\n\nnum_clusters = 12\n\nfeatures = df_pca.iloc[:, :]\n\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\ndf_pca['Cluster'] = kmeans.fit_predict(features)\n\ndf_pca.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/pca_clustered_data.csv', index=False)\n\n\n\n\nCode\nwarnings.filterwarnings('ignore')\n\ndf_pca= pd.read_csv(\"../../data/Clean_Data_project_Pub.Transport_5000/pca_clustered_data.csv\")\n\ncolumns_to_plot = df_pca.columns[:10]\n\nsns.pairplot(df_pca, vars=columns_to_plot, hue='Cluster', palette='viridis')\n\nplt.show()\n\n\n\n\n\nThe following pairplot is very interesting. While in some pairs of the variables there is a complete overlap of the clusters, in others, we can really see the separation of these. PC8 and PC5, PC3 and PC5, and PC5 and PC4 are great visualizations for this. This is a good sign that there seems to be clusters in our data set that are human interpretable and can be used to make generalizations about the census blocks.\n\n\nFinal Conclusion\nWe can determine that there seems to be possible clustering in the data set. With more advance computational power, it could be determined with more precision. However, with the methods utilized, we can determine that there are probably 12 clusters in the data set. Not only the clustering scores support this claim, but the pairplots really display good indications that this is the case. These clusters could be used to make generalizations about the census blocks and use them for public transportation planning."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "What is the future of Public Transportation?",
    "section": "",
    "text": "Public transportation is any method of transportation available to the public that is sponsored by governmental entities. In some countries, it is the most used method of transportation while, in others, it is almost inexistent. What are the benefits of having public transportation? Should governmental entities invest on it? What are the preferred methods of transportation? These are just a few questions that every government asks themselves. The public transportation insdustry is one of the biggest lines of business worldwide, with trillions of everyday users. It is so important that cities plan their growth around it in order to provide equal access for everyone. Not only the methods of transport are taken into account, but all the infrastructure surrounding it such as freeways, roads, tunnels, etc. This infrastructure ensures social, economical, and environmental sustainability on the city (Murray et al. 1998)."
  },
  {
    "objectID": "Introduction.html#summary",
    "href": "Introduction.html#summary",
    "title": "What is the future of Public Transportation?",
    "section": "",
    "text": "Public transportation is any method of transportation available to the public that is sponsored by governmental entities. In some countries, it is the most used method of transportation while, in others, it is almost inexistent. What are the benefits of having public transportation? Should governmental entities invest on it? What are the preferred methods of transportation? These are just a few questions that every government asks themselves. The public transportation insdustry is one of the biggest lines of business worldwide, with trillions of everyday users. It is so important that cities plan their growth around it in order to provide equal access for everyone. Not only the methods of transport are taken into account, but all the infrastructure surrounding it such as freeways, roads, tunnels, etc. This infrastructure ensures social, economical, and environmental sustainability on the city (Murray et al. 1998)."
  },
  {
    "objectID": "Introduction.html#motivation",
    "href": "Introduction.html#motivation",
    "title": "What is the future of Public Transportation?",
    "section": "Motivation",
    "text": "Motivation\nI believe that the use of public transportation has an uncountable number of benefits. Growing up with many family members working for the public transportation sector in Madrid (Spain), I have developed a strong sense of value for this industry. While many variables and many different (sometimes opposite) interests need to be taken into account, public transport has become essential for our society. Millions of people depend on it to carry out essential activities such as shopping, going to work, or going to do other daily chores. For this reason, it is an industry that deserves severe research and development. Furthermore, due to COVID-19, the number of riders of public transportation have decreased, and there is an urgent need to bring them back. This will allow cities to be more socially sustainable and reduce the polution levels (UITP 2023).\nThe goal of this research is to make a case about the worth of having public transportation around the world and make recommendations for future steps that should be taken in this field."
  },
  {
    "objectID": "Introduction.html#polution-concerns",
    "href": "Introduction.html#polution-concerns",
    "title": "What is the future of Public Transportation?",
    "section": "Polution concerns",
    "text": "Polution concerns\nThere is a worldwide concern about pollution. Pollution has been increasing since the industrial revolution and has been a major concern for the past 50 years. The main reason for this is the constant increase in the use of fossil fuels and the lack of alternatives. The transportation sector is one of the biggest contributors to pollution. In the United States, the transportation sector is responsible for 29% of the total greenhouse gas emissions (Transportation and Quality 2023). Transportation is a great percentage and one of the main pollutors. For this reason, it is important to study the effects of public transportation on pollution and how it can help reduce it. It can be analyzed internally (how to make public transportation pollute less) or externally (how to make people use public transportation more and private transportation less)."
  },
  {
    "objectID": "Introduction.html#questions",
    "href": "Introduction.html#questions",
    "title": "What is the future of Public Transportation?",
    "section": "Questions",
    "text": "Questions\n\nHow does public transport affect pollution?\nWhat are the most pressing issues related to public transport?\nHow can we address public transportation at a large scale?\nHow does public transportation affect the economy and development of a city?\nTo what extent does public transportation reduce the usage of private transportation?\nWhich factors influence effectiveness of public transportation the most?\nWhat is the reffect of transportation in pollution?\nHas public transportation been improving overtime?\nDo different public transportation methods hold different values (important for investment)?\nWhat is most important to people regarding public transportation?"
  },
  {
    "objectID": "5100.html",
    "href": "5100.html",
    "title": "Behind the Beats: A Feature Analysis of Songs by Spotify’s Fab Five",
    "section": "",
    "text": "This project looks at specific audio features among the top five most streamed spotify artists of 2022. We used the spotify API to download information on seven audio features for each of the five most streamed artists: BTS, Bad Bunny, The Weeknd, Taylor Swift, and Drake. The features studied are: Accousticness, Danceability, Energy, Loudness, Speechiness, Tempo, and Valence. We performed exploratory data analysis to determine our hypothesis, and then performed three different hypothesis tests to determine which features were common among the artists. We performed T-tests, Ranked Sum tests, and bootstrapping tests. We found that the tempo was the same among all of the top five artists, but all of the other audio features were different for at least one artist."
  },
  {
    "objectID": "5100.html#acousticness",
    "href": "5100.html#acousticness",
    "title": "Behind the Beats: A Feature Analysis of Songs by Spotify’s Fab Five",
    "section": "Acousticness",
    "text": "Acousticness\n\n\nT-Test\npairwise.t.test(df$Acousticness, df$Artist_name)\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$Acousticness and df$Artist_name \n\n             Bad Bunny BTS     Drake   Taylor Swift\nBTS          0.00287   -       -       -           \nDrake        1.00000   7.3e-06 -       -           \nTaylor Swift 0.01984   &lt; 2e-16 0.00053 -           \nThe Weeknd   1.00000   2.3e-07 1.00000 0.02917     \n\nP value adjustment method: holm \n\n\nFrom this pairwise T-Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of acousticness between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nBad Bunny and Taylor Swift\nDrake and Taylor Swift\nTaylor Swift and The Weeknd\n\nOther pairs below have p-value higher than 0.05, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of acousticness.\n\nBad Bunny and Drake\nBad Bunny and The Weeknd\nDrake and The Weekend\n\n\n\nWilcoxon Rank Sum Test\npairwise.wilcox.test(df$Acousticness, df$Artist_name)\n\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df$Acousticness and df$Artist_name \n\n             Bad Bunny BTS     Drake Taylor Swift\nBTS          7.2e-11   -       -     -           \nDrake        0.67      1.8e-10 -     -           \nTaylor Swift 1.00      6.4e-15 0.29  -           \nThe Weeknd   1.00      1.7e-09 1.00  0.67        \n\nP value adjustment method: holm \n\n\nFrom this pairwise Wilcoxon Rank Sum Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of acousticness between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\n\nOther pairs below have p-value higher than 0.05, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of acousticness.\n\nBad Bunny and Drake\nBad Bunny and Taylor Swift\nBad Bunny and The Weeknd\nDrake and Taylor Swift\nDrake and The Weekend\nTaylor Swift and The Weeknd\n\n\n\nBootstrap Test\nc1 = quantile(boot.test(ts,bb,\"Acousticness\",1000),c(0.025,0.975))\nc2 = quantile(boot.test(ts,bts,\"Acousticness\",1000),c(0.025,0.975))\nc3 = quantile(boot.test(ts,dk,\"Acousticness\",1000),c(0.025,0.975))\nc4 = quantile(boot.test(ts,wd,\"Acousticness\",1000),c(0.025,0.975))\nc5 = quantile(boot.test(bts,bb,\"Acousticness\",1000),c(0.025,0.975))\nc6 = quantile(boot.test(bts,dk,\"Acousticness\",1000),c(0.025,0.975))\nc7 = quantile(boot.test(bts,wd,\"Acousticness\",1000),c(0.025,0.975))\nc8 = quantile(boot.test(bb,dk,\"Acousticness\",1000),c(0.025,0.975))\nc9 = quantile(boot.test(bb,wd,\"Acousticness\",1000),c(0.025,0.975))\nc10 = quantile(boot.test(dk,wd,\"Acousticness\",1000),c(0.025,0.975))\n\nresult_matrix = matrix(c(\n    c1, c2, c3, c4, c5, c6, c7, c8, c9, c10\n), ncol = 2, byrow = TRUE)\n\nresult_matrix = data.frame(result_matrix)\ninclude_zero = apply(result_matrix, 1, contains_zero)\nresult_matrix[\"Test\"] = include_zero\ncolnames(result_matrix) = c(\"2.5%\",\"97.5%\",\"Test\")\nresult_matrix\n\n\n          2.5%       97.5%  Test\n1   0.03533296  0.12890193 FALSE\n2   0.15030650  0.22174288 FALSE\n3   0.03782212  0.11784582 FALSE\n4   0.01394658  0.10010982 FALSE\n5  -0.14934077 -0.06437997 FALSE\n6  -0.14860508 -0.07382467 FALSE\n7  -0.16916835 -0.09473065 FALSE\n8  -0.05011409  0.04699877  TRUE\n9  -0.07192631  0.02182043  TRUE\n10 -0.06091254  0.02589820  TRUE\n\n\nFrom this Bootstrap Test with a 95% confidence level, we reject the null hypothesis on pairs where the confidence interval doesn’t include 0. That is, there is a significant difference in the means of acousticness between the following pairs of artists:\n\nTaylor Swift and Bad Bunny\nTaylor Swift and Drake\nTaylor Swift and BTS\nTaylor Swift and The Weeknd\nBTS and Bad Bunny\nBTS and Drake\nBTS and The Weeknd\n\nOther pairs below have confidence intervals inclduing 0, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of acousticness.\n\nBad Bunny and Drake\nBad Bunny and The Weeknd\nDrake and The Weeknd"
  },
  {
    "objectID": "5100.html#danceability",
    "href": "5100.html#danceability",
    "title": "Behind the Beats: A Feature Analysis of Songs by Spotify’s Fab Five",
    "section": "Danceability",
    "text": "Danceability\n\n\nT-Test\npairwise.t.test(df$Danceability, df$Artist_name)\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$Danceability and df$Artist_name \n\n             Bad Bunny BTS     Drake   Taylor Swift\nBTS          &lt; 2e-16   -       -       -           \nDrake        3.3e-11   0.0041  -       -           \nTaylor Swift &lt; 2e-16   0.0082  2.8e-09 -           \nThe Weeknd   &lt; 2e-16   1.2e-12 &lt; 2e-16 2.4e-08     \n\nP value adjustment method: holm \n\n\nFrom this pairwise T-Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of danceability between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nTaylor Swift and Bad Bunny\nTaylor Swift and Drake\nTaylor Swift and The Weeknd\nDrake and Bad Bunny\nDrake and The Weeknd\nBad Bunny and the Weeknd\n\nThus, all pairs hold significant difference.\n\n\nWilcoxon Rank Sum Test\npairwise.wilcox.test(df$Danceability, df$Artist_name)\n\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df$Danceability and df$Artist_name \n\n             Bad Bunny BTS     Drake   Taylor Swift\nBTS          &lt; 2e-16   -       -       -           \nDrake        3.4e-09   0.0089  -       -           \nTaylor Swift &lt; 2e-16   0.0089  1.8e-07 -           \nThe Weeknd   &lt; 2e-16   2.1e-06 3.8e-12 0.0024      \n\nP value adjustment method: holm \n\n\nFrom this pairwise Wilcoxon Rank Sum Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of danceability between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nTaylor Swift and Bad Bunny\nTaylor Swift and Drake\nTaylor Swift and The Weeknd\nDrake and Bad Bunny\nDrake and The Weeknd\nBad Bunny and the Weeknd\n\nThus, all pairs hold significant difference.\n\n\nBootstrap Test\nc1 = quantile(boot.test(ts,bb,\"Danceability\",1000),c(0.025,0.975))\nc2 = quantile(boot.test(ts,bts,\"Danceability\",1000),c(0.025,0.975))\nc3 = quantile(boot.test(ts,dk,\"Danceability\",1000),c(0.025,0.975))\nc4 = quantile(boot.test(ts,wd,\"Danceability\",1000),c(0.025,0.975))\nc5 = quantile(boot.test(bts,bb,\"Danceability\",1000),c(0.025,0.975))\nc6 = quantile(boot.test(bts,dk,\"Danceability\",1000),c(0.025,0.975))\nc7 = quantile(boot.test(bts,wd,\"Danceability\",1000),c(0.025,0.975))\nc8 = quantile(boot.test(bb,dk,\"Danceability\",1000),c(0.025,0.975))\nc9 = quantile(boot.test(bb,wd,\"Danceability\",1000),c(0.025,0.975))\nc10 = quantile(boot.test(dk,wd,\"Danceability\",1000),c(0.025,0.975))\n\nresult_matrix = matrix(c(\n    c1, c2, c3, c4, c5, c6, c7, c8, c9, c10\n), ncol = 2, byrow = TRUE)\n\nresult_matrix = data.frame(result_matrix)\ninclude_zero = apply(result_matrix, 1, contains_zero)\nresult_matrix[\"Test\"] = include_zero\ncolnames(result_matrix) = c(\"2.5%\",\"97.5%\",\"Test\")\nresult_matrix\n\n\n          2.5%        97.5%  Test\n1  -0.18390401 -0.140593444 FALSE\n2  -0.04311440 -0.009922037 FALSE\n3  -0.07810026 -0.040493642 FALSE\n4   0.03570075  0.083669745 FALSE\n5  -0.15966598 -0.113828349 FALSE\n6  -0.05500671 -0.012080898 FALSE\n7   0.06166927  0.111637293 FALSE\n8   0.07801938  0.130198038 FALSE\n9   0.19249438  0.251553016 FALSE\n10  0.09324806  0.146084848 FALSE\n\n\nFrom this Bootstrap Test with a 95% confidence level, we reject the null hypothesis on pairs where the confidence interval doesn’t include 0. That is, there is a significant difference in the means of danceability between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nTaylor Swift and Bad Bunny\nTaylor Swift and Drake\nTaylor Swift and The Weeknd\nDrake and Bad Bunny\nDrake and The Weeknd\nBad Bunny and the Weeknd\n\nThus, all pairs hold significant difference."
  },
  {
    "objectID": "5100.html#energy",
    "href": "5100.html#energy",
    "title": "Behind the Beats: A Feature Analysis of Songs by Spotify’s Fab Five",
    "section": "Energy",
    "text": "Energy\n\n\nT-Test\npairwise.t.test(df$Energy, df$Artist_name)\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$Energy and df$Artist_name \n\n             Bad Bunny BTS     Drake   Taylor Swift\nBTS          9.9e-08   -       -       -           \nDrake        8.3e-09   &lt; 2e-16 -       -           \nTaylor Swift 3.4e-06   &lt; 2e-16 0.05909 -           \nThe Weeknd   0.00014   &lt; 2e-16 0.05190 0.54488     \n\nP value adjustment method: holm \n\n\nFrom this pairwise T-Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of energy between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nBad Bunny and Drake\nBad Bunny and Taylor Swift\nBad Bunny and The Weekend\n\nOther pairs below have p-value higher than 0.05, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of energy.\n\nDrake and Taylor Swift\nDrake and The Weeknd\nTaylor Swift and The Weeknd\n\n\n\nWilcoxon Rank Sum Test\npairwise.wilcox.test(df$Energy, df$Artist_name)\n\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df$Energy and df$Artist_name \n\n             Bad Bunny BTS     Drake   Taylor Swift\nBTS          1.3e-14   -       -       -           \nDrake        7.7e-11   &lt; 2e-16 -       -           \nTaylor Swift 3.3e-05   &lt; 2e-16 0.04077 -           \nThe Weeknd   0.00013   &lt; 2e-16 0.00448 0.49284     \n\nP value adjustment method: holm \n\n\nFrom this pairwise Wilcoxon Rank Sum Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of energy between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nBad Bunny and Drake\nBad Bunny and Taylor Swift\nBad Bunny and The Weeknd\nDrake and Taylor Swift\nDrake and The Weekend\n\nOther pairs below have p-value higher than 0.05, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of energy.\n\nTaylor Swift and The Weeknd\n\n\n\nBootstrap Test\nc1 = quantile(boot.test(ts,bb,\"Energy\",1000),c(0.025,0.975))\nc2 = quantile(boot.test(ts,bts,\"Energy\",1000),c(0.025,0.975))\nc3 = quantile(boot.test(ts,dk,\"Energy\",1000),c(0.025,0.975))\nc4 = quantile(boot.test(ts,wd,\"Energy\",1000),c(0.025,0.975))\nc5 = quantile(boot.test(bts,bb,\"Energy\",1000),c(0.025,0.975))\nc6 = quantile(boot.test(bts,dk,\"Energy\",1000),c(0.025,0.975))\nc7 = quantile(boot.test(bts,wd,\"Energy\",1000),c(0.025,0.975))\nc8 = quantile(boot.test(bb,dk,\"Energy\",1000),c(0.025,0.975))\nc9 = quantile(boot.test(bb,wd,\"Energy\",1000),c(0.025,0.975))\nc10 = quantile(boot.test(dk,wd,\"Energy\",1000),c(0.025,0.975))\n\nresult_matrix = matrix(c(\n    c1, c2, c3, c4, c5, c6, c7, c8, c9, c10\n), ncol = 2, byrow = TRUE)\n\nresult_matrix = data.frame(result_matrix)\ninclude_zero = apply(result_matrix, 1, contains_zero)\nresult_matrix[\"Test\"] = include_zero\ncolnames(result_matrix) = c(\"2.5%\",\"97.5%\",\"Test\")\nresult_matrix\n\n\n           2.5%        97.5%  Test\n1  -0.116691109 -0.062326264 FALSE\n2  -0.223707026 -0.176844856 FALSE\n3   0.002555531  0.051410740 FALSE\n4  -0.035290026  0.021467100  TRUE\n5   0.081173817  0.137668127 FALSE\n6   0.203286302  0.252871030 FALSE\n7   0.165456419  0.220816220 FALSE\n8   0.089295414  0.147130415 FALSE\n9   0.054836050  0.112628271 FALSE\n10 -0.065007252 -0.007318004 FALSE\n\n\nFrom this Bootstrap Test with a 95% confidence level, we reject the null hypothesis on pairs where the confidence interval doesn’t include 0. That is, there is a significant difference in the means of energy between the following pairs of artists:\n\nTaylor Swift and Bad Bunny\nTaylor Swift and Drake\nTaylor Swift and BTS\nBTS and Bad Bunny\nBTS and Drake\nBTS and The Weeknd\nBad Bunny and Drake\nBad Bunny and The Weeknd\nDrake and The Weeknd\n\nOther pairs below have confidence intervals inclduing 0, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of energy.\n\nTaylor Swift and The Weeknd"
  },
  {
    "objectID": "5100.html#loudness",
    "href": "5100.html#loudness",
    "title": "Behind the Beats: A Feature Analysis of Songs by Spotify’s Fab Five",
    "section": "Loudness",
    "text": "Loudness\n\n\nT-Test\npairwise.t.test(df$Loudness, df$Artist_name)\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$Loudness and df$Artist_name \n\n             Bad Bunny BTS     Drake  Taylor Swift\nBTS          0.0097    -       -      -           \nDrake        2.0e-11   &lt; 2e-16 -      -           \nTaylor Swift 1.3e-06   &lt; 2e-16 0.0030 -           \nThe Weeknd   1.9e-14   &lt; 2e-16 0.1199 6.6e-06     \n\nP value adjustment method: holm \n\n\nFrom this pairwise T-Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of loudness between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nBad Bunny and Drake\nBad Bunny and Taylor Swift\nBad Bunny and The Weekend\nDrake and Taylor Swift\nTaylor Swift and The Weeknd\n\nOther pairs below have p-value higher than 0.05, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of loudness.\n\nDrake and The Weeknd\n\n\n\nWilcoxon Rank Sum Test\npairwise.wilcox.test(df$Loudness, df$Artist_name)\n\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df$Loudness and df$Artist_name \n\n             Bad Bunny BTS     Drake   Taylor Swift\nBTS          1.7e-09   -       -       -           \nDrake        &lt; 2e-16   &lt; 2e-16 -       -           \nTaylor Swift 1.3e-07   &lt; 2e-16 0.00048 -           \nThe Weeknd   &lt; 2e-16   &lt; 2e-16 0.63434 0.00088     \n\nP value adjustment method: holm \n\n\nFrom this pairwise Wilcoxon Rank Sum Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of loudness between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nBad Bunny and Drake\nBad Bunny and Taylor Swift\nBad Bunny and The Weekend\nDrake and Taylor Swift\nTaylor Swift and The Weeknd\n\nOther pairs below have p-value higher than 0.05, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of loudness.\n\nDrake and The Weeknd\n\n\n\nBootstrap Test\nc1 = quantile(boot.test(ts,bb,\"Loudness\",1000),c(0.025,0.975))\nc2 = quantile(boot.test(ts,bts,\"Loudness\",1000),c(0.025,0.975))\nc3 = quantile(boot.test(ts,dk,\"Loudness\",1000),c(0.025,0.975))\nc4 = quantile(boot.test(ts,wd,\"Loudness\",1000),c(0.025,0.975))\nc5 = quantile(boot.test(bts,bb,\"Loudness\",1000),c(0.025,0.975))\nc6 = quantile(boot.test(bts,dk,\"Loudness\",1000),c(0.025,0.975))\nc7 = quantile(boot.test(bts,wd,\"Loudness\",1000),c(0.025,0.975))\nc8 = quantile(boot.test(bb,dk,\"Loudness\",1000),c(0.025,0.975))\nc9 = quantile(boot.test(bb,wd,\"Loudness\",1000),c(0.025,0.975))\nc10 = quantile(boot.test(dk,wd,\"Loudness\",1000),c(0.025,0.975))\n\nresult_matrix = matrix(c(\n    c1, c2, c3, c4, c5, c6, c7, c8, c9, c10\n), ncol = 2, byrow = TRUE)\n\nresult_matrix = data.frame(result_matrix)\ninclude_zero = apply(result_matrix, 1, contains_zero)\nresult_matrix[\"Test\"] = include_zero\ncolnames(result_matrix) = c(\"2.5%\",\"97.5%\",\"Test\")\nresult_matrix\n\n\n         2.5%      97.5%  Test\n1  -2.0162853 -1.2149343 FALSE\n2  -2.9255326 -2.1542398 FALSE\n3   0.2974588  1.1012439 FALSE\n4   0.5993673  1.7192915 FALSE\n5   0.4850108  1.3521287 FALSE\n6   2.8528393  3.7147493 FALSE\n7   3.1460527  4.2128561 FALSE\n8   1.8977360  2.7902334 FALSE\n9   2.1493249  3.3053576 FALSE\n10 -0.1742982  0.9838291  TRUE\n\n\nFrom this Bootstrap Test with a 95% confidence level, we reject the null hypothesis on pairs where the confidence interval doesn’t include 0. That is, there is a significant difference in the means of loudness between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nBad Bunny and Drake\nBad Bunny and Taylor Swift\nBad Bunny and The Weekend\nDrake and Taylor Swift\nTaylor Swift and The Weeknd\n\nOther pairs below have confidence intervals inclduing 0, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of loudness.\n\nDrake and The Weeknd"
  },
  {
    "objectID": "5100.html#speechiness",
    "href": "5100.html#speechiness",
    "title": "Behind the Beats: A Feature Analysis of Songs by Spotify’s Fab Five",
    "section": "Speechiness",
    "text": "Speechiness\n\n\nT-Test\npairwise.t.test(df$Speechiness, df$Artist_name)\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$Speechiness and df$Artist_name \n\n             Bad Bunny BTS     Drake   Taylor Swift\nBTS          0.036     -       -       -           \nDrake        2.3e-10   3.7e-08 -       -           \nTaylor Swift 8.3e-10   &lt; 2e-16 &lt; 2e-16 -           \nThe Weeknd   1.1e-05   &lt; 2e-16 &lt; 2e-16 0.067       \n\nP value adjustment method: holm \n\n\nFrom this pairwise T-Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of speechiness between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nBad Bunny and Drake\nBad Bunny and Taylor Swift\nBad Bunny and The Weeknd\nDrake and Taylor Swift\nDrake and The Weeknd\n\nOther pairs below have p-value higher than 0.05, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of speechiness.\n\nTaylor Swift and The Weeknd\n\n\n\nWilcoxon Rank Sum Test\npairwise.wilcox.test(df$Speechiness, df$Artist_name)\n\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df$Speechiness and df$Artist_name \n\n             Bad Bunny BTS     Drake   Taylor Swift\nBTS          0.18      -       -       -           \nDrake        2.2e-07   5.2e-08 -       -           \nTaylor Swift &lt; 2e-16   &lt; 2e-16 &lt; 2e-16 -           \nThe Weeknd   9.5e-11   &lt; 2e-16 &lt; 2e-16 4.4e-15     \n\nP value adjustment method: holm \n\n\nFrom this pairwise Wilcoxon Rank Sum Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of speechiness between the following pairs of artists:\n– BTS and Drake - BTS and Taylor Swift - BTS and The Weeknd - Bad Bunny and Drake - Bad Bunny and Taylor Swift - Bad Bunny and The Weeknd - Drake and Taylor Swift - Drake and The Weeknd - Taylor Swift and The Weeknd\nOther pairs below have p-value higher than 0.05, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of speechiness.\n\nBTS and Bad Bunny\n\n\n\nBootstrap Test\nc1 = quantile(boot.test(ts,bb,\"Speechiness\",1000),c(0.025,0.975))\nc2 = quantile(boot.test(ts,bts,\"Speechiness\",1000),c(0.025,0.975))\nc3 = quantile(boot.test(ts,dk,\"Speechiness\",1000),c(0.025,0.975))\nc4 = quantile(boot.test(ts,wd,\"Speechiness\",1000),c(0.025,0.975))\nc5 = quantile(boot.test(bts,bb,\"Speechiness\",1000),c(0.025,0.975))\nc6 = quantile(boot.test(bts,dk,\"Speechiness\",1000),c(0.025,0.975))\nc7 = quantile(boot.test(bts,wd,\"Speechiness\",1000),c(0.025,0.975))\nc8 = quantile(boot.test(bb,dk,\"Speechiness\",1000),c(0.025,0.975))\nc9 = quantile(boot.test(bb,wd,\"Speechiness\",1000),c(0.025,0.975))\nc10 = quantile(boot.test(dk,wd,\"Speechiness\",1000),c(0.025,0.975))\n\nresult_matrix = matrix(c(\n    c1, c2, c3, c4, c5, c6, c7, c8, c9, c10\n), ncol = 2, byrow = TRUE)\n\nresult_matrix = data.frame(result_matrix)\ninclude_zero = apply(result_matrix, 1, contains_zero)\nresult_matrix[\"Test\"] = include_zero\ncolnames(result_matrix) = c(\"2.5%\",\"97.5%\",\"Test\")\nresult_matrix\n\n\n           2.5%        97.5%  Test\n1  -0.097002941 -0.054936326 FALSE\n2  -0.122815782 -0.085048724 FALSE\n3  -0.173885158 -0.139494985 FALSE\n4  -0.024383283 -0.005721025 FALSE\n5   0.003585266  0.056215699 FALSE\n6  -0.076030121 -0.028700029 FALSE\n7   0.068028423  0.106450626 FALSE\n8  -0.106676198 -0.057226445 FALSE\n9   0.038186327  0.080652485 FALSE\n10  0.123787832  0.157912247 FALSE\n\n\nFrom this Bootstrap Test with a 95% confidence level, we reject the null hypothesis on pairs where the confidence interval doesn’t include 0. That is, there is a significant difference in the means of speechiness between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nTaylor Swift and Bad Bunny\nTaylor Swift and Drake\nTaylor Swift and The Weeknd\nDrake and Bad Bunny\nDrake and The Weeknd\nBad Bunny and the Weeknd\n\nThus, all pairs hold significant difference."
  },
  {
    "objectID": "5100.html#tempo",
    "href": "5100.html#tempo",
    "title": "Behind the Beats: A Feature Analysis of Songs by Spotify’s Fab Five",
    "section": "Tempo",
    "text": "Tempo\n\n\nT-Test\npairwise.t.test(df$Tempo, df$Artist_name)\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$Tempo and df$Artist_name \n\n             Bad Bunny BTS Drake Taylor Swift\nBTS          1         -   -     -           \nDrake        1         1   -     -           \nTaylor Swift 1         1   1     -           \nThe Weeknd   1         1   1     1           \n\nP value adjustment method: holm \n\n\nFrom this pairwise T-Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is not a significant difference in the means of tempo between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nTaylor Swift and Bad Bunny\nTaylor Swift and Drake\nTaylor Swift and The Weeknd\nDrake and Bad Bunny\nDrake and The Weeknd\nBad Bunny and the Weeknd\n\nThus, we fail to reject that any pair of artists hold a significant difference.\n\n\nWilcoxon Rank Sum Test\npairwise.wilcox.test(df$Tempo, df$Artist_name)\n\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df$Tempo and df$Artist_name \n\n             Bad Bunny BTS Drake Taylor Swift\nBTS          1         -   -     -           \nDrake        1         1   -     -           \nTaylor Swift 1         1   1     -           \nThe Weeknd   1         1   1     1           \n\nP value adjustment method: holm \n\n\nFrom this pairwise Wilcoxon Rank Sum Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is not a significant difference in the means of tempo between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nTaylor Swift and Bad Bunny\nTaylor Swift and Drake\nTaylor Swift and The Weeknd\nDrake and Bad Bunny\nDrake and The Weeknd\nBad Bunny and the Weeknd\n\nThus, we fail to reject that any pair of artists hold a significant difference.\n\n\nBootstrap Test\nc1 = quantile(boot.test(ts,bb,\"Tempo\",1000),c(0.025,0.975))\nc2 = quantile(boot.test(ts,bts,\"Tempo\",1000),c(0.025,0.975))\nc3 = quantile(boot.test(ts,dk,\"Tempo\",1000),c(0.025,0.975))\nc4 = quantile(boot.test(ts,wd,\"Tempo\",1000),c(0.025,0.975))\nc5 = quantile(boot.test(bts,bb,\"Tempo\",1000),c(0.025,0.975))\nc6 = quantile(boot.test(bts,dk,\"Tempo\",1000),c(0.025,0.975))\nc7 = quantile(boot.test(bts,wd,\"Tempo\",1000),c(0.025,0.975))\nc8 = quantile(boot.test(bb,dk,\"Tempo\",1000),c(0.025,0.975))\nc9 = quantile(boot.test(bb,wd,\"Tempo\",1000),c(0.025,0.975))\nc10 = quantile(boot.test(dk,wd,\"Tempo\",1000),c(0.025,0.975))\n\nresult_matrix = matrix(c(\n    c1, c2, c3, c4, c5, c6, c7, c8, c9, c10\n), ncol = 2, byrow = TRUE)\n\nresult_matrix = data.frame(result_matrix)\ninclude_zero = apply(result_matrix, 1, contains_zero)\nresult_matrix[\"Test\"] = include_zero\ncolnames(result_matrix) = c(\"2.5%\",\"97.5%\",\"Test\")\nresult_matrix\n\n\n         2.5%    97.5% Test\n1  -5.2959263 7.489606 TRUE\n2  -2.2524370 5.470697 TRUE\n3  -0.8699923 7.377373 TRUE\n4  -1.5355370 6.746366 TRUE\n5  -7.1036031 6.698293 TRUE\n6  -2.7403242 6.171592 TRUE\n7  -4.1822640 5.404266 TRUE\n8  -4.7232067 8.514601 TRUE\n9  -5.6123360 7.397094 TRUE\n10 -5.7779150 3.644051 TRUE\n\n\nFrom this Bootstrap Test with a 95% confidence level, we reject the null hypothesis on pairs where the confidence interval doesn’t include 0. That is, there is not a significant difference in the means of tempo between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nTaylor Swift and Bad Bunny\nTaylor Swift and Drake\nTaylor Swift and The Weeknd\nDrake and Bad Bunny\nDrake and The Weeknd\nBad Bunny and the Weeknd\n\nThus, we fail to reject that any pair of artists hold a significant difference."
  },
  {
    "objectID": "5100.html#valence",
    "href": "5100.html#valence",
    "title": "Behind the Beats: A Feature Analysis of Songs by Spotify’s Fab Five",
    "section": "Valence",
    "text": "Valence\n\n\nT-Test\npairwise.t.test(df$Valence, df$Artist_name)\n\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  df$Valence and df$Artist_name \n\n             Bad Bunny BTS     Drake   Taylor Swift\nBTS          0.03659   -       -       -           \nDrake        4.9e-09   &lt; 2e-16 -       -           \nTaylor Swift 0.00011   &lt; 2e-16 0.00193 -           \nThe Weeknd   2.3e-12   &lt; 2e-16 0.07401 1.2e-06     \n\nP value adjustment method: holm \n\n\nFrom this pairwise T-Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of valence between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nBad Bunny and Taylor Swift\nBad Bunny and Drake\nBad Bunny and The Weeknd\nDrake and Taylor Swift\nTaylor Swift and The Weeknd\n\nOther pairs below have p-value higher than 0.05, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of valence.\n\nDrake and The Weekend\n\n\n\nWilcoxon Rank Sum Test\npairwise.wilcox.test(df$Valence, df$Artist_name)\n\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df$Valence and df$Artist_name \n\n             Bad Bunny BTS     Drake   Taylor Swift\nBTS          0.03206   -       -       -           \nDrake        1.2e-07   &lt; 2e-16 -       -           \nTaylor Swift 0.00081   &lt; 2e-16 0.00081 -           \nThe Weeknd   5.8e-10   &lt; 2e-16 0.03206 1.4e-07     \n\nP value adjustment method: holm \n\n\nFrom this pairwise Wilcoxon Rank Sum Test with a 95% confidence level, we reject the null hypothesis on pairs that have p-value less than 0.05. That is, there is a significant difference in the means of valence between the following pairs of artists:\n\nBTS and Bad Bunny\nBTS and Drake\nBTS and Taylor Swift\nBTS and The Weeknd\nBad Bunny and Drake\nBad Bunny and Taylor Swift\nBad Bunny and The Weeknd\nDrake and Taylor Swift\nDrake and The Weekend\nTaylor Swift and The Weeknd\n\nThus, all pairs hold significant difference.\n\n\nBootstrap Test\nc1 = quantile(boot.test(ts,bb,\"Valence\",1000),c(0.025,0.975))\nc2 = quantile(boot.test(ts,bts,\"Valence\",1000),c(0.025,0.975))\nc3 = quantile(boot.test(ts,dk,\"Valence\",1000),c(0.025,0.975))\nc4 = quantile(boot.test(ts,wd,\"Valence\",1000),c(0.025,0.975))\nc5 = quantile(boot.test(bts,bb,\"Valence\",1000),c(0.025,0.975))\nc6 = quantile(boot.test(bts,dk,\"Valence\",1000),c(0.025,0.975))\nc7 = quantile(boot.test(bts,wd,\"Valence\",1000),c(0.025,0.975))\nc8 = quantile(boot.test(bb,dk,\"Valence\",1000),c(0.025,0.975))\nc9 = quantile(boot.test(bb,wd,\"Valence\",1000),c(0.025,0.975))\nc10 = quantile(boot.test(dk,wd,\"Valence\",1000),c(0.025,0.975))\n\nresult_matrix = matrix(c(\n    c1, c2, c3, c4, c5, c6, c7, c8, c9, c10\n), ncol = 2, byrow = TRUE)\n\nresult_matrix = data.frame(result_matrix)\ninclude_zero = apply(result_matrix, 1, contains_zero)\nresult_matrix[\"Test\"] = include_zero\ncolnames(result_matrix) = c(\"2.5%\",\"97.5%\",\"Test\")\nresult_matrix\n\n\n           2.5%       97.5%  Test\n1  -0.133732186 -0.03990080 FALSE\n2  -0.170775072 -0.11440452 FALSE\n3   0.023498068  0.07690240 FALSE\n4   0.051366914  0.11083356 FALSE\n5   0.002181739  0.09998707 FALSE\n6   0.159851298  0.22222725 FALSE\n7   0.187213802  0.25668078 FALSE\n8   0.090451696  0.18566377 FALSE\n9   0.119957741  0.21792398 FALSE\n10 -0.002718606  0.06488613  TRUE\n\n\nFrom this Bootstrap Test with a 95% confidence level, we reject the null hypothesis on pairs where the confidence interval doesn’t include 0. That is, there is a significant difference in the means of valence between the following pairs of artists:\n\nTaylor Swift and Bad Bunny\nTaylor Swift and Drake\nTaylor Swift and BTS\nTaylor Swift and The Weeknd\nBTS and Bad Bunny\nBTS and Drake\nBTS and The Weeknd\nBad Bunny and Drake\nBad Bunny and The Weeknd\n\nOther pairs below have confidence intervals inclduing 0, which we fail to rejct the null hypotheis. That is there is no significant difference in the means of valence.\n\nDrake and The Weeknd"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jorge Bris Moreno",
    "section": "",
    "text": "I am an aspiring data scientist who wants to make an impact through thoughtful decision-making.\nMy sweet spot is drawing conclusions from data and transmitting them.\nGrowing up, my parents taught me to always make a list of pros and cons before making an important decision. This would help me visualize if a choice is the right one or not. And this is what has led me to where I am at today. We live in a world surrounded by underutilized data. Every decision, the same as a list of pros and cons, can be backed up by properly analyzing data.\nDrawing conclusions from this data is where my skills in statistics, data science, modeling, and programming skills come into place. My current and previous studies have helped me develop these skills while my experiences in different fields such as project management, entrepreneurship, sales, and teaching have helped me put them into practice while building my soft skills."
  },
  {
    "objectID": "index.html#meet-me",
    "href": "index.html#meet-me",
    "title": "Jorge Bris Moreno",
    "section": "",
    "text": "I am an aspiring data scientist who wants to make an impact through thoughtful decision-making.\nMy sweet spot is drawing conclusions from data and transmitting them.\nGrowing up, my parents taught me to always make a list of pros and cons before making an important decision. This would help me visualize if a choice is the right one or not. And this is what has led me to where I am at today. We live in a world surrounded by underutilized data. Every decision, the same as a list of pros and cons, can be backed up by properly analyzing data.\nDrawing conclusions from this data is where my skills in statistics, data science, modeling, and programming skills come into place. My current and previous studies have helped me develop these skills while my experiences in different fields such as project management, entrepreneurship, sales, and teaching have helped me put them into practice while building my soft skills."
  },
  {
    "objectID": "index.html#growing-up",
    "href": "index.html#growing-up",
    "title": "Jorge Bris Moreno",
    "section": "Growing Up",
    "text": "Growing Up\n\n\n\n\n\nThis is my family in Madrid!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jorge Bris Moreno",
    "section": "Education",
    "text": "Education\nGeorgetown University | Washington, DC\n\nM.S in Data Science & Analytics | Jun 2023 - May 2025\nI am currently studying a MS in Data Science and Analytics at Georgetown University.\nFranklin & Marshall College | Lancaster, PA\n\nB.A in Mathematics and Business | Aug 2019 - May 2023\nI double majored in Mathematics and Business at Franklin & Marshall College. During that time, I played varsity tennis, rugby, was part of the Google Student Developer Club, and was a member of Ware Parliament."
  },
  {
    "objectID": "index.html#hobbies",
    "href": "index.html#hobbies",
    "title": "Jorge Bris Moreno",
    "section": "Hobbies",
    "text": "Hobbies\nI love playing sports, especially tennis and rugby. However, I love any other outdoor activity such as hiking, skiing, snowboarding, surfing, and many more! I strongly believe that sports are a great way to not only learn about yourself but be more disciplined and develop a strong work ethic.\nI also love photography. I have been taking pictures for a long time now and I love it. I love capturing moments and being able to look back at them. Photography allows you to look at the world from a different perspective and appreciate the little things in life. (You can see some of my work along with the awesome photographer Raluca Rilla’s pics at @lascallesde in instagram!)"
  },
  {
    "objectID": "index.html#relevant-experience",
    "href": "index.html#relevant-experience",
    "title": "Jorge Bris Moreno",
    "section": "Relevant Experience",
    "text": "Relevant Experience\nStatistician | PhD student’s research colaborator\n\nNov 2022 - Present\nClient Engagement & Resource Assistant | ASSETS\n\nSept 2022 - May 2023\nProject Manager Intern | Miller Mats\n\nMay 2022 - Aug 2022"
  },
  {
    "objectID": "trees.html",
    "href": "trees.html",
    "title": "Regression trees and Random Forests",
    "section": "",
    "text": "The goal of this tab is to build a model to predict the average miles per galon (counting gasoline and diesel). We wil use the following variables as predictors: city, agency, organization type, mode (A system for carrying transit passengers described by specific right-of-way (ROW), technology and operational features), TOS (Describes how public transportation services are provided by the transit agency: directly operated (DO) or purchased transportation (PT) services), and Electric Battery (referring to the use of electric batteries to power the vehicle).\nThat way, we can not only predict the average miles per galon, but also understand which variables are more important to predict the average miles per galon. This could give us insights on what to do to improve the average miles per galon, which factors are better, and how much fossil fuels we will use in the future with different combinations of all these variables.\n\n\nDecision trees are machine learning models that try to predict the value of a target variable by learning simple decision rules inferred from the data features. They can be used for either classification or regression models. While the can handle both categorical and numerical data at once for either type of model (classification or regression), in implementation, some of the oposite variables will need to be transformed with one-hot encoding in order to run the algorithm (tipically for handling categorical variables). Each internal node in the tree represents a feature or attribute, each branch a decision rule, and each leaf a prediction. The prediction then is always done from the top of the tree to the bottom by following each decision rule.\nDecision trees are great for interpreteability and visualization. Their hierarchical structure allow us to visualize the importance of every variable in the decision making process of the tree. However, their tendency to overfit the data makes them less useful for prediction. They tend to capture the noise of the data into their model. To solve this problem, we can use methods like purning, which takes awa the “non-contributing” parts of the tree or hyperparameter tunning, which changes the parameters of the model to make it more generalizable.\nApplying decision trees requires to have a target variable (the one that we want to predict) and a set of features (the ones that we will use to predict the target variable). Classification trees focusses on predicting labels while regression trees on numerical values. Depending on your objective and your data, you can use oine or the other. For example, in this case, we want to predict the average miles per galon, which is a continuous variable, so we will use a regression tree. Nevertheless, if we wanted to take a different approach, we could always convert that variable into a categorical one (dividing the data into low mpg, medium mpg, and high mpg), and then use a classification tree instead.\n\n\n\nRandom forests are another machine learning algorithm that takes advantage of decision trees. They combine multiple number of decision trees to make a prediction. Each tree operates individually. In order to generated, random forests use the bootstrapping method to randomly select a subset of the data and also randomly select a subset of the features. This way, each tree is different from the others. this variability tries to reduce the overfitting of the model. However, they also tend to overfit, the same way as a decision tree. To solve this, we can utilize methods like hyperparameter tunning to make the model more generalizable. It is also worth notiong that, since they are built with trees, they can also handle both categorical and numerical data at once for either type of model (classification or regression) but, in implementation, the one-hot encoding needs to be utilized the same way as for a decision tree.\nFor classification, the final prediction is the one that has the majority of votes from the trees (each tree having one vote). For regression, the final prediction is the overall average of the predictions from each tree. This approach aims for a more accurate than a single decision tree (but this is not always the case). Furthermore, the differences of the features on each tree allows random forests to better handle noise and outliers.\nRandom forests can be used in many different situations. Some of the most famous ones are healthcare, finance, or image recognition. Their flexibility and high performance make them a great choice for many different problems. Furthermore, since they are built with trees, they are also great for interpreteability and visualization.\n\n\n\nFor both, decision trees and random forests, hyperparameter tunning is a great way to improve the performance of the model. Thus, we will use it for both. Hyperparameters are the con figuration of settings in order to build a model. in decision trees, we will focus on how the depth affect the model accruacy and chose the optimal one to train the model. In random forests, we will focus on the number of trees and the depth of each tree. We will use the same approach as for decision trees to find and then train our optimal model.\n\n\n\nThis model will randomly select values and apply them to our predictions. We will use this model as our baseline to compare the performance of our other models."
  },
  {
    "objectID": "trees.html#explanation-of-the-modeling-methods",
    "href": "trees.html#explanation-of-the-modeling-methods",
    "title": "Regression trees and Random Forests",
    "section": "",
    "text": "The goal of this tab is to build a model to predict the average miles per galon (counting gasoline and diesel). We wil use the following variables as predictors: city, agency, organization type, mode (A system for carrying transit passengers described by specific right-of-way (ROW), technology and operational features), TOS (Describes how public transportation services are provided by the transit agency: directly operated (DO) or purchased transportation (PT) services), and Electric Battery (referring to the use of electric batteries to power the vehicle).\nThat way, we can not only predict the average miles per galon, but also understand which variables are more important to predict the average miles per galon. This could give us insights on what to do to improve the average miles per galon, which factors are better, and how much fossil fuels we will use in the future with different combinations of all these variables.\n\n\nDecision trees are machine learning models that try to predict the value of a target variable by learning simple decision rules inferred from the data features. They can be used for either classification or regression models. While the can handle both categorical and numerical data at once for either type of model (classification or regression), in implementation, some of the oposite variables will need to be transformed with one-hot encoding in order to run the algorithm (tipically for handling categorical variables). Each internal node in the tree represents a feature or attribute, each branch a decision rule, and each leaf a prediction. The prediction then is always done from the top of the tree to the bottom by following each decision rule.\nDecision trees are great for interpreteability and visualization. Their hierarchical structure allow us to visualize the importance of every variable in the decision making process of the tree. However, their tendency to overfit the data makes them less useful for prediction. They tend to capture the noise of the data into their model. To solve this problem, we can use methods like purning, which takes awa the “non-contributing” parts of the tree or hyperparameter tunning, which changes the parameters of the model to make it more generalizable.\nApplying decision trees requires to have a target variable (the one that we want to predict) and a set of features (the ones that we will use to predict the target variable). Classification trees focusses on predicting labels while regression trees on numerical values. Depending on your objective and your data, you can use oine or the other. For example, in this case, we want to predict the average miles per galon, which is a continuous variable, so we will use a regression tree. Nevertheless, if we wanted to take a different approach, we could always convert that variable into a categorical one (dividing the data into low mpg, medium mpg, and high mpg), and then use a classification tree instead.\n\n\n\nRandom forests are another machine learning algorithm that takes advantage of decision trees. They combine multiple number of decision trees to make a prediction. Each tree operates individually. In order to generated, random forests use the bootstrapping method to randomly select a subset of the data and also randomly select a subset of the features. This way, each tree is different from the others. this variability tries to reduce the overfitting of the model. However, they also tend to overfit, the same way as a decision tree. To solve this, we can utilize methods like hyperparameter tunning to make the model more generalizable. It is also worth notiong that, since they are built with trees, they can also handle both categorical and numerical data at once for either type of model (classification or regression) but, in implementation, the one-hot encoding needs to be utilized the same way as for a decision tree.\nFor classification, the final prediction is the one that has the majority of votes from the trees (each tree having one vote). For regression, the final prediction is the overall average of the predictions from each tree. This approach aims for a more accurate than a single decision tree (but this is not always the case). Furthermore, the differences of the features on each tree allows random forests to better handle noise and outliers.\nRandom forests can be used in many different situations. Some of the most famous ones are healthcare, finance, or image recognition. Their flexibility and high performance make them a great choice for many different problems. Furthermore, since they are built with trees, they are also great for interpreteability and visualization.\n\n\n\nFor both, decision trees and random forests, hyperparameter tunning is a great way to improve the performance of the model. Thus, we will use it for both. Hyperparameters are the con figuration of settings in order to build a model. in decision trees, we will focus on how the depth affect the model accruacy and chose the optimal one to train the model. In random forests, we will focus on the number of trees and the depth of each tree. We will use the same approach as for decision trees to find and then train our optimal model.\n\n\n\nThis model will randomly select values and apply them to our predictions. We will use this model as our baseline to compare the performance of our other models."
  },
  {
    "objectID": "trees.html#data-and-libraries",
    "href": "trees.html#data-and-libraries",
    "title": "Regression trees and Random Forests",
    "section": "Data and libraries",
    "text": "Data and libraries\nThis is how the data used for creating these models looks like:\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\ndf = pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/2021_Fuel_and_Energy_mpg_fossil_fuels.csv')\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1056 entries, 0 to 1055\nData columns (total 8 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   City                    1053 non-null   object \n 1   Agency                  1056 non-null   object \n 2   Organization Type       1056 non-null   object \n 3   Primary UZA Population  1056 non-null   int64  \n 4   Mode                    1056 non-null   object \n 5   TOS                     1056 non-null   object \n 6   Electric Battery        1056 non-null   int64  \n 7   avg_mpg_fossil_fuels    1056 non-null   float64\ndtypes: float64(1), int64(2), object(5)\nmemory usage: 66.1+ KB\n\n\nIn order to view the shape of the data we want to predict, it will be useful to visualize its shape. This will be more of an EDA step, however, it is better to have it next to our model to understand it better. A violin plot allow us to visualize the distribution of the data and its probability density, which could be useful to understand better the data.\n\n\nCode\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(8, 6))\nsns.violinplot(x=df['avg_mpg_fossil_fuels'], color='skyblue')\n\nplt.xlabel('Average MPG of Fossil Fuels')\nplt.title('Violin Plot of Average MPG of Fossil Fuels')\n\nplt.show()\n\n\n/Users/jorgebrismoreno/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\nAs we can see from this graph, it seems that there is a high density of data close to 0. This is beneficial to visualize and better understand the nature of the data. Moreover, we can see that there is a wide range of possible values, which could mean that either we have a lot of heavy outliers or very few data samples on those values."
  },
  {
    "objectID": "trees.html#base-model-random-classifier",
    "href": "trees.html#base-model-random-classifier",
    "title": "Regression trees and Random Forests",
    "section": "Base model / random classifier",
    "text": "Base model / random classifier\nA random classiffier simply makes random predictions of the data. This is the most basic model we can use to predict our mpg data. We will use this model as a baseline to compare the other models.\n\n\nCode\ndef random_regressor_avg_mpg_fossil_fuels(y_data):\n    y_pred = np.random.uniform(y_data.min(), y_data.max(), size=len(y_data))\n    \n    print(\"---------RANDOM REGRESSOR---------\")\n\n    mse = mean_squared_error(y_data, y_pred)\n    print(\"Mean Squared Error:\", mse)\n    \n    rmse = np.sqrt(mse)\n    print(\"Root Mean Squared Error:\", rmse)\n\ny_avg_mpg_fossil_fuels = df['avg_mpg_fossil_fuels']\n\nrandom_regressor_avg_mpg_fossil_fuels(y_avg_mpg_fossil_fuels)\n\n\n---------RANDOM REGRESSOR---------\nMean Squared Error: 1066824.3696089713\nRoot Mean Squared Error: 1032.8719037755704\n\n\nAs we can see, it has a very low accruacy, which is expected since it is a random classifier. We will keep this values in mind to compare them with the other models."
  },
  {
    "objectID": "trees.html#decision-tree-1",
    "href": "trees.html#decision-tree-1",
    "title": "Regression trees and Random Forests",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nBase Decision Tree\nThis is the base regression tree model for our data set and its results:\n\n\nCode\ny = df['avg_mpg_fossil_fuels']\nX = df.drop(columns=['avg_mpg_fossil_fuels'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\n\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean'))\n])\n\ncategorical_features = X.select_dtypes(include=['object']).columns\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nX_train_transformed = preprocessor.fit_transform(X_train)\n\nfeature_names = list(numeric_features) + list(\n    preprocessor.named_transformers_['cat']\n    .named_steps['onehot'].get_feature_names_out(categorical_features)\n)\n\nregressor = DecisionTreeRegressor(random_state=44)\nregressor.fit(X_train_transformed, y_train)\n\nX_test_transformed = preprocessor.transform(X_test)\ny_pred = regressor.predict(X_test_transformed)\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\n\nprint(\"Mean Squared Error:\", mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\n\n\nMean Squared Error: 3464.875119590503\nRoot Mean Squared Error (RMSE): 58.863189852321995\n\n\n\n\nCode\nplt.figure(figsize=(20, 10))\nplot_tree(regressor, feature_names=feature_names, filled=True)\nplt.show()\n\n\n\n\n\nAs we can see, even though it does a much better job than the random classifier, it is still not very accurate. Furthermore, the depth of the tree is impressingly large, which could be a problem for the model. Thus, we are going to tune this model on the next step.\n\n\nHyperparameter tunning on decision tree\nIn this step, we are going to tune the hyperparameters of the regression tree model. We will vary the depth of the tree to find the optimal one to use.\n\n\nCode\ntest_results = []\ntrain_results = []\n\nfor max_depth in range(1, 34):\n    model = DecisionTreeRegressor(max_depth=max_depth, random_state=44)\n    model.fit(X_train_transformed, y_train)\n\n    y_pred_train = model.predict(X_train_transformed)\n    y_pred_test = model.predict(X_test_transformed)\n\n    train_mse = mean_squared_error(y_train, y_pred_train)\n    test_mse = mean_squared_error(y_test, y_pred_test)\n    \n    train_rmse = np.sqrt(train_mse)\n    test_rmse = np.sqrt(test_mse)\n\n    train_results.append([max_depth, train_mse, train_rmse])\n    test_results.append([max_depth, test_mse, test_rmse])\n\nmax_depth_values = [result[0] for result in test_results]\ntrain_mse_values = [result[1] for result in train_results]\ntest_mse_values = [result[1] for result in test_results]\ntrain_rmse_values = [result[2] for result in train_results]\ntest_rmse_values = [result[2] for result in test_results]\n\nplt.figure(figsize=(10, 6))\nplt.plot(max_depth_values, train_mse_values, label='Train MSE', marker='o', color='blue')\nplt.plot(max_depth_values, test_mse_values, label='Test MSE', marker='o', color='red')\nplt.xlabel('Max Depth of Decision Tree')\nplt.ylabel('Mean Squared Error')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10, 6))\nplt.plot(max_depth_values, train_rmse_values, label='Train RMSE', marker='o', color='blue')\nplt.plot(max_depth_values, test_rmse_values, label='Test RMSE', marker='o', color='red')\nplt.xlabel('Max Depth of Decision Tree')\nplt.ylabel('Root Mean Squared Error')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFrom these graphs, we can see that the best depth for the tree is 5, since it has the lowest error for both, the training and the test data. Thus, we will use this depth for the next model.\n\n\nOptimal tree\nBased on the previous results, we will use a tree with a depth of 5 to predict the mpg data. The tree has the following structure and results:\n\n\nCode\ny = df['avg_mpg_fossil_fuels']\nX = df.drop(columns=['avg_mpg_fossil_fuels'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)\n\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns\ncategorical_features = X.select_dtypes(include=['object']).columns\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean'))\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nmodel = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', DecisionTreeRegressor(max_depth=5, random_state=44))\n])\n\nmodel.fit(X_train, y_train)\n\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\ntrain_mse = mean_squared_error(y_train, y_pred_train)\ntest_mse = mean_squared_error(y_test, y_pred_test)\n\ntrain_rmse = np.sqrt(train_mse)\ntest_rmse = np.sqrt(test_mse)\n\nprint(f\"Train Mean Squared Error: {train_mse}\")\nprint(f\"Test Mean Squared Error: {test_mse}\")\nprint(f\"Train Root Mean Squared Error: {train_rmse}\")\nprint(f\"Test Root Mean Squared Error: {test_rmse}\")\n\n\nTrain Mean Squared Error: 2065.96904233018\nTest Mean Squared Error: 941.6117919314114\nTrain Root Mean Squared Error: 45.45293216427495\nTest Root Mean Squared Error: 30.685693603557528\n\n\n\n\nCode\nmodel.fit(X_train, y_train)\n\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\n\ncategorical_features_names = model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\nfeature_names = list(numeric_features) + list(categorical_features_names)\n\nplt.figure(figsize=(20, 10))\nplot_tree(model.named_steps['regressor'], feature_names=feature_names, filled=True, rounded=True)\nplt.show()\n\n\n\n\n\nWe can see that the MSE is much lower than the previous tree model (cut down by almost half), which means that this model is much more accurate. Furthermore, the depth of the tree is much lower than the previous one, which is good since it means that the model is simpler. Furthermore, we can see that Agency is the most important variable to predict the mpg data, followed by City, Mode, and TOS. This will allow us to make inferences about this variables in the future."
  },
  {
    "objectID": "trees.html#random-forest-1",
    "href": "trees.html#random-forest-1",
    "title": "Regression trees and Random Forests",
    "section": "Random forest",
    "text": "Random forest\n\nBase Random Forest\nThis is the base random forest model for our data set, its results, and the first three trees of the forest:\n\n\nCode\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns\ncategorical_features = X.select_dtypes(include=['object']).columns\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean'))\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nrf_regressor = RandomForestRegressor(n_estimators=100, random_state=44)\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', rf_regressor)\n])\n\npipeline.fit(X_train, y_train)\n\ny_pred_train_rf = pipeline.predict(X_train)\ny_pred_test_rf = pipeline.predict(X_test)\n\ntrain_mse_rf = mean_squared_error(y_train, y_pred_train_rf)\ntest_mse_rf = mean_squared_error(y_test, y_pred_test_rf)\ntrain_rmse_rf = np.sqrt(train_mse_rf)\ntest_rmse_rf = np.sqrt(test_mse_rf)\n\nprint(\"Random Forest Regressor Results:\")\nprint(\"Train Mean Squared Error:\", train_mse_rf)\nprint(\"Test Mean Squared Error:\", test_mse_rf)\nprint(\"Train Root Mean Squared Error:\", train_rmse_rf)\nprint(\"Test Root Mean Squared Error:\", test_rmse_rf)\n\n\nRandom Forest Regressor Results:\nTrain Mean Squared Error: 2544.263281758251\nTest Mean Squared Error: 4549.356195127014\nTrain Root Mean Squared Error: 50.4406907343491\nTest Root Mean Squared Error: 67.44891544811536\n\n\n\n\nCode\nrf_regressor = pipeline.named_steps['regressor']\n\ncategorical_features = X.select_dtypes(include=['object']).columns\nfeature_names = list(numeric_features) + list(\n    pipeline.named_steps['preprocessor'].named_transformers_['cat']\n    .named_steps['onehot'].get_feature_names_out(categorical_features)\n)\n\nfor i in range(3):\n    plt.figure(figsize=(20, 10))\n    plot_tree(rf_regressor.estimators_[i], feature_names=feature_names, filled=True, rounded=True)\n    plt.title(f\"Random Forest Tree {i+1}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, the accruacy is seems to be much worse for our test data compared to the optimal decision tree (as the MSE is much higher). However, the trees look very large and seems that this model can be improved to perform even better while being more interpretable. Thus, we are going to tune this model on the next step.\n\n\nHyperparameter tunning on random forest\nIn this step, we are going to tune the hyperparameters of the random forest model. We will vary the depth of the trees and the number of estimators to find the optimal one to use.\n\n\nCode\nparam_dist = {\n    'regressor__n_estimators': randint(44, 444),\n    'regressor__max_depth': randint(1, 24),\n}\n\nrand_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=20, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\nrand_search.fit(X_train, y_train)\n\nbest_rf_model = rand_search.best_estimator_\n\ny_pred_train_best = best_rf_model.predict(X_train)\ny_pred_test_best = best_rf_model.predict(X_test)\n\nmse_train_best = mean_squared_error(y_train, y_pred_train_best)\nmse_test_best = mean_squared_error(y_test, y_pred_test_best)\nrmse_train_best = np.sqrt(mse_train_best)\nrmse_test_best = np.sqrt(mse_test_best)\n\nprint(\"Best Model Metrics:\")\nprint(f\"Train Mean Squared Error: {mse_train_best}\")\nprint(f\"Test Mean Squared Error: {mse_test_best}\")\nprint(f\"Train Root Mean Squared Error: {rmse_train_best}\")\nprint(f\"Test Root Mean Squared Error: {rmse_test_best}\")\n\nprint(\"Best Hyperparameters:\")\nprint(rand_search.best_params_)\n\n\nBest Model Metrics:\nTrain Mean Squared Error: 6362.505730760978\nTest Mean Squared Error: 1823.3688800609457\nTrain Root Mean Squared Error: 79.76531659036387\nTest Root Mean Squared Error: 42.700923644119754\nBest Hyperparameters:\n{'regressor__max_depth': 2, 'regressor__n_estimators': 326}\n\n\nFrom this results, we can inferr that the best depth of the trees are 2 and the regressor estimators is 326. Thus, we will use these parameters for the next model.\n\n\nOptimal random forest\nBased on the previous results, we will use a random forests with trees of a depth of 3 and 140 estimators to predict the mpg data. The forest has the following results and the first three trees of the forest look like the following:\n\n\nCode\nbest_hyperparameters = {'max_depth': 2, 'n_estimators': 326}\n\npipeline.named_steps['regressor'].set_params(**best_hyperparameters)\n\npipeline.fit(X_train, y_train)\n\ny_pred_train_optimal = pipeline.predict(X_train)\ny_pred_test_optimal = pipeline.predict(X_test)\n\nmse_train_optimal = mean_squared_error(y_train, y_pred_train_optimal)\nmse_test_optimal = mean_squared_error(y_test, y_pred_test_optimal)\nrmse_train_optimal = np.sqrt(mse_train_optimal)\nrmse_test_optimal = np.sqrt(mse_test_optimal)\n\nprint(\"Optimal Model Metrics:\")\nprint(f\"Train Mean Squared Error: {mse_train_optimal}\")\nprint(f\"Test Mean Squared Error: {mse_test_optimal}\")\nprint(f\"Train Root Mean Squared Error: {rmse_train_optimal}\")\nprint(f\"Test Root Mean Squared Error: {rmse_test_optimal}\")\n\n\nOptimal Model Metrics:\nTrain Mean Squared Error: 6362.505730760978\nTest Mean Squared Error: 1823.3688800609457\nTrain Root Mean Squared Error: 79.76531659036387\nTest Root Mean Squared Error: 42.700923644119754\n\n\n\n\nCode\nrf_regressor = pipeline.named_steps['regressor']\n\ncategorical_features = X.select_dtypes(include=['object']).columns\nfeature_names = list(numeric_features) + list(\n    pipeline.named_steps['preprocessor'].named_transformers_['cat']\n    .named_steps['onehot'].get_feature_names_out(categorical_features)\n)\n\nfor i in range(min(3, len(rf_regressor.estimators_))):\n    plt.figure(figsize=(20, 10))\n    plot_tree(rf_regressor.estimators_[i], feature_names=feature_names, filled=True, rounded=True)\n    plt.title(f\"Random Forest Tree {i+1}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, the most important predictors still seem to be Agency, City and Mode. However, the MSE is greater than our optimal decision tree, which means that this model is less accurate."
  },
  {
    "objectID": "trees.html#conclusion",
    "href": "trees.html#conclusion",
    "title": "Regression trees and Random Forests",
    "section": "Conclusion",
    "text": "Conclusion\nFrom the previous results, we can conclude that the best model to predict the mpg data is our optimal decision tree with a depth of 5. This model has a MSE of for 941.61 for the test data, which seems to be a good result due to the distribution of our data as seen before. Furthermore, the most important predictors are Agency, City, TOS, and Mode, which means that these variables are the most important to predict the mpg data. This could give us insights on what to do to improve the average miles per galon, which factors are better, and how much fossil fuels we will use in the future based on different characteristics.\nNow that it seems that we have a good model to predict the mpg data, we can use it to predict the mpg data for the next year. Moreover, we can start to investigate how different agencies, cities, TOS, and Mode might affect the efficiency of the vehicles used and make recommendations for changes in the future. This will help ius visualize how much our carbon footprint will be reduced in the future with those changes and we can predict our future mpg based on those changes."
  },
  {
    "objectID": "Dimension_red.html",
    "href": "Dimension_red.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Code\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom __future__ import print_function\nimport time\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport warnings"
  },
  {
    "objectID": "Dimension_red.html#explanation-of-dimensionality-reduction-methods",
    "href": "Dimension_red.html#explanation-of-dimensionality-reduction-methods",
    "title": "Dimensionality Reduction",
    "section": "Explanation of Dimensionality Reduction methods",
    "text": "Explanation of Dimensionality Reduction methods\nThe goal of this tab is to look closer at every variable in the EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv and see which variables explain the most variance in order to understand that data set better due to its large shape (71 columns). The columns that explain the most variance will be utilized for some methods of clustering due to computational limitations of my laptop and to also visualize the clusterings calculated. This will allow us not only reduce the size of our data set while keeping the most variance but also understand the dimensionality that explains our data and visualize it in a different perspective.\n\nPrincipal Component Analysis (PCA)\nPrincipal Component Analysis is a dimensionality reduction method that is commonly used to reduce the dimensionality of very large data sets like the one dealt in this tab. The goal is to keep as much variance as possible in the data while reducing the number of features as much as possible. For this, PCA finds the principal components of the data.\nFirstly, PCA centers the data by substracting the mean of each feature. Then, PCA uses the covariance matrix to look at the relationship between variables. After this step, the eigenvector are calculated and the ones with the highes eigenvalues are determined to be the princcipal ones. By selecting the variance that we want to keep (in our case will be at least 90%) and subset the first components until that variance is explained, we are able to reduce the dimensionality of our dataset. Then, the selected components are used to produce a projection matrix by combining the eigenvectors and use it to actually transform the original data into the lower-dimensionality space (Shlens 2014).\n\n\nt-distributed Stochastic Neighbor Embedding (t-SNE)\nt-distributed Stochastic Neighbor Embedding or t-SNE is a machine learning algorithm that focusses on dimensionality reduction and visualization of high-dimensionality data. Due to its great visualization capabilities, we will specifically use this method to visualize the clusters calculated and look to see whether there is overlap or not among them (on the first two most explanatory components).\nT-SNE is particularly good at finding relationships between data points, that is why it is a great tool to visualize the clusters calculated. T-SNE models pairwise comparisons etween data points in a high-dimensional space. Then, it uses the Student’s t-distribution to define the probability distribution over pairs of points in a low dimensional space and similarly define the distribution of the high-dimensional data points. After that, it focuses on reducing the divergence of the two distributions. Finally, it adjust the position of the data points to match the pairwise similarities while preserving the local relationships (Wang et al. 2021).\n\n\nPCA vs t-SNE\nPCA focuses on finding the principal components of the dataset while projecting it into a lower dimensional space. This process focusses on maintaining the most variance and rataining the pairwise distances between the data points. On the other hand, t-SNE focuses on maintaining small pairwise distances between the data points and it is not concerned with the variance of the data. Thus, for these reasons, we will use PCA to compute some of our more computational expensive clustering methods and t-SNE to visualize the clusters calculated."
  },
  {
    "objectID": "Dimension_red.html#pca",
    "href": "Dimension_red.html#pca",
    "title": "Dimensionality Reduction",
    "section": "PCA",
    "text": "PCA\nWe will use PCA to determine which variables are the ones that explain the most variance of our data set. We will use the new data set obtained for some clustering methods and a better understanding of the EPA Smart Location Database dataset. For this, we will plot the explained variance based on the number of components/variables kept and choose our ideal number of components based on the variance explained.\nBy rule of thumb, we want to keep at least 80% of the variance. However, to be more percise, we are going to choose the number of components such that the explained variance is greater or equal to 90%.\n\n\nCode\ndf=pd.read_csv('../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv',)\n\nscaler = StandardScaler()\ndf_scaled = pd.DataFrame(scaler.fit_transform(df))\n\npca=PCA()\npca.fit(df_scaled)\n\npca.explained_variance_ratio_\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nFrom this graph, we can seethat between 20 and 30 components, there is a thresshold of 90% of the variance explained. Thus, we will calculate the number of components that explain 90% of the variance in the following code chunk.\n\n\nCode\nnp.where(np.cumsum(pca.explained_variance_ratio_) &gt; 0.9)[0][0]\n\n\n25\n\n\n25 components explain 90% of the variance. Thus, we will use the first 25 components to reduce the dimensionality of our data set. We will generate a new data set with the first 25 components from PCA and use it for some clustering methods and visualization. The following code will generate this new data set.\n\n\nCode\npca=PCA(n_components=25)\npca.fit(df_scaled)\n\npca.transform(df_scaled)\nscores_pca = pca.transform(df_scaled)\n\nscores_pca_df = pd.DataFrame(scores_pca, columns=[f'PC{i}' for i in range(1, scores_pca.shape[1] + 1)])\nscores_pca_df.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/scores_pca.csv', index=False)\n\n\nWe saved the new data set with the 25 components in a new csv file called EPA_SmartLocationDatabase_V3_Jan_2021_Clean_PCA.csv (located in the clean data folder). This new data set will be used for some of the clustering methods due to our computational limitations. Furthermore, it is worth noting that the columns are ordered in descending order of explained variance (the first one being the variable that explains the most of the variance and the last one the least)."
  },
  {
    "objectID": "Dimension_red.html#t-sne",
    "href": "Dimension_red.html#t-sne",
    "title": "Dimensionality Reduction",
    "section": "t-SNE",
    "text": "t-SNE\nWe will use t-SNE to visualize the data set and the clusters calculated with k-means in 2 dimensions. This will help us understand the data set better and see if there are any clusters that can be found. We will plot our data set in 2 dimensions and color the points based on the clusters calculated by k-means. However, we will do this for different perplexity values to see if there is any difference in the clusters.\n\n\nCode\nX = df.iloc[:, :]\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nperplexity_values = [5, 20, 50]\n\nkmeans = KMeans(n_clusters=12, random_state=42)\n\nfig, axes = plt.subplots(1, len(perplexity_values), figsize=(15, 5))\n\nfor i, perplexity in enumerate(perplexity_values):\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n        X_tsne = tsne.fit_transform(X)\n\n    clusters = kmeans.fit_predict(X_tsne)\n\n    tsne_df = pd.DataFrame(data=X_tsne, columns=['Dimension 1', 'Dimension 2'])\n    tsne_df['Cluster'] = clusters\n\n    sns.scatterplot(x='Dimension 1', y='Dimension 2', hue='Cluster', data=tsne_df, palette='viridis', legend='full', ax=axes[i])\n    axes[i].set_title(f't-SNE Visualization (Perplexity = {perplexity})')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThis is a good visualization of the clusters found by k-means. More advanced methods of visualization could be done with higher computational power. However, it does not change too much from different perplexity values, which is a good sign. While the data looks like one big blob, this is probably due to the choice of the two dimensions, as we can see more clear differentiation of the clusters in the pairplots on the clustering tab. However, the clusters do not seem to overlap with eachother, which is a very good sign of the existence of clusters in the data set."
  },
  {
    "objectID": "Dimension_red.html#conclusion",
    "href": "Dimension_red.html#conclusion",
    "title": "Dimensionality Reduction",
    "section": "Conclusion",
    "text": "Conclusion\nWe can see that there are some variables that explain the most variance in the data set. Keeping 25 out of 71 variables will preserve more than 90% of the variance, which means there is a large number of redundant variables in the data set. Furthermore, visualizing the clusters using t_SNE in 2 dimensions was very helpful to see if there was any overlap on the clusters, which there is not an aparent one. Even though it could be seen as one giant cluster, this is probably due to the dimensions chosen. However, the fact that they do not overlap supports our claim that there are clusters on the data set (as the clustering studies also support this claim)."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Data Exploration",
    "section": "",
    "text": "The goals of this tab is to gain a deeper understanding of the data, how variables behave, and identify anomalies, paterns, or potential insights for future exploration."
  },
  {
    "objectID": "EDA.html#eda-goals",
    "href": "EDA.html#eda-goals",
    "title": "Data Exploration",
    "section": "",
    "text": "The goals of this tab is to gain a deeper understanding of the data, how variables behave, and identify anomalies, paterns, or potential insights for future exploration."
  },
  {
    "objectID": "EDA.html#eda",
    "href": "EDA.html#eda",
    "title": "Data Exploration",
    "section": "EDA",
    "text": "EDA\nExplratory Data Analysis (EDA) is a critical step in data science. The goal is to visualize the data and get different insights that can be used to guide the rest of the analysis. It will not only help you understand the data better, but also identify areas of study. Furthermore, visualizing the data allows you to understand its shape, identify outliers, and see potential relationships between variables.\nBy analyzing the data, I hope to answer some questions while also identifying new ones."
  },
  {
    "objectID": "EDA.html#census-blocks-data",
    "href": "EDA.html#census-blocks-data",
    "title": "Data Exploration",
    "section": "Census Blocks Data",
    "text": "Census Blocks Data\nThis data set contains the information about all the census blocks in the US. The data set contains 117 columns and 220740 rows. Each row is a census block. The information about what each variable means can be found inside EPA pdf inside the data folder. For the purpose of EDA, we will focus on the following variables: ‘Ac_Water’, ‘Ac_Land’, ‘TotPop’, ‘AutoOwn0’, ‘E_MedWageWk’. These variables are the ones that I believe will be the most useful for the analysis.\nThe first main two focusses are Total Population and Working Class. These plots display relevant information about the distribution of these factors among all census blocks:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndf &lt;- read.csv(\"../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv\")\n\ncolumns_to_plot &lt;- c('CBSA_POP', 'Ac_Water', 'Ac_Land', 'TotPop', 'P_WrkAge', 'E_HiWageWk', 'AutoOwn0', 'NatWalkInd', 'E_MedWageWk', 'Ac_Unpr')\ndf &lt;- df[,columns_to_plot]\n\nggplot(df, aes(x = TotPop, fill = factor(1))) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Total Population\",\n       x = \"TotPop\",\n       y = \"Density\",\n       fill = \"TotPop\") + \n  scale_fill_manual(values = c(\"lightblue1\")) +\n  theme_minimal()\n\nggplot(df, aes(x = E_MedWageWk, fill = factor(1))) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"# of workers earning more than $1250/month but less than\n$3333/month (work location)\",\n       x = \"E_MedWageWk\",\n       y = \"Density\",\n       fill = \"E_MedWageWk\") + \n  scale_fill_manual(values = c(\"turquoise\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs we can see, while some census blocks have really high values on these two parameters, most of them have low values. This is important to keep in mind when analyzing the data, as it will be important to understand how the data is distributed. The plots are impressingly skewed, which make sense as most of the US population is concentrated in small towns and cities.\nThe third point we will focus on is the number of people who own 0 cars. While the US is a car-centric country, since we are focussing on public transportation, this information could give us great insights, as these people will only be able to use public transportation or walk to get to their destination. The following plot shows the distribution of the number of people who own 0 cars:\n\n\nCode\nggplot(df, aes(x = \"\", y = AutoOwn0, color = factor(1))) +\n  geom_jitter(alpha = 0.7, width = 0.3) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, coef = 0) +\n  labs(title = \"Total number of people who own 0 cars\",\n       x = \"AutoOwn0\",\n       y = \"Count\",\n       fill = \"AutoOwn0\") +\n  scale_color_manual(values = c(\"turquoise\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"AutoOwn\"))\n\n\n\n\n\nThe boxplot is squezzed next to zero and each point represents a value for a census block. As expected, most of the US population owns at least one car. However, there is a significant amount of people who own 0 cars, which is great information for our analysis.\nThis following plot will aim to see how much water is on the census blocks. Since water determines not only what methods of transportation can be built but also exacerbates the costs of construction, it is a factor to keep in consideration. The following plot shows the distribution of the amount of water in the census blocks:\n\n\nCode\nggplot(df, aes(x = \"\", y = Ac_Water, color = factor(1))) +\n  geom_jitter(alpha = 0.7, width = 0.3) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, coef = 0) +\n  labs(title = \"Total water area\",\n       x = \"Ac_Water\",\n       y = \"Count\",\n       fill = \"Ac_Water\") +\n  scale_color_manual(values = c(\"blue\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"Ac_Water\"))\n\n\n\n\n\nThis plot shows how most census blocks have a similar number of acres of water, however, the range is big enough to consider it in our analysis and cause problems with public transportation. Water is a determinant factor for public transportation, as it is not possible to build roads, railways on water, or makes constructing tunnels much harder. This is why it is important to understand how much water is in each census block. Furthermore, the fact that we have a couple of very big outliers will be very useful when drawing general conclusions about different groupings.\nThis last plot for this data set will focus on the amount of land in each census block. This is important as it will determine how much space is available for public transportation. The following plot shows the distribution of the amount of land in the census blocks:\n\n\nCode\nggplot(df, aes(x = \"\", y = Ac_Land, color = factor(1))) +\n  geom_jitter(alpha = 0.7, width = 0.3) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, coef = 0) +\n  labs(title = \"Total land area\",\n       x = \"Ac_Water\",\n       y = \"Count\",\n       fill = \"Ac_Land\") +\n  scale_color_manual(values = c(\"burlywood1\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"Ac_Land\"))\n\n\n\n\n\nThis plot shows how most census blocks have a similar number of acres of land, however, the range is big enough to consider it in our analysis and couse problems with public transportation. Land is a determinant factor for public transportation, as it determines the spread and the number of stops needed to cover these areas. We can see that there are a couple of very big outliers, which is important to take note of."
  },
  {
    "objectID": "EDA.html#fuel-and-energy-data",
    "href": "EDA.html#fuel-and-energy-data",
    "title": "Data Exploration",
    "section": "2021 Fuel and Energy Data",
    "text": "2021 Fuel and Energy Data\nThis data set contains information about public transportation, agencies, cities, and energy consumption. It was downloaded here. We will aim to look for patterns in the data, outliers, counts, etc.\nThe following two plots focus on the average miles per gallon based on the population and the density of the Population of the urbanized area served by the transit agency. This will give us insights on the data we have about population and if there is a correlation between the population and the miles per gallon.\n\n\nCode\nfuel &lt;- read.csv(\"../../data/Clean_Data_project_Pub.Transport_5000/2021_Fuel_and_Energy_mpg_fossil_fuels.csv\")\n\nggplot(fuel, aes(x = Primary.UZA.Population, fill = \"\")) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Population of the urbanized area served by the transit agency\",\n       x = \"Total Population\",\n       y = \"Density\",\n       fill = \"E_MedWageWk\") +\n  scale_fill_manual(values = c(\"lightblue1\")) +\n  theme_minimal()\n\nggplot(fuel, aes(x = Primary.UZA.Population, y = avg_mpg_fossil_fuels)) +\n  geom_point() +\n  labs(title = \"Population vs. Average MPG\",\n       x = \"Primary UZA Population\",\n       y = \"Average MPG (Fossil Fuels)\")\n\n\n\n\n\n\n\n\nFrom these plots we can see that the population density is skewed to the left in our data. However, we can see small peaks at certain points. This helps us understand the scatter plot better. We can see that the data points follow the structure of the density, but it seems that population seems to not be very correlated with the miles per gallon. However, there are a couple of very heavy outliers to keep in mind. This is important to keep in mind when analyzing the data, as it will be important to understand how the data is distributed.\nThe following plot will allow us to see the count of each organization type. Not only is a great way to see the most predominant ones, but understand how much data of each type we have.\n\n\nCode\nfuel %&gt;%\n  group_by(Organization.Type) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = factor(Organization.Type), y = count, fill = factor(Organization.Type))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Organization Type Counts\",\n       x = \"\",  \n       y = \"Count\") +\n  theme(axis.text.x = element_blank()) \n\n\n\n\n\nFrom this plot, the major two organization types are City, County, or Local Government and the Independent Public Agency or Authority of Transit Service. They account for much more than 50% of the data. This is insightful to understand what agencies are there but also which ones are being utilized currently, since they may or may not be the optimal ones.\nThe following plot will allow us to see the relationship of each mode of transportation and the average MPG. It is a great way to make inferences about their relationship and understand which modes of transportation are more efficient.\n\n\nCode\nggplot(fuel, aes(x = Mode, y = avg_mpg_fossil_fuels, fill = Mode)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", position = \"dodge\") +\n  labs(title = \"Mode vs. Average MPG\",\n       x = \"Mode\",\n       y = \"Average MPG (Fossil Fuels)\") +\n  theme_minimal()\n\n\n\n\n\nFrom this plot it seems that the method of transportation clearly matters when it comes to the average MPG. There are some modes of transportation that are clearly more efficient than others. However, this might be due to the lack of data and should be crosschecked.\nThis final plot will allow us to see the relationship of each type of service and the average MPG. It is a great way to make inferences about their relationship and understand which types of services of transportation are more efficient.\n\n\nCode\nggplot(fuel, aes(x = TOS, y = avg_mpg_fossil_fuels, fill = TOS)) +\n  geom_violin() +\n  labs(title = \"Violin Plot: TOS vs. Average MPG\",\n       x = \"TOS\",\n       y = \"Average MPG (Fossil Fuels)\")\n\n\n\n\n\nThis violin plots don’t show much information, as the data is very skewed. However, we can see that the data is very concentrated around the 0. This is important to keep in mind when analyzing the data, as it will be important to understand how the data is distributed."
  },
  {
    "objectID": "EDA.html#border-crossing-entry-data",
    "href": "EDA.html#border-crossing-entry-data",
    "title": "Data Exploration",
    "section": "Border Crossing Entry Data",
    "text": "Border Crossing Entry Data\nThis data set contains information of the vehicles and their type that crossed the border between other countries and the US. It was downloaded here. We will aim to understand the distribution of the data, the outliers, and the relationship between the different variables since this data set will be used to build a naive bayes model. While this data set is not very insightful for the analysis, it can be useful to understand if different mehtods of transportation are valued different or run though different borders.\nThe following plot displays the distribution of the values of different transportation methods. This will allow us to understand the distribution of the data and if there are any outliers. Furthermore, we will be able to see if they are sparse (which would mean a good sign of different methods of transportation being valued differently) or very squed.\n\n\nCode\ncrossing &lt;- read.csv('../../data/Clean_Data_project_Pub.Transport_5000/Clean_Border_Crossing_Entry_Data_20231103.csv')\n\nggplot(crossing, aes(x = \"\", y = Value, color = factor(1))) +\n  geom_jitter(alpha = 0.7, width = 0.3) +\n  geom_boxplot(width = 0.2, outlier.shape = NA, coef = 0) +\n  labs(title = \"Total land area\",\n       x = \"vehicles\",\n       y = \"Value\",\n       fill = \"Value\") +\n  scale_color_manual(values = c(\"darkolivegreen\")) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"Value of vehicles\"))\n\n\n\n\n\nFrom this plot, we can see that the data seems sparse, which means that different methods of transportation are valued differently. This is a good sign as we might be able to use this to predict the method of transportation (even though we cannot determine this until we test it).\nThe following plot will display the count of each method of transportation. This allow us to see if we have similar number of data points for each method of transportation or if some are more popular than others.\n\n\nCode\ncrossing %&gt;%\n  group_by(Measure) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = Measure, y = count, fill = Measure)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Counts by Method of transportation\", x = \"Measure\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\nAll methods seem to have the same popularity, which is great for our analysis. This means that we will have enough data points for each method of transportation to make predictions.\nThis final plot will show us the count of crossings in each state. This will help us identify the most popular states for crossing into or out from the US and understand better the nature of our data set.\n\n\nCode\ncrossing %&gt;%\n  group_by(State) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = State, y = count, fill = State)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Counts by State\",\n       x = \"State\",\n       y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\nThis plot shows us that the most popular states is North Dakota followed by Washington. This is important to keep in mind as these states might have some specific types of transportation that are more popular than others."
  },
  {
    "objectID": "EDA.html#other-datasets-that-will-help-us-with-insights",
    "href": "EDA.html#other-datasets-that-will-help-us-with-insights",
    "title": "Data Exploration",
    "section": "Other datasets that will help us with insights",
    "text": "Other datasets that will help us with insights\nThese data sets will help us with insights on the data we are working with. While they have not been further analyzed with other time due to time constraints, they reveal relevant insights about the public transportation industry.\n\nVehicle Production by countries\nHere, we will be exploring the vehicle_production_countries data set. It will allow us understand which countries are the major producers and how this production changed over time. This will allow us to understand the industry better and see if there are any trends that we can use to make predictions.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(boot)\nlibrary(AICcmodavg)\nlibrary(readxl)\nlibrary(broom)\n\nvehicle_production_countries_comercial &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/vehicle_production_countries.xlsx\", sheet=\"Comercial-vehicles\", skip = 1)\n\nvehicle_production_countries_comercial[-1] &lt;- lapply(vehicle_production_countries_comercial[-1], function(x) ifelse(x %in% c(\"N\", \"U\", \"Z\"), NA, x))\n\nvehicle_production_countries_comercial &lt;- vehicle_production_countries_comercial %&gt;%\n  mutate(across(-1, as.numeric))\n\ncolnames(vehicle_production_countries_comercial)[colnames(vehicle_production_countries_comercial) == \"...1\"] &lt;- \"Country\"\ncolnames(vehicle_production_countries_comercial)\n\nvehicle_production_countries_comercial_transposed &lt;- as.data.frame(t(vehicle_production_countries_comercial))\n\ncolnames(vehicle_production_countries_comercial_transposed) &lt;- vehicle_production_countries_comercial_transposed[1, ]\n\nvehicle_production_countries_comercial_transposed &lt;- vehicle_production_countries_comercial_transposed[-1, ]\n\nvehicle_production_countries_comercial_transposed[] &lt;- lapply(vehicle_production_countries_comercial_transposed, as.numeric)\n\nvehicle_production_countries_comercial_transposed &lt;- vehicle_production_countries_comercial_transposed[, !colnames(vehicle_production_countries_comercial_transposed) %in% \"Total world\"]\n\ndf &lt;- vehicle_production_countries_comercial_transposed %&gt;% \n  rownames_to_column(var = \"Year\")\n\ndf &lt;- df %&gt;%\n  mutate(Year = ifelse(Year == \"(R) 2019\", 2019, ifelse(Year == \"(R) 2020\", 2020, Year)))\n\n\n\n\nCode\ndf_long &lt;- df %&gt;%\n  pivot_longer(-Year, names_to = \"Country\", values_to = \"Value\")\n\nggplot(df_long, aes(x = Year, y = Value, color = Country)) +\n  geom_point() +\n  labs(\n    title = \"Value per year and per country in comercial vehicle production\",\n    x = \"Year\",\n    y = \"Value of comercial vehicles produced\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\", axis.text.x = element_text(angle = 90, hjust = 1))\n\n\nWarning message:\n\"Removed 188 rows containing missing values (geom_point).\"\n\n\n\n\n\nFrom this plot we can see how Japan (back in the 1960s) used to be one of the biggest producers but not anymore. The US has been in the lead since the 1990s until around 2008, which was expected due to the recession. However, it is impressive how China was able to excel during the recession in this market at an almost exponential rate. And it still has kept increasing since then.\nIn order to run an ANOVA test and look if countries, years, and interaction among the both have an statistical significance on comercial vehicle production, we are going to assume that all the missing values are not relevant to the contribution of the overall market and, therefore, close to 0. Thuis, for the sake of this process, we will make them 0.\n\n\nCode\nvehicle_production_countries_comercial &lt;- replace(vehicle_production_countries_comercial, is.na(vehicle_production_countries_comercial), 0)\n\n\n\n\nCode\ndata_long &lt;- vehicle_production_countries_comercial %&gt;%\n  pivot_longer(cols = -Country, names_to = \"Year\", values_to = \"Value\")\n\ndata_long &lt;- data_long[data_long$Country != \"Total world\", ]\n\nmodel &lt;- aov(Value ~ Year * Country, data = data_long)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nWarning message in anova.lm(object):\n\"ANOVA F-tests on an essentially perfect fit are unreliable\"\n\n\nAnalysis of Variance Table\n\nResponse: Value\n              Df     Sum Sq  Mean Sq F value Pr(&gt;F)\nYear          31   85531235  2759072     NaN    NaN\nCountry       32 2172668273 67895884     NaN    NaN\nYear:Country 992 1139993857  1149187     NaN    NaN\nResiduals      0          0      NaN               \n\n\nSeing this, we will simplify our model, taking away the interaction (in order to no overfit the data with our ANOVA model).\n\n\nCode\nmodel &lt;- aov(Value ~ Year + Country, data = data_long)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Value\n           Df     Sum Sq  Mean Sq F value    Pr(&gt;F)    \nYear       31   85531235  2759072  2.4009 3.244e-05 ***\nCountry    32 2172668273 67895884 59.0816 &lt; 2.2e-16 ***\nResiduals 992 1139993857  1149187                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth have a significant effect (which was expected). However, this helped us also understand that their interaction would overfit the model in this case.\n\n\nEnergy and usage of Buses\nIn this part, we will be exploring the energy_consumed_byMill_passenger_MILES file.\n\n\nCode\nenergy_consum &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/energy_consumed_byMill_passenger_MILES.xlsx\", sheet=\"Energy\")\n\ncolnames(energy_consum) &lt;- energy_consum[1, ]\n\nenergy_consum &lt;- energy_consum[-1, ]\n\ncolnames(energy_consum)[1] &lt;- \"Data_to_Explore\"\n\nenergy_consum &lt;- energy_consum[energy_consum$Data_to_Explore %in% c(\n  \"Vehicle-miles (millions)\",\n  \"Passenger-miles (millions)\",\n  \"Energy consumed, total (billion Btu)\",\n  \"Energy intensity (Btu/passenger-mile)\"\n), ]\n\nenergy_consum[, -1] &lt;- apply(energy_consum[, -1], 2, function(x) ifelse(x == \"N\", NA, x))\n\nenergy_consum &lt;- energy_consum %&gt;% select(-\"1960\", -\"1965\", -\"1970\", -\"1975\", -\"1980\", -\"1985\", -\"1990\", -\"1991\", -\"1992\", -\"1993\", -\"1994\", -\"1995\")\n\nenergy_consum &lt;- energy_consum %&gt;%\n  mutate_at(vars(-1), as.numeric)\n\nenergy_consum_long &lt;-as.data.frame(t(energy_consum), index=False)\n\ncolnames(energy_consum_long) &lt;- energy_consum_long[1, ]\n\nenergy_consum_long &lt;- energy_consum_long[-1, ]\n\ncolnames(energy_consum_long) &lt;- c(\"Vehicle_milesMill\", \"Passenger_miles_mill\",\"Energy_consumed_total_bill_Btu\", \"Energy_intensity_Btu_passenger_mile\")\n\nYears = c(1996:2021)\nenergy_consum_long['Years'] &lt;- Years\n\nenergy_consum_long_no_Covid &lt;- energy_consum_long[!(energy_consum_long$Years %in% c(2020, 2021)), ]\n\n\nFor the fitted model in the following plots, year 2020 and 2021 are going to be ignored and considered outliers due to COVID-19. They are heavy outliers.\n\n\nCode\nlm_model &lt;- lm(energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile ~ energy_consum_long_no_Covid$Year)\n\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile,\n     xlab = \"Year\",\n     ylab = \"Energy consumed, total (billion Btu)\",\n     col = \"blue3\",\n     pch = 19)\n\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\nabline(lm_model, col = \"red\")\n\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Energy_intensity_Btu_passenger_mile, col = \"blue3\", pch = 19)\n\n\n\n\n\nFrom this plot we can see how the total energy consumed (in billion Btu) has been constantly decreasing. However, is it because our vehicles are more efficient or due to cuts in public transportation?\n\n\nCode\nlm_model &lt;- lm(energy_consum_long_no_Covid$Passenger_miles_mill ~ energy_consum_long_no_Covid$Year)\n\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Passenger_miles_mill,\n     xlab = \"Year\",\n     ylab = \"Passenger-miles (millions)\",\n     col = \"blue3\",\n     pch = 19)\n\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\nabline(lm_model, col = \"red\")\n\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Passenger_miles_mill, col = \"blue3\", pch = 19)\n\n\n\n\n\nWhile the fitted model shows an increase in the usage of public transportation, we can see that it has been constantly dropping since around 2014. This can bring up many questions such as: Is it due to an investment problem? Is it because there are not enough incentives to use public transportation? Do people own more cars?\n\n\nCode\nlm_model &lt;- lm(energy_consum_long_no_Covid$Vehicle_milesMill ~ energy_consum_long_no_Covid$Year)\n\nplot(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Vehicle_milesMill,\n     xlab = \"Year\",\n     ylab = \"Vehicle miles (millions)\",\n     col = \"blue3\",\n     pch = 19)\n\nconf_int &lt;- predict(lm_model, interval = \"confidence\")\n\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"lwr\"], col = \"red\", lty = 2)\nlines(energy_consum_long_no_Covid$Year, conf_int[, \"upr\"], col = \"red\", lty = 2)\n\npolygon(c(energy_consum_long_no_Covid$Year, rev(energy_consum_long_no_Covid$Year)),\n        c(conf_int[, \"lwr\"], rev(conf_int[, \"upr\"])),\n        col = \"lightgray\", border = NA)\n\nabline(lm_model, col = \"red\")\n\npoints(energy_consum_long_no_Covid$Year, energy_consum_long_no_Covid$Vehicle_milesMill, col = \"blue3\", pch = 19)\n\n\n\n\n\nThe vehicle miles have been constantly increasing, which means that there have been more and more routes added over time. However, why has this not been enough to increase the demand of public transportation?\n\n\nDC Metro Scorecard\nThis part will focus on the DC_Metro_Scorecard data, which counts the reliability and efficiency of DC Metro from 2014 to 2016.\n\n\nCode\nDC_metro &lt;- read_excel(\"../../data/Raw_Data_project_Pub.Transport_5000/DC_Metro_Scorecard.xlsx\", sheet=\"Sheet1\")\n\nDC_metro &lt;- DC_metro%&gt;% select (-\"Crimes Target\", -\"Employee Injury Rate Target\", -\"Customer Injury Rate Target\", -\"Elevator Reliability\", -\"Elevator Reliability Target\", -\"Escalator Reliability Target\",\n-\"Rail Fleet Reliability Target\", -\"Bus On-Time Performance Target\", -\"Bus Fleet Reliability Target\", -\"Escalator Reliability\", -\"Rail On-Time Performance Target\")\n\ncolnames(DC_metro) &lt;- c('Year','Month','Bus_on_time','Bus_fleet_reliability','Rail_fleet_reliability', 'Rail_on_time', 'Customer_injury_rate_per_1_Mill', 'Employee_injury_rate_per_200k_h', 'Crimes_per_1_Mill_passengers', 'Crimes_per_1_Mill_passengers')\n\n\nFirstly, we are going to see wherther the year and the month have a significant effect on the values seen in our data. This will help us understand if it has become better over the years or if there are months that have effects on the outcomes of public transportation due to weather or other circumstances.\n\n\nCode\nBus_on_time &lt;- DC_metro %&gt;% select('Year', 'Month', 'Bus_on_time')\n\nmodel &lt;- aov(Bus_on_time ~ Year + Month, data = Bus_on_time)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Bus_on_time\n          Df    Sum Sq    Mean Sq F value    Pr(&gt;F)    \nYear       2 0.0012562 0.00062809  7.6584 0.0031782 ** \nMonth     12 0.0056355 0.00046963  5.7263 0.0002625 ***\nResiduals 21 0.0017223 0.00008201                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the buses being on time.\n\n\nCode\nBus_fleet_reliability &lt;- DC_metro %&gt;% select('Year', 'Month', 'Bus_fleet_reliability')\n\nmodel &lt;- aov(Bus_fleet_reliability ~ Year + Month, data = Bus_fleet_reliability)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Bus_fleet_reliability\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \nYear       2 5604536 2802268  9.3323 0.001259 **\nMonth     12 9091418  757618  2.5231 0.030574 * \nResiduals 21 6305777  300275                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the bus fleet reliability (miles travelled before a breakdown).\n\n\nCode\nRail_fleet_reliability &lt;- DC_metro %&gt;% select('Year', 'Month', 'Rail_fleet_reliability')\n\nmodel &lt;- aov(Rail_fleet_reliability ~ Year + Month, data = Rail_fleet_reliability)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Rail_fleet_reliability\n          Df     Sum Sq   Mean Sq F value  Pr(&gt;F)  \nYear       2  536503502 268251751  2.8728 0.07892 .\nMonth     12 2480920803 206743400  2.2141 0.05343 .\nResiduals 21 1960928969  93377570                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case, year and month don’t seem to have a significant effect on rail transportation fleet reliability (miles travelled before a breakdown).\n\n\nCode\nRail_on_time &lt;- DC_metro %&gt;% select('Year', 'Month', 'Rail_on_time')\n\nmodel &lt;- aov(Rail_on_time ~ Year + Month, data = Rail_on_time)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Rail_on_time\n          Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    \nYear       2 0.060978 0.0304888 52.2126 7.084e-09 ***\nMonth     12 0.020835 0.0017363  2.9734   0.01397 *  \nResiduals 21 0.012263 0.0005839                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nClearly year and month seem to have a significant effect on the rail transportation being on time.\n\n\nCode\nCustomer_injury_rate_per_1_Mill &lt;- DC_metro %&gt;% select('Year', 'Month', 'Customer_injury_rate_per_1_Mill')\n\nmodel &lt;- aov(Customer_injury_rate_per_1_Mill ~ Year + Month, data = Customer_injury_rate_per_1_Mill)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Customer_injury_rate_per_1_Mill\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nYear       2  0.1121 0.05606  0.1770 0.83903  \nMonth     12 11.8039 0.98366  3.1056 0.01119 *\nResiduals 21  6.6515 0.31674                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOnly the month seems to have a significant effect on the customer injury rate (which is expected due to weather). However, if year does not, have we taken any necessary actions to improve the security on public transportation?\n\n\nCode\nEmployee_injury_rate_per_200k_h &lt;- DC_metro %&gt;% select('Year', 'Month', 'Employee_injury_rate_per_200k_h')\n\nmodel &lt;- aov(Employee_injury_rate_per_200k_h ~ Year + Month, data = Employee_injury_rate_per_200k_h)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Employee_injury_rate_per_200k_h\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nYear       2 11.822  5.9109  9.0165 0.00149 **\nMonth     12 12.759  1.0632  1.6218 0.16025   \nResiduals 21 13.767  0.6556                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case as year has a significant effect on employee injury rates and months do not, we can assume that some actions have been taken during the years.\n\n\nCode\nCrimes_per_1_Mill_passengers &lt;- DC_metro %&gt;% select('Year', 'Month', 'Crimes_per_1_Mill_passengers')\n\nmodel &lt;- aov(Crimes_per_1_Mill_passengers ~ Year + Month, data = Crimes_per_1_Mill_passengers)\n\nsummary_model &lt;- summary(model)\n\nprint(anova(model, test = \"F\"))\n\n\nAnalysis of Variance Table\n\nResponse: Crimes_per_1_Mill_passengers\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nYear       1  0.2313 0.23133  0.6465 0.44209  \nMonth     12 14.6777 1.22314  3.4182 0.03656 *\nResiduals  9  3.2205 0.35783                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOnly months seem to have a significant effect on crime rates produced on public transportation. We could use this information to decide what months of the year we should increase security and question if this effect is due to holidays or other important events.\n\n\nCode\n\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Year\", str_replace, \"CY \", \"\")\nhead(DC_metro)\n\nDC_metro$Year &lt;- as.numeric(DC_metro$Year)\n\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jan\", \"01\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Feb\", \"02\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Mar\", \"03\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Apr\", \"04\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"May\", \"05\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jun\", \"06\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Jul\", \"07\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Aug\", \"08\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Sep\", \"09\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Oct\", \"10\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Nov\", \"11\")\nDC_metro &lt;- DC_metro %&gt;%\n      mutate_at(\"Month\", str_replace, \"Dec\", \"12\")\n\nDC_metro &lt;- DC_metro[order(DC_metro$Year, DC_metro$Month), ]\n\nDC_metro &lt;- subset(DC_metro, DC_metro$Month!='YTD')\nhead(DC_metro)\n\nlibrary(zoo)\n\nDC_metro$Date &lt;- as.yearmon(paste(DC_metro$Month, DC_metro$Year, sep = \" \"), format = \"%m %Y\")\n\n\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Bus_on_time)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  \n  ylab(\"Bus on Time\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nWhile the spread of the points is very wide, it is concerning to see a trend in which the Buses seem to be more and more delayed over time. This means that we are not taking the necessary steps to improve it.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Bus_fleet_reliability)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") + \n  ylab(\"Fleet Reliability\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nFleet reliability has improved over time (even though the spread of the data is too wide). That is probably due to vehicle improvements.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Rail_fleet_reliability)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") + \n  ylab(\"Rail Fleet reliability\")  \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nThere seems to not be a clear improvement on the fleet reliability of rail transportation over this period of time.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Rail_on_time)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") + \n  ylab(\"Rail on time performance\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nRail transportation seems to clearly have been getting worse over time. This problem is concerning and should be tackled as soon as possible. Is it due to safety reasons, investment problems, or poor planification?\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Customer_injury_rate_per_1_Mill)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  \n  ylab(\"Customer injuries per 1 Million passangers\")  \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nCustomer injuries seem to have been stable overtime (with a couple of outliers that could be interesting to look for).\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Employee_injury_rate_per_200k_h)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") +  \n  ylab(\"Employee injuries per 200k h\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nThe employee injuries have been increasing over time. This means that if the trend keeps following this pattern, we should probably invest more in safety and take some more precautions for the employees.\n\n\nCode\noptions(warn = -1) \nggplot(DC_metro, aes(x=DC_metro$Date, y=DC_metro$Crimes_per_1_Mill_passengers)) +\n  geom_point() +\n  geom_smooth(method=lm , color=\"red\", fill=\"#69b3a2\", se=TRUE) +\n  xlab(\"Date\") + \n  ylab(\"Crimes per 1 Million passangers\") \n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nThe crime rates have been increasing over time. This means that if the trend keeps following this pattern, we should probably invest in more security on public transportation."
  },
  {
    "objectID": "EDA.html#text-data",
    "href": "EDA.html#text-data",
    "title": "Data Exploration",
    "section": "Text Data",
    "text": "Text Data\nIn order to see what what are the most important concerns regarding public transportation to the users, we are going top explore what people mention the most in their reddits about public transportation through a word cloud.\n\n\nCode\nimport re\nimport spacy.lang.en.stop_words as stopwords\nimport spacy\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndf=pd.read_json(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/sentiment_results.json\")\nprint(df.shape)\nprint(df.columns)\n\ndf = df[df.text != '']\nprint(df.shape)\nprint(df.columns)\n\ntexts = []\ny = []\n\nparser = spacy.load('en_core_web_sm')\nstop_words = stopwords.STOP_WORDS\n\n\nfor i in range(df.shape[0]):\n\n    keep = \"abcdefghijklmnopqrstuvwxyz \"\n    replace = \".,!;\"\n    tmp = \"\"\n    text_value = df[\"text\"].iloc[i]  \n\n\n    text_value = re.sub('[^a-zA-Z ]+', '', text_value.replace(\"&lt;br /&gt;\", \"\").lower())\n    text_value = parser(text_value)\n    tokens = [token.lower_ for token in text_value]\n    tokens = [token.lemma_ for token in text_value if token not in stop_words]\n\n\n    tmp = \" \".join(tokens)\n    texts.append(tmp)\nwordcloud_text = \" \".join(texts)\n\n\n\n\nCode\ndef generate_word_cloud(my_text):\n    from wordcloud import WordCloud, STOPWORDS\n    import matplotlib.pyplot as plt\n\n    def plot_cloud(wordcloud):\n        plt.figure(figsize=(40, 30))\n        plt.imshow(wordcloud) \n        plt.axis(\"off\");\n\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000, \n        random_state=1, \n        background_color='salmon', \n        colormap='Pastel1', \n        collocations=False,\n        stopwords = STOPWORDS).generate(my_text)\n    plot_cloud(wordcloud)\n    plt.show()\n\nworldcloud_text = \" \".join(texts)\n\nwords_to_remove = [\"one\", \"go\", \"even\", \"give\", \"will\", \"need\", \"say\", \"well\", \"still\", \"make\", \"think\", \"look\", \"etc\", \"actually\", \"yet\", \"put\"]\n\nfor word in words_to_remove:\n    wordcloud_text = wordcloud_text.replace(word, \"\")\n\n\n\ngenerate_word_cloud(wordcloud_text)\n\n\n\n\n\nFrom this world cloud, we can see what topics are the most talked about by people regarding public transportation. This can help us see what matters to them the most and focus on these topics."
  },
  {
    "objectID": "Data_Gathering.html",
    "href": "Data_Gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "This tab shows how and where the data was gathered from. Moreover, it displays a few row of the row data in order to display the data structure and its content.\n\n\nIt is worth noting that not all data sets have been used for the study, but some just for some EDA as they could give us interesting insights for the study. The ones that have been used just for EDA purposes will be under “Other data sets”."
  },
  {
    "objectID": "Data_Gathering.html#data-gathering",
    "href": "Data_Gathering.html#data-gathering",
    "title": "Data Gathering",
    "section": "",
    "text": "This tab shows how and where the data was gathered from. Moreover, it displays a few row of the row data in order to display the data structure and its content.\n\n\nIt is worth noting that not all data sets have been used for the study, but some just for some EDA as they could give us interesting insights for the study. The ones that have been used just for EDA purposes will be under “Other data sets”."
  },
  {
    "objectID": "Data_Gathering.html#census-blocks-data-set",
    "href": "Data_Gathering.html#census-blocks-data-set",
    "title": "Data Gathering",
    "section": "Census blocks data set",
    "text": "Census blocks data set\nThis data set contains the information about all the census blocks in the US. It was downloaded here. The data set contains 117 columns and 220740 rows. Each row is a census block. The information about what each variable means can be found inside EPA pdf inside the data folder. The data looks like this:\n\n\nCode\nimport pandas as pd\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Final.csv'\n\ndf = pd.read_csv(file_path)\ndf.head()\n\n\n\n\n\n\n\n\n\nOBJECTID\nGEOID10\nGEOID20\nSTATEFP\nCOUNTYFP\nTRACTCE\nBLKGRPCE\nCSA\nCSA_Name\nCBSA\n...\nD5DRI\nD5DE\nD5DEI\nD2A_Ranked\nD2B_Ranked\nD3B_Ranked\nD4A_Ranked\nNatWalkInd\nShape_Length\nShape_Area\n\n\n\n\n0\n1\n4.811300e+11\n4.811300e+11\n48\n113\n7825\n4\n206.0\nDallas-Fort Worth, TX-OK\n19100.0\n...\n0.184697\n0.000476\n0.137707\n6\n14\n15\n17\n14.000000\n3110.360820\n297836.0831\n\n\n1\n2\n4.811300e+11\n4.811300e+11\n48\n113\n7825\n2\n206.0\nDallas-Fort Worth, TX-OK\n19100.0\n...\n0.323221\n0.000801\n0.231868\n3\n10\n12\n14\n10.833333\n3519.469110\n484945.1466\n\n\n2\n3\n4.811300e+11\n4.811300e+11\n48\n113\n7825\n3\n206.0\nDallas-Fort Worth, TX-OK\n19100.0\n...\n0.314628\n0.000736\n0.213146\n1\n1\n7\n17\n8.333333\n1697.091802\n106705.9281\n\n\n3\n4\n4.811300e+11\n4.811300e+11\n48\n113\n7824\n1\n206.0\nDallas-Fort Worth, TX-OK\n19100.0\n...\n0.229821\n0.000708\n0.205018\n16\n10\n17\n17\n15.666667\n2922.609204\n481828.4303\n\n\n4\n5\n4.811300e+11\n4.811300e+11\n48\n113\n7824\n2\n206.0\nDallas-Fort Worth, TX-OK\n19100.0\n...\n0.164863\n0.000433\n0.125296\n4\n7\n11\n14\n10.166667\n3731.971773\n687684.7752\n\n\n\n\n5 rows × 117 columns"
  },
  {
    "objectID": "Data_Gathering.html#fuel-and-energy-data-set",
    "href": "Data_Gathering.html#fuel-and-energy-data-set",
    "title": "Data Gathering",
    "section": "Fuel and energy data set",
    "text": "Fuel and energy data set\nThis data set contains information about public transportation, agencies, cities, and energy consumption. It was downloaded here. The data set contains 64 columns and 1315 rows. The data looks like this:\n\n\nCode\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/2021_Fuel_and Energy.xlsm'\n\ndf = pd.read_excel(file_path, sheet_name='Fuel and Energy')\n\ndf.head()\n\n\n\n\n\n\n\n\n\nAgency\nCity\nState\nLegacy NTD ID\nNTD ID\nOrganization Type\nReporter Type\nPrimary UZA Population\nAgency VOMS\nMode\n...\nOther Fuel (mpg) Questionable\nElectric Propulsion (mi/kwh)\nElectric Propulsion (mi/kwh) Questionable\nElectric Battery (mi/kwh)\nElectric Battery (mi/kwh) Questionable\nAny data questionable?\nUnnamed: 60\nUnnamed: 61\n1\nUnnamed: 63\n\n\n\n\n0\nMTA New York City Transit\nBrooklyn\nNY\n2008\n20008\nSubsidiary Unit of a Transit Agency, Reporting...\nFull Reporter\n18351295\n10075\nDR\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNo\nNaN\nNaN\nHide questionable data tags\nNaN\n\n\n1\nMTA New York City Transit\nBrooklyn\nNY\n2008\n20008\nSubsidiary Unit of a Transit Agency, Reporting...\nFull Reporter\n18351295\n10075\nHR\n...\nNaN\n0.225575\nNaN\nNaN\nNaN\nNo\nNaN\nNaN\nShow questionable data tags\nNaN\n\n\n2\nMTA New York City Transit\nBrooklyn\nNY\n2008\n20008\nSubsidiary Unit of a Transit Agency, Reporting...\nFull Reporter\n18351295\n10075\nCB\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNo\nNaN\n1.0\n1\n1.0\n\n\n3\nMTA New York City Transit\nBrooklyn\nNY\n2008\n20008\nSubsidiary Unit of a Transit Agency, Reporting...\nFull Reporter\n18351295\n10075\nMB\n...\nNaN\nNaN\nNaN\n1.672273\nNaN\nNo\nNaN\nNaN\nNaN\nNaN\n\n\n4\nMTA New York City Transit\nBrooklyn\nNY\n2008\n20008\nSubsidiary Unit of a Transit Agency, Reporting...\nFull Reporter\n18351295\n10075\nRB\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNo\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 64 columns"
  },
  {
    "objectID": "Data_Gathering.html#reddit-data",
    "href": "Data_Gathering.html#reddit-data",
    "title": "Data Gathering",
    "section": "Reddit Data",
    "text": "Reddit Data\nThe following code extracted reddit urls from reddit posts about Public Transportation and saved them to a .json. We only focussed on those that specifically talked about public transportation in order to later analyze the sentiment of people’s opinions on public transportation.\n\n\nCode\nlibrary(RedditExtractoR)\nlibrary(jsonlite)\n\ntop_Pub_Transp_urls &lt;- find_thread_urls(keywords=\"public transportation\")\njsonlite::write_json(top_Pub_Transp_urls, \"top_pub_transp_urls.json\")\n\n\nThen, the following code extracted the content of those reddit posts and performed sentiment analysis on them, generated a data frame, and saved it to a .csv file\n\n\nCode\nimport pandas as pd\nimport json\n\nwith open('sentiment_scores.json', 'r') as json_file:\n    sentiment_scores = json.load(json_file)\n\nids = []\nneg_scores = []\nneu_scores = []\npos_scores = []\ncompound_scores = []\n\nfor idx, item in enumerate(sentiment_scores, start=1):\n    ids.append(idx)\n    sentiment_score = item.get('sentiment_score', {})\n    neg_scores.append(sentiment_score.get('neg', 0))\n    neu_scores.append(sentiment_score.get('neu', 0))\n    pos_scores.append(sentiment_score.get('pos', 0))\n    compound_scores.append(sentiment_score.get('compound', 0))\n\ndata = {\n    'ID': ids,\n    'Negative Score': neg_scores,\n    'Neutral Score': neu_scores,\n    'Positive Score': pos_scores,\n    'Compound Score': compound_scores\n}\n\ndf = pd.DataFrame(data)\n\ndf.to_csv('sentiment_scores.csv', index=False)\n\n\nThe final data’s first few rows look like this (Note that only the sentiments of the posts are shown here but the data set with the content can be found in our data folder under Reddit_sentiment_data):"
  },
  {
    "objectID": "Data_Gathering.html#border-crossing-data-set",
    "href": "Data_Gathering.html#border-crossing-data-set",
    "title": "Data Gathering",
    "section": "Border crossing data set",
    "text": "Border crossing data set\nThis data set contains information of the vehicles and their type that crossed the border between other countries and the US. It was downloaded here. The data set contains 10 columns and 386549 rows. The data looks like this:\n\n\nCode\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/Border_Crossing_Entry_Data_20231103.csv'\ndf = pd.read_csv(file_path)\n\ndf.head()\n\n\n\n\n\n\n\n\n\nPort Name\nState\nPort Code\nBorder\nDate\nMeasure\nValue\nLatitude\nLongitude\nPoint\n\n\n\n\n0\nDetroit\nMichigan\n3801\nUS-Canada Border\nAug 2023\nTrains\n128\n42.332\n-83.048\nPOINT (-83.047924 42.331685)\n\n\n1\nAlcan\nAlaska\n3104\nUS-Canada Border\nJul 2023\nBus Passengers\n696\n62.615\n-141.001\nPOINT (-141.001444 62.614961)\n\n\n2\nCalais\nMaine\n115\nUS-Canada Border\nJul 2023\nBuses\n16\n45.189\n-67.275\nPOINT (-67.275381 45.188548)\n\n\n3\nNoonan\nNorth Dakota\n3420\nUS-Canada Border\nJul 2023\nTrucks\n142\n48.999\n-103.004\nPOINT (-103.004361 48.999333)\n\n\n4\nWarroad\nMinnesota\n3423\nUS-Canada Border\nMay 2023\nBuses\n41\n48.999\n-95.377\nPOINT (-95.376555 48.999)"
  },
  {
    "objectID": "Data_Gathering.html#other-data-sets",
    "href": "Data_Gathering.html#other-data-sets",
    "title": "Data Gathering",
    "section": "Other data sets",
    "text": "Other data sets\n\nAPI for cityofchicago.org\nThe following code extracted the data frame about buses information in Chicago and saved it into a csv file. After careful analysis, while it was a very interesting dataset, others have been used for our analysis, but for requirement purposes, this has been left in the data folder. The data looks like this:\n\n\nCode\nimport pandas as pd\nfrom sodapy import Socrata\n\nclient = Socrata(\"data.cityofchicago.org\", None)\n\nresults = client.get(\"bynn-gwxy\", limit=2000)\n\nresults_df = pd.DataFrame.from_records(results)\n\nresults_df.to_csv('Chicago_avg_Buses.csv')\n\n\n\n\n\n\n\n\n\nEnergy consumed by passenger miles\nThe files energy_consumed_byMill_passenger_MILES.xlsx and vehicle_production_countries.xlsx were downloaded from: bts.gov.\nThese data sets focuses on the energy consumed by passenger miles and the vehicle production countries. The data looks like this (note that the data sets have many more columns but for the sake of space, only a few are shown here):\n\n\n\n\n\n\n\n\n\n\n\n\nDC Metro Scorecard\nThe file DC_Metro_Scorecard.xlsx and the zip folders: Walkable_distance_to_PubTrans, data.world\nThe data looks like this (in order of mention):"
  },
  {
    "objectID": "Data_cleaning.html",
    "href": "Data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This tab aims to clean the data sets that will be used in all the others. The goal is to have our data sets ready to be used in the analysis. Before doing so, we will explain some common processes that are done or checked by most data scientists. However, it is worth mentioning that data cleaning is not a linear process, and it is not always possible to follow a specific order. In fact, it is common to go back and forth between the different steps.\n\n\nThe first most common step is to import and visualize how the data looks like. This is shown already in our data gathering tab by displaying the top 4 or 5 rows. It is also common to display the number of columns and rows that the data has to give us a sense of the size of the data set. In addition, it is important to check the names the data types of each column in order to see whether it will be beneficial to change them for your use (depending on the type of analysis you will do).\n\n\n\nDealing with missing values can be complicated. There is not a right or wrong way to do it. However, the goal should be to deal with them in a way that the data-set integrity is mantained while being able to properly perform and assess the analysis intended. The most important thing is to record the process of dealing with missing values and to explain why you chose to do it in that way. This is important because it will allow others to understand your process and to replicate it if needed while also reporting the limitations of your analysis due to those missing values.\n\n\n\nAnother common practice is to deal with outliers. If you detect outliers, you should decide whether to keep them or not. This depends on the type of analysis that you do and will also depend on the type of data that you have.\n\n\n\nIt is also important to check for duplicates. You do not want to count a data point twice just because of an administrative error. If you do so, you will give more importance to that data point than it should have.\n\n\n\nFor these three data sets, we will cheack for NaN values and select the columns (variables) that we are going to use. These data sets do not hold duplicates. We want to keep outliers for our tests due to the fact that they may be important for our analysis. However, in each tab, this will be crosschecked. Additiopnally, there are no diuplicates in our data sets."
  },
  {
    "objectID": "Data_cleaning.html#data-cleaning",
    "href": "Data_cleaning.html#data-cleaning",
    "title": "Data Cleaning",
    "section": "",
    "text": "This tab aims to clean the data sets that will be used in all the others. The goal is to have our data sets ready to be used in the analysis. Before doing so, we will explain some common processes that are done or checked by most data scientists. However, it is worth mentioning that data cleaning is not a linear process, and it is not always possible to follow a specific order. In fact, it is common to go back and forth between the different steps.\n\n\nThe first most common step is to import and visualize how the data looks like. This is shown already in our data gathering tab by displaying the top 4 or 5 rows. It is also common to display the number of columns and rows that the data has to give us a sense of the size of the data set. In addition, it is important to check the names the data types of each column in order to see whether it will be beneficial to change them for your use (depending on the type of analysis you will do).\n\n\n\nDealing with missing values can be complicated. There is not a right or wrong way to do it. However, the goal should be to deal with them in a way that the data-set integrity is mantained while being able to properly perform and assess the analysis intended. The most important thing is to record the process of dealing with missing values and to explain why you chose to do it in that way. This is important because it will allow others to understand your process and to replicate it if needed while also reporting the limitations of your analysis due to those missing values.\n\n\n\nAnother common practice is to deal with outliers. If you detect outliers, you should decide whether to keep them or not. This depends on the type of analysis that you do and will also depend on the type of data that you have.\n\n\n\nIt is also important to check for duplicates. You do not want to count a data point twice just because of an administrative error. If you do so, you will give more importance to that data point than it should have.\n\n\n\nFor these three data sets, we will cheack for NaN values and select the columns (variables) that we are going to use. These data sets do not hold duplicates. We want to keep outliers for our tests due to the fact that they may be important for our analysis. However, in each tab, this will be crosschecked. Additiopnally, there are no diuplicates in our data sets."
  },
  {
    "objectID": "Data_cleaning.html#information-about-us-census-blocks-dataset",
    "href": "Data_cleaning.html#information-about-us-census-blocks-dataset",
    "title": "Data Cleaning",
    "section": "Information about US census blocks dataset",
    "text": "Information about US census blocks dataset\nThe followin code attempts to clean the EPA_SmartLocationDatabase_V3_Jan_2021_Final.csv file. It keeps the columns that will be relevant for the analysis and clustering of the census block data of the US cities. The columns excluded are all the administrative variables, the individualistic columns such as IDs that would not allow us to perform clustering properly, and other columns that are not relevant for the analysis (all columns droped are found under the comment “# Drop unwanted columns” in the code chunk, and their definitions inside EPA pdf inside the data folder).\nAll variables have been checked and there are no missing values that would affect our study.\n\n\nCode\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Final.csv'\n\ndf = pd.read_csv(file_path)\n\n# Drop unwanted columns\ndf.drop(columns=[\"OBJECTID\", \"GEOID10\", \"GEOID20\", \"STATEFP\", \"COUNTYFP\", \"COUNTYFP\", \"TRACTCE\", \"BLKGRPCE\", \"CSA\", \"CSA_Name\", \"CBSA\", \"CBSA_Name\", \"CountHU\", \"HH\", \"D1A\", \"D1C8_OFF\", \"D4D\", \"D4E\", \"D2A_JPHH\", \"D2B_E5MIX\", \"D2B_E5MIXA\", \"D2B_E8MIX\",\"D2B_E8MIXA\",\"D2A_EPHHM\",\"D2C_TRPMX1\",\"D2C_TRPMX2\",\"D2C_TRIPEQ\",\"D2R_JOBPOP\",\"D2R_WRKEMP\",\"D2A_WRKEMP\",\"D2C_WREMLX\",\"D4A\",\"D4B025\",\"D4B050\",\"D4C\",\"D5AR\",\"D5AE\",\"D5BR\",\"D5BE\",\"D5CR\",\"D5CRI\",\"D5CE\",\"D5CEI\",\"D5DR\",\"D5DRI\",\"D5DE\",\"D5DEI\"], inplace=True)\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nprint(df.isna().sum())\n\ndf.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/EPA_SmartLocationDatabase_V3_Jan_2021_Clean.csv', index=False)"
  },
  {
    "objectID": "Data_cleaning.html#text-data",
    "href": "Data_cleaning.html#text-data",
    "title": "Data Cleaning",
    "section": "Text data",
    "text": "Text data\nThe following code cleans our text data file top_pub_transp_urls.json. It removes the escape sequences, non-alphabetic characters, the extra white spaces and strips the text. Then, it saves it in the clean data folder as cleaned_text_data.json (it can be found inside the reddit data on the raw data folder, but also inside the clean data folder).\nNote: the sentiment analysis was already extracted and saved in the data tab.\n\n\nCode\nimport json\nimport re\n\nwith open('top_pub_transp_urls.json', 'r') as json_file:\n    data = json.load(json_file)\n\ndef clean_text(text):\n    cleaned_text = re.sub(r'\\\\u....', '', text) \n    cleaned_text = re.sub(r'[^A-Za-z\\s]', ' ', cleaned_text) \n    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text) \n    cleaned_text = cleaned_text.strip() \n    return cleaned_text\n\ncleaned_text_data = []\n\nfor item in data:\n    cleaned_text = clean_text(item.get(\"text\", \"\"))\n    cleaned_text_data.append({\"url\": item[\"url\"], \"cleaned_text\": cleaned_text})\n\nwith open('cleaned_text_data.json', 'w') as json_file:\n    json.dump(cleaned_text_data, json_file, indent=4)\n\n#print(\"Text data cleaned and saved to cleaned_text_data.json.\")"
  },
  {
    "objectID": "Data_cleaning.html#fuel-and-energy-data",
    "href": "Data_cleaning.html#fuel-and-energy-data",
    "title": "Data Cleaning",
    "section": "Fuel and Energy data",
    "text": "Fuel and Energy data\nThe following code selects the columns that are relevant for the analysis and drops the rest. Since this data set will be used for modeling purposes, the redundant columns that were directly connected to the target variable have been dropped (in this case, since we will focuss on predicting avg miles per galon of fossil fuels, all other information of usage of fossil fuels is being dropped). It also creates a new column that averages the mpg between diesel and gasoline, and then drops all the rows that have missing values in this new (since they will not be able to be used for our modeling purposes). The final data set is saved in the clean data folder as 2021_Fuel_and_Energy_mpg_fossil_fuels.csv.\nNote: the columns kept are: City, Agency, Organization Type, Primary UZA Population, Mode, TOS, Electric Battery, and the new column that averages the mpg between diesel and gasoline (called avg_mpg_fossil_fuels).\n\n\nCode\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/2021_Fuel_and Energy.xlsm'\n\ndf = pd.read_excel(file_path, sheet_name='Fuel and Energy')\n\ncolumns_to_keep = ['City', 'Agency', 'Organization Type', 'Primary UZA Population', 'Mode', 'TOS', 'Electric Battery', 'Diesel (mpg)', 'Gasoline (mpg)']\n\n# Keep only the specified columns\ndf_filtered = df[columns_to_keep]\n\ndf_filtered['avg_mpg_fossil_fuels'] = df_filtered[['Diesel (mpg)', 'Gasoline (mpg)']].mean(axis=1, skipna=True)\n\ndf_filtered = df_filtered.dropna(subset=['avg_mpg_fossil_fuels'])\n\ndf_filtered = df_filtered.drop(['Diesel (mpg)', 'Gasoline (mpg)'], axis=1)\n\ndf_filtered.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/2021_Fuel_and_Energy_mpg_fossil_fuels.csv', index=False)"
  },
  {
    "objectID": "Data_cleaning.html#border-crossing-entry-data",
    "href": "Data_cleaning.html#border-crossing-entry-data",
    "title": "Data Cleaning",
    "section": "Border Crossing Entry Data",
    "text": "Border Crossing Entry Data\nThe following code cleans the data of Border_Crossing_Entry_Data_20231103.csv (found in the raw data folder). Since this data set will be used for modeling purposes (in the Naive Bayes tab), we will use the columns: Value, State, Measure. The cleaning code will only select this columns and save them into a new csv called Clean_Border_Crossing_Entry_Data_20231103.csv in the clean data folder.\nNote: NaN values have been checked and there are none\n\n\nCode\nimport pandas as pd\n\nfile_path = '../../data/Raw_Data_project_Pub.Transport_5000/Border_Crossing_Entry_Data_20231103.csv'\ndf = pd.read_csv(file_path)\n\ncolumns_to_keep = [\"Value\", \"State\", \"Measure\"]\n\ndf=df[columns_to_keep]\n\ndf.isna().sum()\n\ndf.to_csv('../../data/Clean_Data_project_Pub.Transport_5000/Clean_Border_Crossing_Entry_Data_20231103.csv', index=False)"
  },
  {
    "objectID": "Conclusion.html",
    "href": "Conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "This section of the conclusion will be a summary of the insights from the different tabs. Before we dive into making overall conclusions and suggestions for the improvement of public transportation, it is important to have a summary of the insights obtained from our different analysis and models.\n\n\nFrom a general perspective, it seems that the performance of public transportation, its usage, and its contamination levels have not improved much in the last 10 years. Furthermore, it seems like it could be improved in many areas to be more efficient and sustainable.\n\n\n\nOur Naive Bayes model shows that predicting the type of transportation based on the State crossed and its value is not really accurate. While other type of analysis could be done to see if they are related or not with more precision, our model is not able to predict the type of transportation. This seems to be a clear fact that the type of transportation is not related to the State crossed and its value. However, I suggest studying this topic further with other analysis.\nOn the other hand, Naive Bayes does a great job at predicting the sentiment of reddit posts/comments. While this model could be used to identify the negative reddit posts (or posts in other platforms if checked its accruacy), it could be more effective if utilized with the purpose of identifying the negative posts in order to exteract them and identify the main issues users encounter. That way, the issues could be addressed and solved.\n\n\n\nOur clustering tab indicates that clusters seem to be present in the census blocks of the US. This is a great finding since we can infer that there are different “types” of census blocks and we could address them in a group manner. This could allow the US to invest in their development as the reduced number of clusters allow the US to invest in the development of the clusters in a more efficient and economic manner. If dealt with them individually, the US would have to invest more money and time in the development of the census blocks, which probably is not possible.\n\n\n\nOur data set for the census blocks had too many redundant variables, as PCA identified that 25 would explain more than 90% of the variance. This is a great insight as, if inferences made for different clusters are to be made, it would be much more computational efficient to use the PCA variables. FDurthermore, if the inferences are to be extrapolated to other or new census blocks, it would be much more efficient to use the PCA variables to determine their cluster and then make the inferences.\n\n\n\nDecision trees seem to be a great model to predict the average miles per gallon based on city, agency, organization type, Primary UZA population, mode of transportation, and type of service, electric atteries used. Even more effectively, just using Agency, type of service, and mode of transportation seems to be the right choice. Thus, this model could be used to find the optimal combination of these three components in order to choos the most efficient one in every case. That way, we could reduce our carbon footprint while reducing our fossil fuel’s cost.\n\n\n\nPeople seem to care about time and stops the most when writing on reddit about public transportation. This gives us an insight that time is one of the most important things for people and having stops available for when they use public transportation seems to be logical. Thus, we could invest in the development of more stops and more efficient routes to reduce the time people spend on public transportation.\nAdditionally, another key insight worth highlighting is that transportation has been performing worse over time. However, the energy consumed seems to also have decreased (slowly) overtime. Furthermore, month seems to have an apparent effect on performance, which could be overcome by investing in more resources during the months where performance is lower. Finally, understanding what countries play the biggest role in the manufacturing of public transportation vehicles could be a great insight to understand the impact of public transportation in the world. More advances in R&D could be made to improve the manufacturing of these vehicles and reduce their carbon footprint in these countries."
  },
  {
    "objectID": "Conclusion.html#insights-and-general-comments-from-different-tabs",
    "href": "Conclusion.html#insights-and-general-comments-from-different-tabs",
    "title": "Conclusion",
    "section": "",
    "text": "This section of the conclusion will be a summary of the insights from the different tabs. Before we dive into making overall conclusions and suggestions for the improvement of public transportation, it is important to have a summary of the insights obtained from our different analysis and models.\n\n\nFrom a general perspective, it seems that the performance of public transportation, its usage, and its contamination levels have not improved much in the last 10 years. Furthermore, it seems like it could be improved in many areas to be more efficient and sustainable.\n\n\n\nOur Naive Bayes model shows that predicting the type of transportation based on the State crossed and its value is not really accurate. While other type of analysis could be done to see if they are related or not with more precision, our model is not able to predict the type of transportation. This seems to be a clear fact that the type of transportation is not related to the State crossed and its value. However, I suggest studying this topic further with other analysis.\nOn the other hand, Naive Bayes does a great job at predicting the sentiment of reddit posts/comments. While this model could be used to identify the negative reddit posts (or posts in other platforms if checked its accruacy), it could be more effective if utilized with the purpose of identifying the negative posts in order to exteract them and identify the main issues users encounter. That way, the issues could be addressed and solved.\n\n\n\nOur clustering tab indicates that clusters seem to be present in the census blocks of the US. This is a great finding since we can infer that there are different “types” of census blocks and we could address them in a group manner. This could allow the US to invest in their development as the reduced number of clusters allow the US to invest in the development of the clusters in a more efficient and economic manner. If dealt with them individually, the US would have to invest more money and time in the development of the census blocks, which probably is not possible.\n\n\n\nOur data set for the census blocks had too many redundant variables, as PCA identified that 25 would explain more than 90% of the variance. This is a great insight as, if inferences made for different clusters are to be made, it would be much more computational efficient to use the PCA variables. FDurthermore, if the inferences are to be extrapolated to other or new census blocks, it would be much more efficient to use the PCA variables to determine their cluster and then make the inferences.\n\n\n\nDecision trees seem to be a great model to predict the average miles per gallon based on city, agency, organization type, Primary UZA population, mode of transportation, and type of service, electric atteries used. Even more effectively, just using Agency, type of service, and mode of transportation seems to be the right choice. Thus, this model could be used to find the optimal combination of these three components in order to choos the most efficient one in every case. That way, we could reduce our carbon footprint while reducing our fossil fuel’s cost.\n\n\n\nPeople seem to care about time and stops the most when writing on reddit about public transportation. This gives us an insight that time is one of the most important things for people and having stops available for when they use public transportation seems to be logical. Thus, we could invest in the development of more stops and more efficient routes to reduce the time people spend on public transportation.\nAdditionally, another key insight worth highlighting is that transportation has been performing worse over time. However, the energy consumed seems to also have decreased (slowly) overtime. Furthermore, month seems to have an apparent effect on performance, which could be overcome by investing in more resources during the months where performance is lower. Finally, understanding what countries play the biggest role in the manufacturing of public transportation vehicles could be a great insight to understand the impact of public transportation in the world. More advances in R&D could be made to improve the manufacturing of these vehicles and reduce their carbon footprint in these countries."
  },
  {
    "objectID": "Conclusion.html#general-conclusion",
    "href": "Conclusion.html#general-conclusion",
    "title": "Conclusion",
    "section": "General Conclusion",
    "text": "General Conclusion\nAs shown in our study, transportation is a key component of society. It not only allows people to move from one place to another but is also needed for the economy to function. Furthermorem, it plays a big role in city and economic development as well as in the environment (accounting for almost 30% of the polution in the US). Thus, it is important to understand how it works and how it can be improved.\nOne of the biggest takeaways from this study is that census blocks can be grouped since they seem to have similar characteristics. This insight is very revealing as implementing changes or building public transportation models for each place in the US is too time consuming and costly for the US. Thus, the first suggestion to improve public transportation would be to generate twelve (one for each cluster) base transportation models that could be slightly tweaked to fit the more specific needs of each census block. This would allow the US to generate twelve very robust models that would work much better than creating over 220,000 simple models. Furthermore, improvements on each could be implemented in more places just with slight changes, making the process much more efficient.\nAnother key insight is that the Agency, type of service, and mode of transportation seem to be deterministic in the fuel consumption efficiency. This could be used to find the most optimal combination of these three components in order to choose the most efficient one in every case. That way, we could reduce our carbon footprint while reducing our fossil fuel’s cost.\nIncentivizing the use of public transpotation is also very important. That way, we would be able to reduce the amount of cars on the road, reducing the amount of polution and traffic. This could be done by improving the areas that are most important to people. Our Naive Bayes model can do a good job at detecting negative comments from people which then can be analyzed in order to identify the main issues people have with public transportation. With this information, issues can be addressed and solved. Additionally, what seems to be the most important to people is time, and the public transportation performance has actually been decreasing over the past years. We have information that could be used to revert this tendency such as the months that make the reliability of public transportation worse. Thus, we could invest in more resources during the months where performance is lower.\nMoreover, understanding what countries play the biggest role in the manufacturing of public transportation vehicles could be a great insight to understand the impact of public transportation in the world. More advances in R&D could be made to improve the manufacturing of these vehicles and reduce their carbon footprint in these countries. Associations with these countries could be done to reach a common goal of reducing the carbon footprint of public transportation while improving its performance.\nFinally, it is worth noting that this is a very complex topic and that this analysis only provides an overview of some of the most important aspects of public transportation. There are many other aspects that could be analyzed and that could provide more insights. Further research in fuels, economic factors, and other areas are definitely needed to be able to effectively implement the reccomendations made in this study. However, this analysis provides a good starting point to understand the current situation of public transportation and how it might be improved."
  },
  {
    "objectID": "Conclusion.html#lets-preserve-the-beautiful-planet-we-live-in",
    "href": "Conclusion.html#lets-preserve-the-beautiful-planet-we-live-in",
    "title": "Conclusion",
    "section": "Let’s preserve the beautiful planet we live in!",
    "text": "Let’s preserve the beautiful planet we live in!"
  },
  {
    "objectID": "Naive_Bayes.html",
    "href": "Naive_Bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a supervised machine learning algorithm based on Bayes’ Theorem. It is used for classification tasks where every pair of features being classified is independent of each other. Naive Bayes leverages the probability of an event occurring given the probability of another event that has already occurred using Baye’s theorem to asses the probability of a feature belonging to a class.\nWhile Naive Bayes might seem a simple process, its results are very insightful and have been proven to work very efficiently. One of the benefits is that it needs very little data to be trained and, as more data comes in, it can always be trained incrementally. However, it assumes independence among the predictors. Thus, while being a very good model for predicting spam emails, sentiment analysis, or document categorization, it is not as robust for other more complex tasks. The three most common Naive Bayes algorithms are: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\nOn this tab, Naive Bayes will be used to predict the method of transportation (“Measure” in the data set) based on the value and the State of entrance from the Clean_Border_Crossing_Entry_Data_20231103.csv file (found in the clean data folder). While it will not be a great addition to our project, for the sake of learning and assignment fullfillment, this is a good example of how Naive Bayes works. On the other hand, more insightfull, Naive Bayes will be used in our text data to predict the sentiment of reddit posts.\n\n\nThe formula for Bayes’ Theorem is: \\(P(\\text{A}|\\text{B}) = \\frac{P(\\text{B}|\\text{A}) \\times P(\\text{A})} {P(\\text{B})}\\)\n\n\n\nIt is used when the variables are continuous and have a normal distribution\nFormula: \\(P(\\text{class}|\\text{data}) = \\frac{P(\\text{data}|\\text{class}) \\times P(\\text{class})} {P(\\text{data})}\\)\n\n\n\nIt is used when the variables are discrete and have a multinomial distribution.\n\n\n\nIt is used when the variables are binary and have a Bernoulli distribution.\n\n\n\n\nMissing values are ignored while training the model and ignored when a probability is calculated.\nPerforms well even when the independence assumption is not satisfied.\nIt is easily to interpretate and has fast prediction time.\n\n\n\n\nThe data for producing the model will be divided into training and testing data. The training data will account for 80% of the data and the testing data will account for 20% of the data. Then the model will be trained using the train ing data and then tested its accruacy with the testing data since it would be new data that our model has never seen before. That way, we will be able to asses the accruacy of our model or, for our text data, assess the number of features we want to take into account in order to not overfit our model and have a more accurate prediction."
  },
  {
    "objectID": "Naive_Bayes.html#introduction",
    "href": "Naive_Bayes.html#introduction",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a supervised machine learning algorithm based on Bayes’ Theorem. It is used for classification tasks where every pair of features being classified is independent of each other. Naive Bayes leverages the probability of an event occurring given the probability of another event that has already occurred using Baye’s theorem to asses the probability of a feature belonging to a class.\nWhile Naive Bayes might seem a simple process, its results are very insightful and have been proven to work very efficiently. One of the benefits is that it needs very little data to be trained and, as more data comes in, it can always be trained incrementally. However, it assumes independence among the predictors. Thus, while being a very good model for predicting spam emails, sentiment analysis, or document categorization, it is not as robust for other more complex tasks. The three most common Naive Bayes algorithms are: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\nOn this tab, Naive Bayes will be used to predict the method of transportation (“Measure” in the data set) based on the value and the State of entrance from the Clean_Border_Crossing_Entry_Data_20231103.csv file (found in the clean data folder). While it will not be a great addition to our project, for the sake of learning and assignment fullfillment, this is a good example of how Naive Bayes works. On the other hand, more insightfull, Naive Bayes will be used in our text data to predict the sentiment of reddit posts.\n\n\nThe formula for Bayes’ Theorem is: \\(P(\\text{A}|\\text{B}) = \\frac{P(\\text{B}|\\text{A}) \\times P(\\text{A})} {P(\\text{B})}\\)\n\n\n\nIt is used when the variables are continuous and have a normal distribution\nFormula: \\(P(\\text{class}|\\text{data}) = \\frac{P(\\text{data}|\\text{class}) \\times P(\\text{class})} {P(\\text{data})}\\)\n\n\n\nIt is used when the variables are discrete and have a multinomial distribution.\n\n\n\nIt is used when the variables are binary and have a Bernoulli distribution.\n\n\n\n\nMissing values are ignored while training the model and ignored when a probability is calculated.\nPerforms well even when the independence assumption is not satisfied.\nIt is easily to interpretate and has fast prediction time.\n\n\n\n\nThe data for producing the model will be divided into training and testing data. The training data will account for 80% of the data and the testing data will account for 20% of the data. Then the model will be trained using the train ing data and then tested its accruacy with the testing data since it would be new data that our model has never seen before. That way, we will be able to asses the accruacy of our model or, for our text data, assess the number of features we want to take into account in order to not overfit our model and have a more accurate prediction."
  },
  {
    "objectID": "Naive_Bayes.html#naive-bayes-for-labeled-record-data",
    "href": "Naive_Bayes.html#naive-bayes-for-labeled-record-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes for labeled record data",
    "text": "Naive Bayes for labeled record data\nNaive Bayes has been used in our record data from our data set: Clean_Border_Crossing_Entry_Data_20231103.csv. The model tries to predict the method of transportation (“Measure” in the data set) based on the value and the State of entrance. The goal is to predict the method of transportation based on the value and the State of entrance. All categorical variables are converted into factor types in order to run the model (that is the variable: State).\nThe scores obtained from our model are printed bellow. Due to the great number of labels, printing the confusion matrix or the overall statistics for each label will be confusing, as well as plotting the confusion matrix. Therefore, we will only print the statistics.\n\n\nCode\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(e1071)\n\nchicago &lt;- read.csv('../../data/Clean_Data_project_Pub.Transport_5000/Clean_Border_Crossing_Entry_Data_20231103.csv')\n\nnumeric_cols &lt;- chicago[sapply(chicago, is.numeric)]\n\nscaled_numeric_cols &lt;- as.data.frame(scale(numeric_cols))\n\nchicago[sapply(df, is.numeric)] &lt;- scaled_numeric_cols\n\nvars &lt;- c(\"Value\", \"State\")\n\nsub &lt;- createDataPartition(y = chicago$Measure, \n                           p = 0.80, \n                           list = FALSE)\n\nchicago[] &lt;- lapply(chicago, as.factor)\n\ncen_bcs &lt;- preProcess(x = chicago[ ,vars],\n                      method = c(\"BoxCox\", \"center\", \"scale\"))\n\nDM_bcs &lt;- predict(object = cen_bcs,\n                      newdata = chicago)\n\ntrain &lt;- DM_bcs[sub, ] \ntest &lt;- DM_bcs[-sub, ]\n\nnb_model &lt;- naiveBayes(Measure ~ ., data = train)\n\npredictions &lt;- predict(nb_model, newdata = test)\n\npredictions &lt;- factor(predictions, levels = levels(test$Measure))\n\nconfusion_matrix &lt;- confusionMatrix(predictions, test$Measure)\n\noverall_stats &lt;- confusion_matrix$overall\nprint(overall_stats[c(\"Accuracy\", \"AccuracyLower\", \"AccuracyUpper\", \"Kappa\", \"McnemarPValue\")])\n\n\nWarning message in pre_process_options(method, column_types):\n\"The following pre-processing methods were eliminated: 'BoxCox', 'center', 'scale'\"\n\n\n     Accuracy AccuracyLower AccuracyUpper         Kappa McnemarPValue \n   0.16986819    0.16722706    0.17253412    0.09602056    0.00000000 \n\n\nThe following code could be utilized to view the confusion matrix and the overall statistics for each label. However, it has not been ran due to the reasons mentioned above.\n\n\nCode\nconfusion_matrix_df &lt;- as.data.frame(as.table(confusion_matrix$table))\n\nlibrary(ggplot2)\ngg &lt;- ggplot(data = confusion_matrix_df, aes(x = Reference, y = Prediction)) +\n  geom_tile(aes(fill = Freq)) +\n  geom_text(aes(label = Freq), vjust = 1) +\n  scale_fill_gradient(low = \"#7cf09b\", high = \"#3ee882\") +\n  labs(\n    x = \"Reference\",\n    y = \"Prediction\",\n    fill = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12)) +\n  ggtitle(\"Confusion Matrix\")\n\nprint(gg)\n\n\n\nConclusion\nThe model has a very low accruacy score, only 17%. This can be because this type of model is not the best for our type of data or our labels cannot be properly predicted based on the features we have since they are not deterministic on the label. Further study can be done for to see which of these two reasons is the cause of the low accuracy score. However, it seems that predicting the method of transportation based on the value and the State of entrance is not a good idea."
  },
  {
    "objectID": "Naive_Bayes.html#naive-bayes-for-labeled-text-data",
    "href": "Naive_Bayes.html#naive-bayes-for-labeled-text-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes for labeled text data",
    "text": "Naive Bayes for labeled text data\nNaive Bayes has been used in our text data from redit in order to predict the sentiment of the comments. The data has been labeled by the sentiment of the comments as shown in the data gathering tab. The data has been split into a training set and a test set. The training set has been used to train the model and the test set has been used to test the model.\nNote: The following code includes a bit more parsing of the data than the one in the data cleaning tab due to some inconsistencies found in the data. Furthermore, the sentiment scores have been changed to positive if the score is greater or equal to 0 and negative if the score is less than 0.\nCredits: Most of the code for this section has been changed from the code privided by Professor James Hickman from the DSAN program at Georgetown University.\n\n\nCode\nimport re\nimport spacy.lang.en.stop_words as stopwords\nimport spacy\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\ndf=pd.read_json(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/sentiment_results.json\")\nprint(df.shape)\nprint(df.columns)\n\ndf = df[df.text != '']\n#print(df.shape)\n#print(df.columns)\n\ndf['sentiment'] = df['sentiment_score'].apply(lambda score: 'positive' if score &gt;= 0 else 'negative')\n\n\n\n\nCode\ntexts = []\ny = []\n\nparser = spacy.load('en_core_web_sm')\nstop_words = stopwords.STOP_WORDS\n\n\nfor i in range(df.shape[0]):\n    keep = \"abcdefghijklmnopqrstuvwxyz \"\n    replace = \".,!;\"\n    tmp = \"\"\n    text_value = df[\"text\"].iloc[i]\n    sentiment_value = df[\"sentiment\"].iloc[i] \n\n\n    text_value = re.sub('[^a-zA-Z ]+', '', text_value.replace(\"&lt;br /&gt;\", \"\").lower())\n    text_value = parser(text_value)\n    tokens = [token.lower_ for token in text_value]\n\n    tokens = [token.lemma_ for token in text_value if token not in stop_words]\n\n\n    tmp = \" \".join(tokens)\n    texts.append(tmp)\n\n    if sentiment_value == \"positive\":\n        y.append(1)\n    elif sentiment_value == \"negative\":\n        y.append(0)\n\n    if i &lt; 3:\n        print(i)\n        print(tmp.replace(\"&lt;br /&gt;\", \"\"), '\\n')\n        print(tmp)\n        print(sentiment_value, y[i])\n\ny=np.array(y)\n\n\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(texts,MAX_FEATURES=10000)\n\n\n\n\nCode\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\n\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\n\nprint()\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\n\ndf2.columns = range(df2.columns.size)\nprint(df2.head())\nprint(df2.sum(axis=0))\nx=df2.to_numpy()\n\ndef train_MNB_model(X, Y, i_print=False):\n    if i_print:\n        print(X.shape, Y.shape)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    model = MultinomialNB()\n\n    start = time.process_time()\n    model.fit(X_train, y_train)\n    time_train = time.process_time() - start\n\n    start = time.process_time()\n    yp_train = model.predict(X_train)\n    yp_test = model.predict(X_test)\n    time_eval = time.process_time() - start\n\n    acc_train = accuracy_score(y_train, yp_train) * 100\n    acc_test = accuracy_score(y_test, yp_test) * 100\n\n    if i_print:\n        print(acc_train, acc_test, time_train, time_eval)\n\n    return acc_train, acc_test, time_train, time_eval\n\nprint(type(x), type(y))\nprint(x.shape, y.shape)\n(acc_train, acc_test, time_train, time_eval) = train_MNB_model(x, y, i_print=True)\n\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\ninitialize_arrays()\n\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\nsave_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/results_naive_bayes\")\n\n\n\n\nCode\ndef plot_results(path_root):\n\n    plt.plot(num_features,train_accuracies,'-ob')\n    plt.plot(num_features,test_accuracies,'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\nplot_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/results_naive_bayes\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsave_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/partial_grid_search\")\nplot_results(\"../../data/Raw_Data_project_Pub.Transport_5000/Reddit_sentiment_data/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nFrom these graphs, we can see that Naive Bayes does a good job in predicting whether the posts are positive or negative based on the text if the number of features is kept under 2000 or between 4000 and 6000. However, if the number of features is greater than 6000, the accruacy score drops significantly. This is because the model would be overfitting the data based on the training data."
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "Data",
    "section": "",
    "text": "The data sets can be found in the following link: Press here!\nThey have not been added to our Git Hub repository due to their size."
  },
  {
    "objectID": "Data.html#data",
    "href": "Data.html#data",
    "title": "Data",
    "section": "",
    "text": "The data sets can be found in the following link: Press here!\nThey have not been added to our Git Hub repository due to their size."
  },
  {
    "objectID": "Data.html#meanwhile-enjoy-these-pictures-i-took-that-have-different-methods-of-transportation",
    "href": "Data.html#meanwhile-enjoy-these-pictures-i-took-that-have-different-methods-of-transportation",
    "title": "Data",
    "section": "Meanwhile, enjoy these pictures I took that have different methods of transportation!",
    "text": "Meanwhile, enjoy these pictures I took that have different methods of transportation!"
  }
]