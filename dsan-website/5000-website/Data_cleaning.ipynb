{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Data Cleaning\n",
        "format:\n",
        "  html:\n",
        "    css: h1\n",
        "    embed-resources: true\n",
        "    code-fold: true\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Load the existing JSON file with Reddit post data\n",
        "with open('top_pub_transp_urls.json', 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    # Remove unwanted characters and escape sequences\n",
        "    cleaned_text = re.sub(r'\\\\u....', '', text)  # Remove escape sequences\n",
        "    cleaned_text = re.sub(r'[^A-Za-z\\s]', ' ', cleaned_text)  # Remove non-alphabetic characters\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra whitespace\n",
        "    cleaned_text = cleaned_text.strip()  # Strip leading and trailing spaces\n",
        "    return cleaned_text\n",
        "\n",
        "# Extract and clean the text data\n",
        "cleaned_text_data = []\n",
        "\n",
        "for item in data:\n",
        "    cleaned_text = clean_text(item.get(\"text\", \"\"))\n",
        "    cleaned_text_data.append({\"url\": item[\"url\"], \"cleaned_text\": cleaned_text})\n",
        "\n",
        "# Save the cleaned text data to a new JSON file\n",
        "with open('cleaned_text_data.json', 'w') as json_file:\n",
        "    json.dump(cleaned_text_data, json_file, indent=4)\n",
        "\n",
        "print(\"Text data cleaned and saved to cleaned_text_data.json.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous code cleans the text data such that we only get the urls and the text of each reddit post. It can be found in the data tab. That way it can be later used for key word extraction.\n",
        "\n",
        "Note: the sentiment analysis was already extracted and saved in the data tab."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "clean data as vocabulary each col and each row number of appareances"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
